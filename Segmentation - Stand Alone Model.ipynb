{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim \n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "import resnet_no_fc\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Threat score for road detection\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image folder\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_scene_index = np.arange(106, 132)\n",
    "val_labeled_scene_index = np.arange(132, 134)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_collate_fn(batch):\n",
    "    BLOCK_SIZE = 5\n",
    "    road_maps = []\n",
    "    road_bins = []\n",
    "    images = []\n",
    "    for x in batch:\n",
    "        \n",
    "        grid = []\n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.as_tensor(x[0][i]))\n",
    "        \n",
    "        # Get road_image and cast it to float\n",
    "        road_image = torch.as_tensor(x[2])\n",
    "        road_maps.append(road_image)\n",
    "        road_image = road_image.float()\n",
    "        \n",
    "        for x in range(0, 800, BLOCK_SIZE):\n",
    "            for y in range(0, 800, BLOCK_SIZE):\n",
    "                block = road_image[x:x+BLOCK_SIZE, y:y+BLOCK_SIZE]\n",
    "                score = torch.sum(block).item()\n",
    "                # If more than have the pixels are 1, classify as road\n",
    "                if score > (BLOCK_SIZE**2) / 2:\n",
    "                    grid.append(1.0)\n",
    "                else:\n",
    "                    grid.append(0.0)\n",
    "            \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        road_bins.append(torch.as_tensor(grid))\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(road_bins), torch.stack(road_maps)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "aug_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        torchvision.transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=aug_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, shuffle=True, num_workers=10, collate_fn=segmentation_collate_fn)\n",
    "\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1, shuffle=True, num_workers=2, collate_fn=segmentation_collate_fn)\n",
    "\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, batch_size=2, shuffle=True, num_workers=2, collate_fn=segmentation_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Segmentation Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.segmentation = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 25600)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_images = x.shape[1]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        \n",
    "        # Reshape to feed in images\n",
    "        x = x.reshape(-1, channels, height, width)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.segmentation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logic, return average loss over training set after each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch, log_file_path, log_interval = 250):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Number correct for accuracy\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Train loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # Loop through examples\n",
    "    for batch_idx, (images, bins, road_map) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # Send data and target to device\n",
    "        data, target = images.to(device), bins.to(device)\n",
    "        \n",
    "        # Zero out optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass data through model - right now only segmentation\n",
    "        output = model(data)\n",
    "        # Should be batch_size X 800 X 800\n",
    "        output = output.squeeze()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            f.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    \n",
    "    # Average train loss\n",
    "    average_train_loss = train_loss / len(train_loader)\n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_bins(bins, block_size, threshold):\n",
    "    road_map = torch.zeros((800, 800))\n",
    "    idx = 0\n",
    "    for x in range(0, 800, block_size):\n",
    "        for y in range(0, 800, block_size):\n",
    "            road_map[x:x+block_size, y:y+block_size] = bins[idx]\n",
    "            idx += 1\n",
    "    return road_map > threshold\n",
    "\n",
    "\n",
    "# Define test method\n",
    "def test(model, device, test_loader, log_file_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "#     thresholds = [0.42]\n",
    "    \n",
    "#     threat_scores = torch.zeros(1)\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, bins, road_map) in enumerate(test_loader):\n",
    "\n",
    "            # Send data and target to device\n",
    "            data, target= images.to(device), bins.to(device)\n",
    "\n",
    "            # Pass data through model - right now only segmentation\n",
    "            output = model(data)\n",
    "            # Should be batch_size X 6400\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Now squeeze for reconstruction\n",
    "#             output = output.squeeze()\n",
    "            \n",
    "            # Compute threat score at 4 different thresholds\n",
    "#              for idx in range(len(thresholds)):\n",
    "#                 reconstructed_road_map = reconstruct_from_bins(output, 5, thresholds[idx]).cpu()\n",
    "#                 ts_road_map = compute_ts_road_map(reconstructed_road_map, road_map)\n",
    "#                 threat_scores[idx] += ts_road_map\n",
    "         \n",
    "    # Compute the average test_loss\n",
    "    # avg_test_loss = TODO\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Compute average threat scores\n",
    "#     avg_threat_scores = threat_scores / len(test_loader)\n",
    "    \n",
    "#     print('\\Threat scores: \\t {}:{}\\n'.format(thresholds[0], avg_threat_scores[0]))\n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    \n",
    "#     f.write('\\Threat scores: \\t {}:{}\\n'.format(thresholds[0], avg_threat_scores[0]))\n",
    "    f.write('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    f.close()\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation model\n",
    "model = SimpleModel().to(device)\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "# Scheduler\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.25, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3276 (0%)]\tLoss: 0.709417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-84ec77cc3737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     scheduler.step(val_threat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-30597827bdd3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_file_path, log_interval)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Make a step with the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Print loss (uncomment lines below once implemented)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = \"/scratch/brs426/ben_models/test.log\"\n",
    "best_val_loss = 100\n",
    "save_path = \"/scratch/brs426/ben_models/test.p\"\n",
    "epochs = 40\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train model\n",
    "    loss = train(model, device, trainloader, optimizer, epoch, f)\n",
    "    val_loss = test(model, device, valloader, f)\n",
    "#     scheduler.step(val_threat)\n",
    "    #Save model\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threat Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0059812068939208984s\n",
      "0.01024937629699707s\n",
      "0.014714479446411133s\n",
      "0.01437997817993164s\n",
      "0.014620065689086914s\n",
      "0.006680011749267578s\n",
      "0.014737129211425781s\n",
      "0.001615285873413086s\n",
      "0.013742446899414062s\n",
      "0.01269841194152832s\n",
      "0.014742136001586914s\n",
      "0.005669355392456055s\n",
      "0.01585078239440918s\n",
      "0.015659093856811523s\n",
      "0.014640569686889648s\n",
      "0.014751672744750977s\n",
      "0.01471090316772461s\n",
      "0.00287628173828125s\n",
      "0.014663219451904297s\n",
      "0.005843400955200195s\n",
      "0.012358427047729492s\n",
      "0.01502370834350586s\n",
      "0.014719009399414062s\n",
      "0.014740705490112305s\n",
      "0.014388322830200195s\n",
      "0.01466989517211914s\n",
      "0.014648675918579102s\n",
      "0.012973785400390625s\n",
      "0.01464390754699707s\n",
      "0.014610528945922852s\n",
      "0.014638423919677734s\n",
      "0.021623611450195312s\n",
      "0.013297319412231445s\n",
      "0.022171735763549805s\n",
      "0.014658212661743164s\n",
      "0.014745235443115234s\n",
      "0.016164541244506836s\n",
      "0.012195587158203125s\n",
      "0.014753580093383789s\n",
      "0.006547451019287109s\n",
      "0.014774560928344727s\n",
      "0.00626683235168457s\n",
      "0.014811992645263672s\n",
      "0.012063264846801758s\n",
      "0.014634132385253906s\n",
      "0.0037424564361572266s\n",
      "0.014597177505493164s\n",
      "0.0017247200012207031s\n",
      "0.01654505729675293s\n",
      "0.014695167541503906s\n",
      "0.014672279357910156s\n",
      "0.016069889068603516s\n",
      "0.01469278335571289s\n",
      "0.01618194580078125s\n",
      "0.01118612289428711s\n",
      "0.015790939331054688s\n",
      "0.01482844352722168s\n",
      "0.01566767692565918s\n",
      "0.01482534408569336s\n",
      "0.014687538146972656s\n",
      "0.014356136322021484s\n",
      "0.015859603881835938s\n",
      "0.018688440322875977s\n",
      "0.01614832878112793s\n",
      "0.011984109878540039s\n",
      "0.014764070510864258s\n",
      "0.0060214996337890625s\n",
      "0.014639854431152344s\n",
      "0.01194143295288086s\n",
      "0.014776945114135742s\n",
      "0.0030465126037597656s\n",
      "0.010132789611816406s\n",
      "0.002956390380859375s\n",
      "0.011104345321655273s\n",
      "0.0018906593322753906s\n",
      "0.010528087615966797s\n",
      "0.009855508804321289s\n",
      "0.010529518127441406s\n",
      "0.0037343502044677734s\n",
      "0.010458230972290039s\n",
      "0.0035400390625s\n",
      "0.010933399200439453s\n",
      "0.00325775146484375s\n",
      "0.011265277862548828s\n",
      "0.0027618408203125s\n",
      "0.010542631149291992s\n",
      "0.0033447742462158203s\n",
      "0.010315656661987305s\n",
      "0.0035507678985595703s\n",
      "0.014698028564453125s\n",
      "0.014685392379760742s\n",
      "0.013165950775146484s\n",
      "0.0025892257690429688s\n",
      "0.014787435531616211s\n",
      "0.006349086761474609s\n",
      "0.016032695770263672s\n",
      "0.001986980438232422s\n",
      "0.0146636962890625s\n",
      "0.0063250064849853516s\n",
      "0.014671087265014648s\n",
      "0.003295421600341797s\n",
      "0.010027170181274414s\n",
      "0.002202272415161133s\n",
      "0.010576486587524414s\n",
      "0.0033500194549560547s\n",
      "0.01100921630859375s\n",
      "0.002710580825805664s\n",
      "0.010675668716430664s\n",
      "0.003225564956665039s\n",
      "0.010737180709838867s\n",
      "0.0032529830932617188s\n",
      "0.01091456413269043s\n",
      "0.0031478404998779297s\n",
      "0.010091304779052734s\n",
      "0.0006742477416992188s\n",
      "0.010512113571166992s\n",
      "0.011554956436157227s\n",
      "0.010778188705444336s\n",
      "0.0025949478149414062s\n",
      "0.010655641555786133s\n",
      "0.0032739639282226562s\n",
      "0.0113067626953125s\n",
      "0.003365039825439453s\n",
      "0.010775089263916016s\n",
      "0.0036525726318359375s\n",
      "0.010698080062866211s\n",
      "0.0031790733337402344s\n",
      "0.013340473175048828s\n",
      "0.0016198158264160156s\n",
      "0.010119438171386719s\n",
      "0.0033347606658935547s\n",
      "0.01077413558959961s\n",
      "0.0035104751586914062s\n",
      "0.010958194732666016s\n",
      "0.002722024917602539s\n",
      "0.010759115219116211s\n",
      "0.0027990341186523438s\n",
      "0.010689496994018555s\n",
      "0.0019049644470214844s\n",
      "0.010573625564575195s\n",
      "0.003262042999267578s\n",
      "0.011314153671264648s\n",
      "0.003078460693359375s\n",
      "0.012255430221557617s\n",
      "0.0021123886108398438s\n",
      "0.010326862335205078s\n",
      "0.0024127960205078125s\n",
      "0.010901212692260742s\n",
      "0.015395164489746094s\n",
      "0.010448455810546875s\n",
      "0.0032417774200439453s\n",
      "0.010509014129638672s\n",
      "0.003312349319458008s\n",
      "0.010713338851928711s\n",
      "0.0009043216705322266s\n",
      "0.014751911163330078s\n",
      "0.006628990173339844s\n",
      "0.014589786529541016s\n",
      "0.014616250991821289s\n",
      "0.014765501022338867s\n",
      "0.01907038688659668s\n",
      "0.016430139541625977s\n",
      "0.015599489212036133s\n",
      "0.013624191284179688s\n",
      "0.0010344982147216797s\n",
      "0.0147552490234375s\n",
      "0.006345987319946289s\n",
      "0.014642477035522461s\n",
      "0.0033228397369384766s\n",
      "0.015317678451538086s\n",
      "0.003231525421142578s\n",
      "0.014656782150268555s\n",
      "0.00660252571105957s\n",
      "0.014673948287963867s\n",
      "0.008842945098876953s\n",
      "0.01462411880493164s\n",
      "0.0063512325286865234s\n",
      "0.014527082443237305s\n",
      "0.006488800048828125s\n",
      "0.014802694320678711s\n",
      "0.014889001846313477s\n",
      "0.01650261878967285s\n",
      "0.021607398986816406s\n",
      "0.01653432846069336s\n",
      "0.014690399169921875s\n",
      "0.002047300338745117s\n",
      "0.021634340286254883s\n",
      "0.014722585678100586s\n",
      "0.014759540557861328s\n",
      "0.014885425567626953s\n",
      "0.014806032180786133s\n",
      "0.015247821807861328s\n",
      "0.014751434326171875s\n",
      "0.014728307723999023s\n",
      "0.014773130416870117s\n",
      "0.0012402534484863281s\n",
      "0.014808177947998047s\n",
      "0.00170135498046875s\n",
      "0.011411666870117188s\n",
      "0.002617359161376953s\n",
      "0.010683298110961914s\n",
      "0.0024433135986328125s\n",
      "0.010030746459960938s\n",
      "0.003428936004638672s\n",
      "0.011553049087524414s\n",
      "0.0027599334716796875s\n",
      "0.010178089141845703s\n",
      "0.0023140907287597656s\n",
      "0.010719776153564453s\n",
      "0.003583669662475586s\n",
      "0.011595726013183594s\n",
      "0.0019502639770507812s\n",
      "0.010543584823608398s\n",
      "0.002722501754760742s\n",
      "0.010892868041992188s\n",
      "0.00353240966796875s\n",
      "0.014758110046386719s\n",
      "0.0061414241790771484s\n",
      "0.014801740646362305s\n",
      "0.012370109558105469s\n",
      "0.015779972076416016s\n",
      "0.01484227180480957s\n",
      "0.01467275619506836s\n",
      "0.001886129379272461s\n",
      "0.014771223068237305s\n",
      "0.003702878952026367s\n",
      "0.01465153694152832s\n",
      "0.014381170272827148s\n",
      "0.016353368759155273s\n",
      "0.001251220703125s\n",
      "0.015404462814331055s\n",
      "0.011167526245117188s\n",
      "0.014789581298828125s\n",
      "0.014669179916381836s\n",
      "0.014695882797241211s\n",
      "0.014672994613647461s\n",
      "0.014673948287963867s\n",
      "0.01066279411315918s\n",
      "0.011516571044921875s\n",
      "0.003273487091064453s\n",
      "0.013936758041381836s\n",
      "0.002267122268676758s\n",
      "0.01012420654296875s\n",
      "0.010812997817993164s\n",
      "0.010629415512084961s\n",
      "0.0033385753631591797s\n",
      "0.010654926300048828s\n",
      "0.003345012664794922s\n",
      "0.010390520095825195s\n",
      "0.011943817138671875s\n",
      "0.014798402786254883s\n",
      "0.014998435974121094s\n",
      "Threshold 0.4: tensor([0.8212])\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_from_bins(bins, block_size, threshold):\n",
    "    road_map = torch.zeros((800, 800))\n",
    "    idx = 0\n",
    "    for x in range(0, 800, block_size):\n",
    "        for y in range(0, 800, block_size):\n",
    "            road_map[x:x+block_size, y:y+block_size] = bins[idx]\n",
    "            idx += 1\n",
    "    return road_map > threshold\n",
    "\n",
    "\n",
    "def faster_reconstruct_from_bins(bins, block_size, threshold):\n",
    "    downsampled_roadmap = bins.reshape(1, 1, 800 // block_size, 800 // block_size)\n",
    "    up = nn.Upsample(size=(800, 800), mode='nearest')\n",
    "    road_map = up(downsampled_roadmap)\n",
    "    return road_map > threshold\n",
    "\n",
    "# Predicting everything as road\n",
    "model = SimpleModel().to(device)\n",
    "model.load_state_dict(torch.load(\"/scratch/brs426/ben_models/simple_model_block_size_5_test_threat_alone.p\"))\n",
    "model.eval()\n",
    "\n",
    "thresholds = [0.4]\n",
    "\n",
    "threat_scores = torch.zeros(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, bins, road_map) in enumerate(valloader):\n",
    "\n",
    "        # Send data and target to device\n",
    "        data, target= images.to(device), bins.to(device)\n",
    "\n",
    "        # Pass data through model - right now only segmentation\n",
    "        output = model(data)\n",
    "        # Should be batch_size X 6400\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = F.binary_cross_entropy(output, target)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "        # Now squeeze for reconstruction\n",
    "        output = output.squeeze()\n",
    "\n",
    "        # Compute threat score at 4 different thresholds\n",
    "        for idx in range(len(thresholds)):\n",
    "            #reconstructed_road_map = reconstruct_from_bins(output, 5, thresholds[idx]).cpu()\n",
    "            start = time.time()\n",
    "            fast_reconstruction = faster_reconstruct_from_bins(output, 5, thresholds[idx]).cpu()\n",
    "            end = time.time()\n",
    "            print(\"{}s\".format(end - start))\n",
    "            #print(\"Total\", torch.sum(reconstructed_road_map == fast_reconstruction))\n",
    "            ts_road_map = compute_ts_road_map(fast_reconstruction, road_map)\n",
    "            threat_scores[idx] += ts_road_map\n",
    "\n",
    "avg_threat_scores = threat_scores / len(valloader)\n",
    "print(\"Threshold 0.4: {}\".format(avg_threat_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_bins(bins, block_size, threshold):\n",
    "    road_map = torch.zeros((800, 800))\n",
    "    idx = 0\n",
    "    for x in range(0, 800, block_size):\n",
    "        for y in range(0, 800, block_size):\n",
    "            road_map[x:x+block_size, y:y+block_size] = bins[idx]\n",
    "            idx += 1\n",
    "    return road_map > threshold\n",
    "\n",
    "def faster_reconstruct_from_bins(bins, block_size, threshold):\n",
    "    downsampled_roadmap = bins.reshape(800 / block_size, 800 / block_size)\n",
    "    road_map = nn.Upsample(size=(800, 800), mode='nearest')\n",
    "    return road_map > threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
