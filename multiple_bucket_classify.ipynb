{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/vr1059/self-driving-data/data'\n",
    "annotation_csv = '/scratch/vr1059/self-driving-data/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.degrees(np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mappings from bin to class, and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_label = 1\n",
    "# class_dict = dict()\n",
    "# reverse_class_dict = []\n",
    "# reverse_class_dict.append((-100, -100))\n",
    "# for i in range(400, 800, 50):\n",
    "#     for j in range(100, 700, 50):\n",
    "#         class_dict[(i, j)] = class_label\n",
    "#         class_label += 1\n",
    "#         reverse_class_dict.append((i, j))\n",
    "        \n",
    "# class_dict[(-100, -100)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 1\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "reverse_class_dict.append((-100, -100))\n",
    "for i in range(400, 800, 50):\n",
    "    for j in range(100, 700, 50):\n",
    "        top_right_corner = (i+50, j)\n",
    "        bottom_right_corner = (i+50, j+50)\n",
    "        \n",
    "        v1 = np.array([bottom_right_corner[0] - 400, 800 - bottom_right_corner[1] - 400])\n",
    "        v3 = np.array([top_right_corner[0] - 400, 800 - top_right_corner[1] - 400])\n",
    "        v2 = np.array([2, 0])\n",
    "        \n",
    "        if abs(angle_between(v1, v2)) <= 35 or abs(angle_between(v3, v2)) <= 35:\n",
    "            class_dict[(i, j)] = class_label\n",
    "            class_label += 1\n",
    "            reverse_class_dict.append((i, j))\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "class_dict[(-100, -100)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_collate_fn(batch):\n",
    "    front_imgs = []\n",
    "    front_right_imgs = []\n",
    "    front_left_imgs = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        # input\n",
    "        front_left_imgs.append(torch.as_tensor(x[0][0]))\n",
    "        front_imgs.append(torch.as_tensor(x[0][1]))\n",
    "        front_right_imgs.append(torch.as_tensor(x[0][2]))\n",
    "        road_imgs.append(torch.as_tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(59)\n",
    "        counts = np.zeros(20)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "            # Get bird's eye view coordinates \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "            \n",
    "            # Get the top of the bounding box (top-center) point. Covers the edge case when the top of the car\n",
    "            # is peeking into the image, but we can't . ...Although...\n",
    "            # If we can only see the top part of the car, that means there's another view that \n",
    "            # has a much bigger slice of it, and can capture it. \n",
    "            # So I actually don't need to do this. And it will prob be helpful,\n",
    "            # because why make this view try very hard to learn this, when another view can handle it just fine. \n",
    "            # Eh, I'll focus on this later. I just want to get this working for now. \n",
    "            if xs[2] - xs[0] > 5:\n",
    "                top_center_x, top_center_y = 0.5*(xs[2] + xs[3]), 0.5*(ys[2] + ys[3])\n",
    "            else:\n",
    "                top_center_x, top_center_y = 0.5*(xs[0] + xs[1]), 0.5*(ys[0] + ys[1])\n",
    "                \n",
    "            # We do (800 - top_center_y) because matplotlib y-axis starts from the top. \n",
    "            v1 = np.array([top_center_x - 400, 800 - top_center_y - 400])\n",
    "            v2 = np.array([2, 0])\n",
    "            \n",
    "            if abs(angle_between(v1, v2)) <= 35 and x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "                current_bbs.append((xs, ys))\n",
    "                # we're in the bucket of the front_img\n",
    "                top_of_car = (top_center_x.item(), top_center_y.item())\n",
    "                key = (round_down(top_of_car[0]), round_down(top_of_car[1]))\n",
    "                if key not in class_dict:\n",
    "                    print(key)\n",
    "                bin_id = class_dict[key]\n",
    "                bins[bin_id] = 1\n",
    "                count += 1\n",
    "                \n",
    "        target.append(torch.as_tensor(bins))\n",
    "        counts[count] = 1\n",
    "        target_counts.append(torch.as_tensor(counts))\n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(front_imgs), torch.stack(target), torch.stack(road_imgs), bbs, torch.stack(target_counts)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def front_collate_fn(batch):\n",
    "    front_imgs = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        # input\n",
    "        flip_flag = False\n",
    "        # Flipping with probability 0.5\n",
    "        if np.random.choice([0, 1]):\n",
    "            flip_flag = True\n",
    "            img = x[0][1].numpy()\n",
    "            flipped_img = np.fliplr(img.transpose(1, 2, 0)).transpose(2, 0, 1)\n",
    "            front_imgs.append(torch.as_tensor(flipped_img.copy()))\n",
    "        else:\n",
    "            front_imgs.append(torch.tensor(x[0][1]))\n",
    "        \n",
    "        road_imgs.append(torch.as_tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(59)\n",
    "        counts = np.zeros(20)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "            # Get bird's eye view coordinates \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "            \n",
    "            # Get the top of the bounding box (top-center) point. Covers the edge case when the top of the car\n",
    "            # is peeking into the image, but we can't . ...Although...\n",
    "            # If we can only see the top part of the car, that means there's another view that \n",
    "            # has a much bigger slice of it, and can capture it. \n",
    "            # So I actually don't need to do this. And it will prob be helpful,\n",
    "            # because why make this view try very hard to learn this, when another view can handle it just fine. \n",
    "            # Eh, I'll focus on this later. I just want to get this working for now. \n",
    "            if xs[2] - xs[0] > 5:\n",
    "                top_center_x, top_center_y = 0.5*(xs[2] + xs[3]), 0.5*(ys[2] + ys[3])\n",
    "            else:\n",
    "                top_center_x, top_center_y = 0.5*(xs[0] + xs[1]), 0.5*(ys[0] + ys[1])\n",
    "                \n",
    "            # We do (800 - top_center_y) because matplotlib y-axis starts from the top. \n",
    "            v1 = np.array([top_center_x - 400, 800 - top_center_y - 400])\n",
    "            v2 = np.array([2, 0])\n",
    "            \n",
    "            if abs(angle_between(v1, v2)) <= 35 and x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "                current_bbs.append((xs, ys))\n",
    "                # we're in the bucket of the front_img\n",
    "                if flip_flag:\n",
    "                    # vertically flip where the top_center coordinates of the bounding box are. \n",
    "                    top_of_car = (top_center_x.item(), 800 - top_center_y.item())\n",
    "                else:\n",
    "                    top_of_car = (top_center_x.item(), top_center_y.item())\n",
    "                key = (round_down(top_of_car[0]), round_down(top_of_car[1]))\n",
    "                if key not in class_dict:\n",
    "                    print(key)\n",
    "                bin_id = class_dict[key]\n",
    "                bins[bin_id] = 1\n",
    "                count += 1\n",
    "                \n",
    "        target.append(torch.as_tensor(bins))\n",
    "        counts[count] = 1\n",
    "        target_counts.append(torch.as_tensor(counts))\n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(front_imgs), torch.stack(target), torch.stack(road_imgs), bbs, torch.stack(target_counts)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.RandomAffine(8),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=64, num_workers=2, shuffle=True, collate_fn=front_collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=64, num_workers=2, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 64)),\n",
    "            ('relu', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(64, 59)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(64, 20))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if(\"bn\" not in name):\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "# for layer in unfreeze_layers:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "        \n",
    "model = model.to(device)\n",
    "location_criterion = nn.BCEWithLogitsLoss()\n",
    "count_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=64, num_workers=2, shuffle=True, collate_fn=front_collate_fn)\n",
    "    \n",
    "    train_losses = []\n",
    "    loc_losses = []\n",
    "    count_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        y_hat, y_count = model(sample)\n",
    "        \n",
    "        loc_loss = location_criterion(y_hat, target.float())\n",
    "        count_loss = count_criterion(y_count, target_count.float())\n",
    "        loss = 15 * loc_loss + count_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        loc_losses.append(loc_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                10. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    print(\"Average Train Loc Epoch Loss: \", np.mean(loc_losses))\n",
    "    print(\"Average Train Count Epoch Loss: \", np.mean(count_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    loc_losses = []\n",
    "    count_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat, y_count = model(sample)\n",
    "            \n",
    "            loc_loss = location_criterion(y_hat, target.float())\n",
    "            count_loss = count_criterion(y_count, target_count.float())\n",
    "            loss = 15 * loc_loss + count_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            loc_losses.append(loc_loss.item())\n",
    "            count_losses.append(count_loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Loc Epoch Loss: \", np.mean(loc_losses))\n",
    "    print(\"Average Validation Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), 'best_val_loss_class_counts_flip.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2772 (0%)]\tLoss: 10.990244\n",
      "Train Epoch: 0 [640/2772 (2%)]\tLoss: 6.515646\n",
      "Train Epoch: 0 [1280/2772 (5%)]\tLoss: 5.117235\n",
      "Train Epoch: 0 [1920/2772 (7%)]\tLoss: 4.833751\n",
      "Train Epoch: 0 [2560/2772 (9%)]\tLoss: 4.444820\n",
      "\n",
      "Average Train Epoch Loss:  5.653563526543704\n",
      "Average Train Loc Epoch Loss:  0.3287117200141603\n",
      "Average Train Count Epoch Loss:  0.7228877727280963\n",
      "Average Validation Epoch Loss:  6.0960118770599365\n",
      "Average Validation Loc Epoch Loss:  0.3707309663295746\n",
      "Average Validation Count Epoch Loss:  0.5350473895668983\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2772 (0%)]\tLoss: 3.713620\n",
      "Train Epoch: 1 [640/2772 (2%)]\tLoss: 3.663109\n",
      "Train Epoch: 1 [1280/2772 (5%)]\tLoss: 3.532333\n",
      "Train Epoch: 1 [1920/2772 (7%)]\tLoss: 2.969621\n",
      "Train Epoch: 1 [2560/2772 (9%)]\tLoss: 3.084568\n",
      "\n",
      "Average Train Epoch Loss:  3.35056273503737\n",
      "Average Train Loc Epoch Loss:  0.20161043039777063\n",
      "Average Train Count Epoch Loss:  0.3264062987132506\n",
      "Average Validation Epoch Loss:  5.452432334423065\n",
      "Average Validation Loc Epoch Loss:  0.347235020250082\n",
      "Average Validation Count Epoch Loss:  0.24390708096325397\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/2772 (0%)]\tLoss: 3.555267\n",
      "Train Epoch: 2 [640/2772 (2%)]\tLoss: 2.986467\n",
      "Train Epoch: 2 [1280/2772 (5%)]\tLoss: 3.144715\n",
      "Train Epoch: 2 [1920/2772 (7%)]\tLoss: 2.959774\n",
      "Train Epoch: 2 [2560/2772 (9%)]\tLoss: 2.658507\n",
      "\n",
      "Average Train Epoch Loss:  2.976323512467471\n",
      "Average Train Loc Epoch Loss:  0.18608649921688167\n",
      "Average Train Count Epoch Loss:  0.18502602184360678\n",
      "Average Validation Epoch Loss:  4.799320757389069\n",
      "Average Validation Loc Epoch Loss:  0.306241899728775\n",
      "Average Validation Count Epoch Loss:  0.20569237880408764\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/2772 (0%)]\tLoss: 2.862673\n",
      "Train Epoch: 3 [640/2772 (2%)]\tLoss: 3.102355\n",
      "Train Epoch: 3 [1280/2772 (5%)]\tLoss: 2.790624\n",
      "Train Epoch: 3 [1920/2772 (7%)]\tLoss: 3.438823\n",
      "Train Epoch: 3 [2560/2772 (9%)]\tLoss: 3.024902\n",
      "\n",
      "Average Train Epoch Loss:  2.841462184082378\n",
      "Average Train Loc Epoch Loss:  0.17874846878376874\n",
      "Average Train Count Epoch Loss:  0.16023516282439232\n",
      "Average Validation Epoch Loss:  4.680347561836243\n",
      "Average Validation Loc Epoch Loss:  0.2985377460718155\n",
      "Average Validation Count Epoch Loss:  0.2022814229130745\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/2772 (0%)]\tLoss: 2.768582\n",
      "Train Epoch: 4 [640/2772 (2%)]\tLoss: 3.426791\n",
      "Train Epoch: 4 [1280/2772 (5%)]\tLoss: 2.472950\n",
      "Train Epoch: 4 [1920/2772 (7%)]\tLoss: 2.578110\n",
      "Train Epoch: 4 [2560/2772 (9%)]\tLoss: 2.814022\n",
      "\n",
      "Average Train Epoch Loss:  2.8011733239347283\n",
      "Average Train Loc Epoch Loss:  0.176446832716465\n",
      "Average Train Count Epoch Loss:  0.1544708213345571\n",
      "Average Validation Epoch Loss:  4.792924880981445\n",
      "Average Validation Loc Epoch Loss:  0.3055608458817005\n",
      "Average Validation Count Epoch Loss:  0.20951208844780922\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/2772 (0%)]\tLoss: 2.713377\n",
      "Train Epoch: 5 [640/2772 (2%)]\tLoss: 2.989599\n",
      "Train Epoch: 5 [1280/2772 (5%)]\tLoss: 3.074363\n",
      "Train Epoch: 5 [1920/2772 (7%)]\tLoss: 2.667627\n",
      "Train Epoch: 5 [2560/2772 (9%)]\tLoss: 2.936692\n",
      "\n",
      "Average Train Epoch Loss:  2.7104508768428457\n",
      "Average Train Loc Epoch Loss:  0.17073033038865437\n",
      "Average Train Count Epoch Loss:  0.14949592914093623\n",
      "Average Validation Epoch Loss:  4.775478363037109\n",
      "Average Validation Loc Epoch Loss:  0.3044836111366749\n",
      "Average Validation Count Epoch Loss:  0.20822415873408318\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/2772 (0%)]\tLoss: 2.355892\n",
      "Train Epoch: 6 [640/2772 (2%)]\tLoss: 2.906290\n",
      "Train Epoch: 6 [1280/2772 (5%)]\tLoss: 2.668070\n",
      "Train Epoch: 6 [1920/2772 (7%)]\tLoss: 2.897006\n",
      "Train Epoch: 6 [2560/2772 (9%)]\tLoss: 2.247615\n",
      "\n",
      "Average Train Epoch Loss:  2.665528904307972\n",
      "Average Train Loc Epoch Loss:  0.1678734631700949\n",
      "Average Train Count Epoch Loss:  0.14742695844986223\n",
      "Average Validation Epoch Loss:  4.61064088344574\n",
      "Average Validation Loc Epoch Loss:  0.29236943274736404\n",
      "Average Validation Count Epoch Loss:  0.22509931214153767\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/2772 (0%)]\tLoss: 2.491114\n",
      "Train Epoch: 7 [640/2772 (2%)]\tLoss: 2.509985\n",
      "Train Epoch: 7 [1280/2772 (5%)]\tLoss: 2.414754\n",
      "Train Epoch: 7 [1920/2772 (7%)]\tLoss: 2.905150\n",
      "Train Epoch: 7 [2560/2772 (9%)]\tLoss: 2.576325\n",
      "\n",
      "Average Train Epoch Loss:  2.5817325331948022\n",
      "Average Train Loc Epoch Loss:  0.16232918575406075\n",
      "Average Train Count Epoch Loss:  0.14679476110772652\n",
      "Average Validation Epoch Loss:  4.608488380908966\n",
      "Average Validation Loc Epoch Loss:  0.2933262772858143\n",
      "Average Validation Count Epoch Loss:  0.20859413780272007\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/2772 (0%)]\tLoss: 2.257100\n",
      "Train Epoch: 8 [640/2772 (2%)]\tLoss: 2.436040\n",
      "Train Epoch: 8 [1280/2772 (5%)]\tLoss: 2.776478\n",
      "Train Epoch: 8 [1920/2772 (7%)]\tLoss: 2.632962\n",
      "Train Epoch: 8 [2560/2772 (9%)]\tLoss: 2.915891\n",
      "\n",
      "Average Train Epoch Loss:  2.5026538209481672\n",
      "Average Train Loc Epoch Loss:  0.15721542963927443\n",
      "Average Train Count Epoch Loss:  0.1444223648445173\n",
      "Average Validation Epoch Loss:  4.335243225097656\n",
      "Average Validation Loc Epoch Loss:  0.27486685290932655\n",
      "Average Validation Count Epoch Loss:  0.2122402787208557\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/2772 (0%)]\tLoss: 2.614697\n",
      "Train Epoch: 9 [640/2772 (2%)]\tLoss: 2.317453\n",
      "Train Epoch: 9 [1280/2772 (5%)]\tLoss: 2.457232\n",
      "Train Epoch: 9 [1920/2772 (7%)]\tLoss: 2.272856\n",
      "Train Epoch: 9 [2560/2772 (9%)]\tLoss: 2.539774\n",
      "\n",
      "Average Train Epoch Loss:  2.4533475529063833\n",
      "Average Train Loc Epoch Loss:  0.1540427908978679\n",
      "Average Train Count Epoch Loss:  0.14270568401976066\n",
      "Average Validation Epoch Loss:  4.200329959392548\n",
      "Average Validation Loc Epoch Loss:  0.26508430019021034\n",
      "Average Validation Count Epoch Loss:  0.2240653894841671\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/2772 (0%)]\tLoss: 2.078136\n",
      "Train Epoch: 10 [640/2772 (2%)]\tLoss: 2.170031\n",
      "Train Epoch: 10 [1280/2772 (5%)]\tLoss: 2.496509\n",
      "Train Epoch: 10 [1920/2772 (7%)]\tLoss: 2.123135\n",
      "Train Epoch: 10 [2560/2772 (9%)]\tLoss: 2.555698\n",
      "\n",
      "Average Train Epoch Loss:  2.3965169082988393\n",
      "Average Train Loc Epoch Loss:  0.15028003552420574\n",
      "Average Train Count Epoch Loss:  0.14231639727950096\n",
      "Average Validation Epoch Loss:  4.2822107672691345\n",
      "Average Validation Loc Epoch Loss:  0.271354952827096\n",
      "Average Validation Count Epoch Loss:  0.21188639476895332\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [0/2772 (0%)]\tLoss: 2.407726\n",
      "Train Epoch: 11 [640/2772 (2%)]\tLoss: 1.877274\n",
      "Train Epoch: 11 [1280/2772 (5%)]\tLoss: 2.553858\n",
      "Train Epoch: 11 [1920/2772 (7%)]\tLoss: 2.584764\n",
      "Train Epoch: 11 [2560/2772 (9%)]\tLoss: 2.892630\n",
      "\n",
      "Average Train Epoch Loss:  2.326546110890128\n",
      "Average Train Loc Epoch Loss:  0.1456779038364237\n",
      "Average Train Count Epoch Loss:  0.1413775770501657\n",
      "Average Validation Epoch Loss:  4.371231257915497\n",
      "Average Validation Loc Epoch Loss:  0.2775772586464882\n",
      "Average Validation Count Epoch Loss:  0.2075723558664322\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/2772 (0%)]\tLoss: 2.177293\n",
      "Train Epoch: 12 [640/2772 (2%)]\tLoss: 2.494504\n",
      "Train Epoch: 12 [1280/2772 (5%)]\tLoss: 2.376860\n",
      "Train Epoch: 12 [1920/2772 (7%)]\tLoss: 2.183812\n",
      "Train Epoch: 12 [2560/2772 (9%)]\tLoss: 2.138409\n",
      "\n",
      "Average Train Epoch Loss:  2.2680765905163507\n",
      "Average Train Loc Epoch Loss:  0.1418877372687513\n",
      "Average Train Count Epoch Loss:  0.13976054858754983\n",
      "Average Validation Epoch Loss:  4.388655304908752\n",
      "Average Validation Loc Epoch Loss:  0.277799554169178\n",
      "Average Validation Count Epoch Loss:  0.2216620035469532\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [0/2772 (0%)]\tLoss: 2.326896\n",
      "Train Epoch: 13 [640/2772 (2%)]\tLoss: 2.456656\n",
      "Train Epoch: 13 [1280/2772 (5%)]\tLoss: 2.307317\n",
      "Train Epoch: 13 [1920/2772 (7%)]\tLoss: 1.865734\n",
      "Train Epoch: 13 [2560/2772 (9%)]\tLoss: 2.472535\n",
      "\n",
      "Average Train Epoch Loss:  2.232106859033758\n",
      "Average Train Loc Epoch Loss:  0.13953970643607053\n",
      "Average Train Count Epoch Loss:  0.1390112474222075\n",
      "Average Validation Epoch Loss:  4.271104961633682\n",
      "Average Validation Loc Epoch Loss:  0.27006446942687035\n",
      "Average Validation Count Epoch Loss:  0.22013788670301437\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [0/2772 (0%)]\tLoss: 2.069774\n",
      "Train Epoch: 14 [640/2772 (2%)]\tLoss: 2.386886\n",
      "Train Epoch: 14 [1280/2772 (5%)]\tLoss: 2.579551\n",
      "Train Epoch: 14 [1920/2772 (7%)]\tLoss: 2.041886\n",
      "Train Epoch: 14 [2560/2772 (9%)]\tLoss: 2.358266\n",
      "\n",
      "Average Train Epoch Loss:  2.1872888071970507\n",
      "Average Train Loc Epoch Loss:  0.13661059026013722\n",
      "Average Train Count Epoch Loss:  0.13812994279644705\n",
      "Average Validation Epoch Loss:  4.2923479080200195\n",
      "Average Validation Loc Epoch Loss:  0.2700642719864845\n",
      "Average Validation Count Epoch Loss:  0.2413838766515255\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [0/2772 (0%)]\tLoss: 2.221248\n",
      "Train Epoch: 15 [640/2772 (2%)]\tLoss: 2.251726\n",
      "Train Epoch: 15 [1280/2772 (5%)]\tLoss: 2.236088\n",
      "Train Epoch: 15 [1920/2772 (7%)]\tLoss: 2.195569\n",
      "Train Epoch: 15 [2560/2772 (9%)]\tLoss: 1.826646\n",
      "\n",
      "Average Train Epoch Loss:  2.137532724575563\n",
      "Average Train Loc Epoch Loss:  0.13334406912326813\n",
      "Average Train Count Epoch Loss:  0.1373716794293035\n",
      "Average Validation Epoch Loss:  4.302679061889648\n",
      "Average Validation Loc Epoch Loss:  0.2725216820836067\n",
      "Average Validation Count Epoch Loss:  0.21485375985503197\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [0/2772 (0%)]\tLoss: 1.854321\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed random seeds. Also refresh train dataset and dataloader (with collate_fn called) every epoch. \n",
    "# Testing if this helps generalization, rather than having the same 3000-sized dataset every time. \n",
    "\n",
    "# 16.1 best val loss (epoch 14)\n",
    "# I wonder if decreasing the number of classes would help. \n",
    "# ResNet pre-trained=True, freeze everything except last two layers, differential learning. \n",
    "\n",
    "# 0.31 best val loss\n",
    "# 0.327 best val loss, ResNet 34 with L2 penalty (0.5)\n",
    "\n",
    "# 0.2999 best val loss from combined classify + counts. \n",
    "\n",
    "# 0.286 best val loc loss, 2.79 best val count loss\n",
    "# 0.275 best val loc loss, 3.23 best val count loss\n",
    "\n",
    "# 0.288 best val loc loss, 2.82 best val count loss\n",
    "\n",
    "# 0.34 best val loc loss, 2.73 best val count loss\n",
    "\n",
    "# 0.265 best val loc loss, 2.708 best val count loss\n",
    "\n",
    "# 0.268, 2.59 (hue -0.5)\n",
    "# 0.260 2.45 (hue, 12 factor on loc lsos)\n",
    "\n",
    "# 0.277\n",
    "# 0.272, 2.36\n",
    "# 0.260, 2.45\n",
    "\n",
    "# 0.258, 2.39\n",
    "# Penalize the model for saying there isn't a car when there is. \n",
    "\n",
    "# Trying BCELoss for counts. \n",
    "# Added an extra 128 compression layer before bins/count prediction. \n",
    "# 0.255 best val loc loss. \n",
    "\n",
    "# Trying compression to 60 before bins/count prediction layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying targets are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_imgs, bbs = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[idx].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, class_box):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = torch.tensor([4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((boom, torch.tensor([6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_imgs[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_box(ax, class_box)\n",
    "    \n",
    "# I also need to draw the bounding boxes. \n",
    "for bb in bbs[idx]:\n",
    "    x_ = torch.cat((bb[0], torch.tensor([bb[0][0]])))\n",
    "    y_ = torch.cat((bb[1], torch.tensor([bb[1][0]])))\n",
    "    ax.plot(x_, y_, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
