{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 132)\n",
    "val_labeled_scene_index = np.arange(132, 134)\n",
    "test_labeled_scene_index = np.arange(132, 134)\n",
    "\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 180 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 180)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.x_offset = nn.Sequential(OrderedDict([\n",
    "            ('xoff1', nn.Linear(self.concat_dim, 256)),\n",
    "            ('tanh1', nn.Tanh())\n",
    "        ]))\n",
    "        \n",
    "        self.y_offset = nn.Sequential(OrderedDict([\n",
    "            ('yoff1', nn.Linear(self.concat_dim, 256)),\n",
    "            ('tanh2', nn.Tanh())\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "        self.segmentation = nn.Sequential(OrderedDict([\n",
    "            ('linear1_segmentation', nn.Linear(self.concat_dim, 25600)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_images = x.shape[1]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        # Reshape here\n",
    "        x = x.view(-1, channels, height, width)\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x), self.segmentation(x), self.x_offset(x), self.y_offset(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = SimpleModel().to(device)\n",
    "model.load_state_dict(torch.load('/scratch/vr1059/all_six_images_classify_count_offset.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    BLOCK_SIZE = 5\n",
    "    images = []\n",
    "    target = []\n",
    "    road_maps = []\n",
    "    road_bins = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        \n",
    "        grid = []\n",
    "        # Get road_image and cast it to float\n",
    "        road_image = torch.as_tensor(x[2])\n",
    "        road_maps.append(road_image)\n",
    "        road_image = road_image.float()\n",
    "        \n",
    "        # Split up into blocks and assign pixel value for block\n",
    "        for x_ in range(0, 800, BLOCK_SIZE):\n",
    "            for y in range(0, 800, BLOCK_SIZE):\n",
    "                block = road_image[x_:x_+BLOCK_SIZE, y:y+BLOCK_SIZE]\n",
    "                score = torch.sum(block).item()\n",
    "                # If more than have the pixels are 1, classify as road\n",
    "                if score > (BLOCK_SIZE**2) / 2:\n",
    "                    grid.append(1.0)\n",
    "                else:\n",
    "                    grid.append(0.0)\n",
    "                \n",
    "        road_bins.append(torch.Tensor(grid))\n",
    "        \n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.Tensor(x[0][i]))\n",
    "        \n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(256)\n",
    "        counts = np.zeros(90)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "#             if x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "            # Grab the current bounding box. \n",
    "            current_bbs.append(corners)\n",
    "\n",
    "            # Get its four bird's-eye view coordinates.\n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "\n",
    "            # Find the bin/grid cell it falls in, get its class mapping. \n",
    "            center_x, center_y = torch.mean(xs).item(), torch.mean(ys).item()\n",
    "            key = (round_down(center_x), round_down(center_y))\n",
    "            if key not in class_dict:\n",
    "                print(key)\n",
    "            bin_id = class_dict[key]\n",
    "            bins[bin_id] = 1\n",
    "            count += 1\n",
    "            \n",
    "        \n",
    "        counts[count] = 1\n",
    "\n",
    "        # Label Smoothing #\n",
    "        if count > 10 and count < 88:\n",
    "            counts[count+1] = 0.2\n",
    "            counts[count-1] = 0.2\n",
    "        target_counts.append(torch.Tensor(counts))\n",
    "        \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        target.append(torch.Tensor(bins))\n",
    "        \n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(target), torch.stack(road_maps), bbs, torch.stack(target_counts), torch.stack(road_bins)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.ToTensor()\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=test_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(labeled_testset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average threat score tensor(0.0121)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "threat_scores = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(test_loader):\n",
    "        bb_samples = []\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        \n",
    "        y_hat, y_count, segmentation, x_offset, y_offset = model(sample)\n",
    "        \n",
    "        if torch.argmax(y_count).item() > 15:\n",
    "            result = torch.topk(y_hat, k = 6 + torch.argmax(y_count).item())\n",
    "            pred_ids = result.indices\n",
    "        else:\n",
    "            result = torch.topk(y_hat, k = torch.argmax(y_count).item())\n",
    "            pred_ids = result.indices\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            bin_x, bin_y = reverse_class_dict[idx.item()]\n",
    "            \n",
    "            bin_x_center, bin_y_center = bin_x + 25, bin_y + 25\n",
    "                        \n",
    "            x_off = x_offset[0][idx.item()] * 25\n",
    "            y_off = y_offset[0][idx.item()] * 25\n",
    "            \n",
    "            new_center_x = bin_x_center + x_off\n",
    "            new_center_y = bin_y_center + y_off\n",
    "            \n",
    "            xs = torch.Tensor([new_center_x - 25, new_center_x - 25, new_center_x + 25, new_center_x + 25])\n",
    "            ys = torch.Tensor([new_center_y - 10, new_center_y + 10, new_center_y - 10, new_center_y + 10])\n",
    "\n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "\n",
    "            xs /= 10.\n",
    "            ys /= 10.\n",
    "\n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "\n",
    "        bounding_boxes = torch.stack(bounding_boxes).double()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "        bb_samples = tuple(bb_samples)\n",
    "                \n",
    "        bb_samples = bb_samples[0].cpu()\n",
    "        bbs = torch.stack(bbs[0]).cpu()\n",
    "        ts_bounding_box = compute_ats_bounding_boxes(bb_samples, bbs)\n",
    "        \n",
    "        threat_scores += ts_bounding_box\n",
    "    \n",
    "    print(\"Average threat score\", threat_scores / len(test_loader))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best is 0.012 so far"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
