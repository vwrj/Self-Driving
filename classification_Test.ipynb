{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/vr1059/self-driving-data/data'\n",
    "annotation_csv = '/scratch/vr1059/self-driving-data/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.degrees(np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 1\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "reverse_class_dict.append((-100, -100))\n",
    "for i in range(400, 800, 50):\n",
    "    for j in range(100, 600, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))\n",
    "        \n",
    "class_dict[(-100, -100)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def front_collate_fn(batch):\n",
    "    front_imgs = []\n",
    "    front_right_imgs = []\n",
    "    front_left_imgs = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    for x in batch:\n",
    "        # input\n",
    "        front_left_imgs.append(torch.tensor(x[0][0]))\n",
    "        front_imgs.append(torch.tensor(x[0][1]))\n",
    "        front_right_imgs.append(torch.tensor(x[0][2]))\n",
    "        road_imgs.append(torch.tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        bbs.append(bb_tens)\n",
    "        x_min = 800\n",
    "        bb_cand = (-100., -100.)\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "            # Get bird's eye view coordinates. \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "            if xs[2] - xs[0] > 5:\n",
    "                top_center_x, top_center_y = 0.5*(xs[2] + xs[3]), 0.5*(ys[2] + ys[3])\n",
    "            else:\n",
    "                top_center_x, top_center_y = 0.5*(xs[0] + xs[1]), 0.5*(ys[0] + ys[1])\n",
    "                \n",
    "            # We do (800 - top_center_y) because matplotlib y-axis starts from the top. \n",
    "            v1 = np.array([top_center_x - 400, 800 - top_center_y - 400])\n",
    "            v2 = np.array([2, 0])\n",
    "            \n",
    "            if abs(angle_between(v1, v2)) <= 35 and x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "                if top_center_x < x_min:\n",
    "                    x_min = top_center_x\n",
    "                    bb_cand = (top_center_x.item(), top_center_y.item())\n",
    "         \n",
    "        if int(bb_cand[0]) == -100:\n",
    "            target.append((0, bb_cand[0]/100., bb_cand[1]/100.))\n",
    "        else:\n",
    "            key = (round_down(bb_cand[0]), round_down(bb_cand[1]))\n",
    "            if key not in class_dict:\n",
    "                print(bb_cand)\n",
    "            label = class_dict[key]\n",
    "            target.append((label, bb_cand[0]/100., bb_cand[1]/100.))\n",
    "                \n",
    "    boom = torch.stack(front_imgs), torch.tensor(target), torch.stack(road_imgs), bbs, torch.stack(front_right_imgs), torch.stack(front_left_imgs)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.3, saturation = 0.2, hue = (-0.3, 0.3)),\n",
    "        transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(10),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=256, shuffle=True, collate_fn=front_collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=256, shuffle=False, collate_fn=front_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet18()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(512, 81)),\n",
    "        ]))\n",
    "        \n",
    "        self.regression = nn.Sequential(OrderedDict([\n",
    "            ('linear_reg', nn.Linear(512, 2)),\n",
    "        ]))\n",
    "        \n",
    "#         self.regression.linear1.bias = nn.Parameter(torch.tensor(400.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.classification(x), self.regression(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device)\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "reg_criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    class_losses = []\n",
    "    reg_losses = []\n",
    "    for i, (sample, target, road_img, bbs, front_right, front_left) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat_class, y_hat_reg = model(sample)\n",
    "        target_class = target[:, 0]\n",
    "        target_reg = target[:, 1:]\n",
    "        \n",
    "        class_loss = class_criterion(y_hat_class, target_class.long())\n",
    "        reg_loss = reg_criterion(y_hat_reg, target_reg)\n",
    "        loss = class_loss + 0.5 * reg_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        class_losses.append(class_loss.item())\n",
    "        reg_losses.append(reg_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                10. * i / len(train_loader), loss.item()))\n",
    "            print('Classify Loss: {}'.format(np.mean(class_losses)))\n",
    "            print('Regression Loss: {}'.format(np.mean(reg_losses)))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    class_losses = []\n",
    "    reg_losses = []\n",
    "    for i, (sample, target, road_img, bbs, front_right, front_left) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat_class, y_hat_reg = model(sample)\n",
    "            target_class = target[:, 0]\n",
    "            target_reg = target[:, 1:]\n",
    "\n",
    "            class_loss = class_criterion(y_hat_class, target_class.long())\n",
    "            reg_loss = reg_criterion(y_hat_reg, target_reg)\n",
    "            loss = class_loss + 0.5 * reg_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            class_losses.append(class_loss.item())\n",
    "            reg_losses.append(reg_loss.item())\n",
    "\n",
    "#         if i % 5 == 0:\n",
    "#             print('Val Epoch: {} [{}/{} ({:.0f}%)]\\tAverage Loss So Far: {:.6f}'.format(\n",
    "#                 epoch, i * len(sample), len(val_loader.dataset),\n",
    "#                 5. * i / len(val_loader), np.mean(val_losses)))\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Classify Loss: \", np.mean(class_losses))\n",
    "    print(\"Average Validation Regression Loss: \", np.mean(reg_losses))\n",
    "    print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), 'best_val_loss_simple_class_plus_reg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2772 (0%)]\tLoss: 13.109037\n",
      "Classify Loss: 4.451223850250244\n",
      "Regression Loss: 17.31562614440918\n",
      "Train Epoch: 0 [2120/2772 (9%)]\tLoss: 7.122281\n",
      "Classify Loss: 4.082628141749989\n",
      "Regression Loss: 11.446815664117986\n",
      "\n",
      "Average Train Epoch Loss:  9.806036082181064\n",
      "Average Validation Epoch Loss:  7.805248260498047\n",
      "Average Validation Classify Loss:  4.101257085800171\n",
      "Average Validation Regression Loss:  7.407982349395752\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/2772 (0%)]\tLoss: 6.867484\n",
      "Classify Loss: 3.685173749923706\n",
      "Regression Loss: 6.364619731903076\n",
      "Train Epoch: 1 [2120/2772 (9%)]\tLoss: 5.094430\n",
      "Classify Loss: 3.399478717283769\n",
      "Regression Loss: 4.8500120423056865\n",
      "\n",
      "Average Train Epoch Loss:  5.824484738436612\n",
      "Average Validation Epoch Loss:  4.593433380126953\n",
      "Average Validation Classify Loss:  3.7844713926315308\n",
      "Average Validation Regression Loss:  1.6179240942001343\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/2772 (0%)]\tLoss: 4.977601\n",
      "Classify Loss: 2.9510140419006348\n",
      "Regression Loss: 4.0531744956970215\n",
      "Train Epoch: 2 [2120/2772 (9%)]\tLoss: 4.340690\n",
      "Classify Loss: 2.8834752386266533\n",
      "Regression Loss: 3.25788950920105\n",
      "\n",
      "Average Train Epoch Loss:  4.512420047413219\n",
      "Average Validation Epoch Loss:  4.524097561836243\n",
      "Average Validation Classify Loss:  3.6997084617614746\n",
      "Average Validation Regression Loss:  1.6487779915332794\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/2772 (0%)]\tLoss: 3.884391\n",
      "Classify Loss: 2.541083812713623\n",
      "Regression Loss: 2.686614513397217\n",
      "Train Epoch: 3 [2120/2772 (9%)]\tLoss: 3.719222\n",
      "Classify Loss: 2.620292295109142\n",
      "Regression Loss: 2.4898750998757104\n",
      "\n",
      "Average Train Epoch Loss:  3.8652298233725806\n",
      "Average Validation Epoch Loss:  4.830215930938721\n",
      "Average Validation Classify Loss:  3.5850034952163696\n",
      "Average Validation Regression Loss:  2.4904251098632812\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/2772 (0%)]\tLoss: 3.431224\n",
      "Classify Loss: 2.4927291870117188\n",
      "Regression Loss: 1.8769892454147339\n",
      "Train Epoch: 4 [2120/2772 (9%)]\tLoss: 3.607268\n",
      "Classify Loss: 2.4800772233442827\n",
      "Regression Loss: 2.0350395332683218\n",
      "\n",
      "Average Train Epoch Loss:  3.497597000815652\n",
      "Average Validation Epoch Loss:  4.225319027900696\n",
      "Average Validation Classify Loss:  3.378724694252014\n",
      "Average Validation Regression Loss:  1.693188637495041\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/2772 (0%)]\tLoss: 3.401017\n",
      "Classify Loss: 2.439901113510132\n",
      "Regression Loss: 1.9222310781478882\n",
      "Train Epoch: 5 [2120/2772 (9%)]\tLoss: 3.041551\n",
      "Classify Loss: 2.387253241105513\n",
      "Regression Loss: 1.643302939154885\n",
      "\n",
      "Average Train Epoch Loss:  3.20890474319458\n",
      "Average Validation Epoch Loss:  4.085106134414673\n",
      "Average Validation Classify Loss:  3.1688767671585083\n",
      "Average Validation Regression Loss:  1.832458734512329\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/2772 (0%)]\tLoss: 2.817994\n",
      "Classify Loss: 2.204206705093384\n",
      "Regression Loss: 1.2275748252868652\n",
      "Train Epoch: 6 [2120/2772 (9%)]\tLoss: 2.925319\n",
      "Classify Loss: 2.296234607696533\n",
      "Regression Loss: 1.3413493633270264\n",
      "\n",
      "Average Train Epoch Loss:  2.966909256848422\n",
      "Average Validation Epoch Loss:  3.8013932704925537\n",
      "Average Validation Classify Loss:  3.046893835067749\n",
      "Average Validation Regression Loss:  1.508998990058899\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/2772 (0%)]\tLoss: 2.819782\n",
      "Classify Loss: 2.1382248401641846\n",
      "Regression Loss: 1.363114833831787\n",
      "Train Epoch: 7 [2120/2772 (9%)]\tLoss: 2.907521\n",
      "Classify Loss: 2.180272947658192\n",
      "Regression Loss: 1.1130792986262927\n",
      "\n",
      "Average Train Epoch Loss:  2.736812569878318\n",
      "Average Validation Epoch Loss:  3.855929136276245\n",
      "Average Validation Classify Loss:  3.04551100730896\n",
      "Average Validation Regression Loss:  1.6208360642194748\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/2772 (0%)]\tLoss: 2.760951\n",
      "Classify Loss: 2.1142280101776123\n",
      "Regression Loss: 1.2934457063674927\n",
      "Train Epoch: 8 [2120/2772 (9%)]\tLoss: 2.503564\n",
      "Classify Loss: 2.069956150921908\n",
      "Regression Loss: 0.9095966924320568\n",
      "\n",
      "Average Train Epoch Loss:  2.5247545025565405\n",
      "Average Validation Epoch Loss:  3.7985459566116333\n",
      "Average Validation Classify Loss:  3.090137004852295\n",
      "Average Validation Regression Loss:  1.4168180376291275\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/2772 (0%)]\tLoss: 2.469409\n",
      "Classify Loss: 2.002190113067627\n",
      "Regression Loss: 0.9344378113746643\n",
      "Train Epoch: 9 [2120/2772 (9%)]\tLoss: 2.306943\n",
      "Classify Loss: 1.9732137810100208\n",
      "Regression Loss: 0.7404668385332281\n",
      "\n",
      "Average Train Epoch Loss:  2.3434471867301245\n",
      "Average Validation Epoch Loss:  3.726311206817627\n",
      "Average Validation Classify Loss:  3.0446170568466187\n",
      "Average Validation Regression Loss:  1.3633882403373718\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/2772 (0%)]\tLoss: 2.164901\n",
      "Classify Loss: 1.86310613155365\n",
      "Regression Loss: 0.6035906076431274\n",
      "Train Epoch: 10 [2120/2772 (9%)]\tLoss: 2.140330\n",
      "Classify Loss: 1.87414719841697\n",
      "Regression Loss: 0.6867936551570892\n",
      "\n",
      "Average Train Epoch Loss:  2.217544057152488\n",
      "Average Validation Epoch Loss:  3.943654775619507\n",
      "Average Validation Classify Loss:  2.995437264442444\n",
      "Average Validation Regression Loss:  1.896434873342514\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/2772 (0%)]\tLoss: 2.270598\n",
      "Classify Loss: 2.0112431049346924\n",
      "Regression Loss: 0.5187101364135742\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.28 lowest val loss so far, cross entropy\n",
    "# I want to try learning classification and regression simultaneously\n",
    "# Actually, I want to try a counting network. \n",
    "# Count how many cars it can see. Classification. \n",
    "\n",
    "# 2.23 (epoch 14) Trying to add RandomAffine, see if lowest classify loss goes down. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + Regression\n",
    "# Best combined loss: 3.72 (total), 3.04 classify, 1.36 regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
