{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim \n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Threat score for road detection\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image folder\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'\n",
    "\n",
    "# Indices\n",
    "train_labeled_scene_index = np.arange(106, 132)\n",
    "val_labeled_scene_index = np.arange(132,134)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Self-Supervised Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ssl_model = \"/scratch/brs426/ben_models/resnet50_no_bn_jigsaw_single_image_hard_200_permutations_50_bonus_SSL_checkpoint.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures - Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.segmentation = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 25600)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_images = x.shape[1]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        \n",
    "        # Reshape to feed in images\n",
    "        x = x.reshape(-1, channels, height, width)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.segmentation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in model and match weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_checkpoint = torch.load(path_to_ssl_model)\n",
    "model = SimpleModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_weights = ssl_checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in resnet weights \n",
    "resnet_dict = model.encoder.state_dict()\n",
    "for weight in ssl_weights:\n",
    "    if weight[7:] in model.encoder.state_dict():\n",
    "        resnet_dict[weight[7:]].data.copy_(ssl_weights[weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_collate_fn(batch):\n",
    "    BLOCK_SIZE = 5\n",
    "    road_maps = []\n",
    "    road_bins = []\n",
    "    images = []\n",
    "    for x in batch:\n",
    "        \n",
    "        grid = []\n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.as_tensor(x[0][i]))\n",
    "        \n",
    "        # Get road_image and cast it to float\n",
    "        road_image = torch.as_tensor(x[2])\n",
    "        road_maps.append(road_image)\n",
    "        road_image = road_image.float()\n",
    "        \n",
    "        for x in range(0, 800, BLOCK_SIZE):\n",
    "            for y in range(0, 800, BLOCK_SIZE):\n",
    "                block = road_image[x:x+BLOCK_SIZE, y:y+BLOCK_SIZE]\n",
    "                score = torch.sum(block).item()\n",
    "                # If more than have the pixels are 1, classify as road\n",
    "                if score > (BLOCK_SIZE**2) / 2:\n",
    "                    grid.append(1.0)\n",
    "                else:\n",
    "                    grid.append(0.0)\n",
    "            \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        road_bins.append(torch.as_tensor(grid))\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(road_bins), torch.stack(road_maps)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "aug_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        torchvision.transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=aug_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, shuffle=True, num_workers=10, collate_fn=segmentation_collate_fn)\n",
    "\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1, shuffle=True, num_workers=2, collate_fn=segmentation_collate_fn)\n",
    "\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, batch_size=2, shuffle=True, num_workers=2, collate_fn=segmentation_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logic, return average loss over training set after each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch, log_file_path, log_interval = 250):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Number correct for accuracy\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Train loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # Loop through examples\n",
    "    for batch_idx, (images, bins, road_map) in enumerate(train_loader):\n",
    "        \n",
    "        # 2 x 6 x 3 x w x h\n",
    "        # Send data and target to device\n",
    "        data, target = images.to(device), bins.to(device)\n",
    "        \n",
    "        # Zero out optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass data through model - right now only segmentation\n",
    "        output = model(data)\n",
    "        # Should be batch_size X 25600\n",
    "        output = output.squeeze()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            f.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    \n",
    "    # Average train loss\n",
    "    average_train_loss = train_loss / len(train_loader)\n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_bins(bins, block_size, threshold):\n",
    "    road_map = torch.zeros((800, 800))\n",
    "    idx = 0\n",
    "    for x in range(0, 800, block_size):\n",
    "        for y in range(0, 800, block_size):\n",
    "            road_map[x:x+block_size, y:y+block_size] = bins[idx]\n",
    "            idx += 1\n",
    "    return road_map > threshold\n",
    "\n",
    "\n",
    "# Define test method\n",
    "def test(model, device, test_loader, log_file_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    #thresholds = [0.42]\n",
    "    \n",
    "    threat_scores = torch.zeros(1)\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, bins, road_map) in enumerate(test_loader):\n",
    "\n",
    "            # Send data and target to device\n",
    "            data, target= images.to(device), bins.to(device)\n",
    "\n",
    "            # Pass data through model - right now only segmentation\n",
    "            output = model(data)\n",
    "            # Should be batch_size X 6400\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Now squeeze for reconstruction\n",
    "            #output = output.squeeze()\n",
    "            \n",
    "            # Compute threat score at 4 different thresholds\n",
    "            #for idx in range(len(thresholds)):\n",
    "            #    reconstructed_road_map = reconstruct_from_bins(output, 5, thresholds[idx]).cpu()\n",
    "            #    ts_road_map = compute_ts_road_map(reconstructed_road_map, road_map)\n",
    "            #    threat_scores[idx] += ts_road_map\n",
    "         \n",
    "    # Compute the average test_loss\n",
    "    # avg_test_loss = TODO\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Compute average threat scores\n",
    "    #avg_threat_scores = threat_scores / len(test_loader)\n",
    "    \n",
    "    #print('\\Threat scores: \\t {}:{}\\n'.format(thresholds[0], avg_threat_scores[0]))\n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    \n",
    "    #f.write('\\Threat scores: \\t {}:{}\\n'.format(thresholds[0], avg_threat_scores[0]))\n",
    "    f.write('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    f.close()\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze everything but batch norm and last resnet layer\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if name.find(\"bn\") >= 0 or name.find(\"layer4\") >= 0:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
    "# Scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.025, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3276 (0%)]\tLoss: 0.707536\n",
      "\n",
      "Train set: Average loss: 0.4132, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3695, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 1 [0/3276 (0%)]\tLoss: 0.433862\n",
      "\n",
      "Train set: Average loss: 0.3399, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3819, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 2 [0/3276 (0%)]\tLoss: 0.256017\n",
      "\n",
      "Train set: Average loss: 0.3048, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3451, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 3 [0/3276 (0%)]\tLoss: 0.248365\n",
      "\n",
      "Train set: Average loss: 0.2834, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3163, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 4 [0/3276 (0%)]\tLoss: 0.222870\n",
      "\n",
      "Train set: Average loss: 0.2674, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3148, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 5 [0/3276 (0%)]\tLoss: 0.178933\n",
      "\n",
      "Train set: Average loss: 0.2552, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3059, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 6 [0/3276 (0%)]\tLoss: 0.235449\n",
      "\n",
      "Train set: Average loss: 0.2422, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2873, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 7 [0/3276 (0%)]\tLoss: 0.243714\n",
      "\n",
      "Train set: Average loss: 0.2346, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2905, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 8 [0/3276 (0%)]\tLoss: 0.217400\n",
      "\n",
      "Train set: Average loss: 0.2255, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2796, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 9 [0/3276 (0%)]\tLoss: 0.168924\n",
      "\n",
      "Train set: Average loss: 0.2169, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2675, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 10 [0/3276 (0%)]\tLoss: 0.216925\n",
      "\n",
      "Train set: Average loss: 0.2114, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2781, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 11 [0/3276 (0%)]\tLoss: 0.200939\n",
      "\n",
      "Train set: Average loss: 0.2036, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2608, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 12 [0/3276 (0%)]\tLoss: 0.194037\n",
      "\n",
      "Train set: Average loss: 0.1987, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2616, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 13 [0/3276 (0%)]\tLoss: 0.205548\n",
      "\n",
      "Train set: Average loss: 0.1940, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2641, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 14 [0/3276 (0%)]\tLoss: 0.231170\n",
      "\n",
      "Train set: Average loss: 0.1900, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2529, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 15 [0/3276 (0%)]\tLoss: 0.148109\n",
      "\n",
      "Train set: Average loss: 0.1828, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2605, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 16 [0/3276 (0%)]\tLoss: 0.226060\n",
      "\n",
      "Train set: Average loss: 0.1787, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 17 [0/3276 (0%)]\tLoss: 0.157806\n",
      "\n",
      "Train set: Average loss: 0.1751, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2522, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 18 [0/3276 (0%)]\tLoss: 0.109314\n",
      "\n",
      "Train set: Average loss: 0.1702, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2276, Accuracy: 0/252 (0%)\n",
      "\n",
      "Train Epoch: 19 [0/3276 (0%)]\tLoss: 0.244177\n",
      "\n",
      "Train set: Average loss: 0.1673, Accuracy: 0/3276 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2265, Accuracy: 0/252 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = \"/scratch/brs426/ben_models/simple_model_pretrained_permutations_block_size_5_test_threat_20_epochs_for_report.log\"\n",
    "best_val_loss = 100\n",
    "save_path = \"/scratch/brs426/ben_models/simple_model_pretrained_permutations_block_size_5_test_threat_20_epochs_for_report.p\"\n",
    "epochs = 20\n",
    "for epoch in range(0, epochs):\n",
    "    # Train model\n",
    "    loss = train(model, device, trainloader, optimizer, epoch, f)\n",
    "    val_loss = test(model, device, valloader, f)\n",
    "    #scheduler.step(val_threat)\n",
    "    #Save model\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "Threshold 0.4: tensor([0.8227])\n"
     ]
    }
   ],
   "source": [
    "# Predicting everything as road\n",
    "model.eval()\n",
    "\n",
    "thresholds = [0.4]\n",
    "\n",
    "threat_scores = torch.zeros(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, bins, road_map) in enumerate(valloader):\n",
    "\n",
    "        # Send data and target to device\n",
    "        data, target= images.to(device), bins.to(device)\n",
    "\n",
    "        # Pass data through model - right now only segmentation\n",
    "        output = model(data)\n",
    "        # Should be batch_size X 6400\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = F.binary_cross_entropy(output, target)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "        # Now squeeze for reconstruction\n",
    "        output = output.squeeze()\n",
    "        print(output.shape)\n",
    "\n",
    "        # Compute threat score at 4 different thresholds\n",
    "        for idx in range(len(thresholds)):\n",
    "            reconstructed_road_map = reconstruct_from_bins(output, 5, thresholds[idx]).cpu()\n",
    "            ts_road_map = compute_ts_road_map(reconstructed_road_map, road_map)\n",
    "            threat_scores[idx] += ts_road_map\n",
    "\n",
    "avg_threat_scores = threat_scores / len(valloader)\n",
    "print(\"Threshold 0.4: {}\".format(avg_threat_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
