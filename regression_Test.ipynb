{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/vr1059/self-driving-data/data'\n",
    "annotation_csv = '/scratch/vr1059/self-driving-data/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.degrees(np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.3, saturation = 0.2, hue = (-0.3, 0.3)),\n",
    "        transforms.Grayscale(3)\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "def front_collate_fn(batch):\n",
    "    front_imgs = []\n",
    "    front_right_imgs = []\n",
    "    front_left_imgs = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    for x in batch:\n",
    "        # input\n",
    "        front_left_imgs.append(torch.tensor(x[0][0]))\n",
    "        front_imgs.append(torch.tensor(x[0][1]))\n",
    "        front_right_imgs.append(torch.tensor(x[0][2]))\n",
    "        road_imgs.append(torch.tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        bbs.append(bb_tens)\n",
    "        x_min = 800\n",
    "        bb_cand = (-500, -500, 0)\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "            # Get bird's eye view coordinates. \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "            if xs[2] - xs[0] > 5:\n",
    "                top_center_x, top_center_y = 0.5*(xs[2] + xs[3]), 0.5*(ys[2] + ys[3])\n",
    "            else:\n",
    "                top_center_x, top_center_y = 0.5*(xs[0] + xs[1]), 0.5*(ys[0] + ys[1])\n",
    "                \n",
    "            v1 = np.array([top_center_x - 400, 800 - top_center_y - 400])\n",
    "            v2 = np.array([2, 0])\n",
    "            \n",
    "            if abs(angle_between(v1, v2)) <= 35 and x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "                if top_center_x < x_min:\n",
    "                    x_min = top_center_x\n",
    "                    bb_cand = (top_center_x.item(), top_center_y.item(), 1)\n",
    "                \n",
    "        target.append(bb_cand)\n",
    "                    \n",
    "    boom = torch.stack(front_imgs), torch.tensor(target), torch.stack(road_imgs), bbs, torch.stack(front_right_imgs), torch.stack(front_left_imgs)\n",
    "    return boom\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=128, shuffle=True, collate_fn=front_collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=128, shuffle=False, collate_fn=front_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet18()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        self.regression = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(512, 2))\n",
    "        ]))\n",
    "        \n",
    "#         self.regression.linear1.bias = nn.Parameter(torch.tensor(4.), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.regression(x)\n",
    "    \n",
    "model = SimpleModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model.state_dict()\n",
    "pretrained_dict = torch.load('best_val_loss_simple.pt')\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict) \n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "for layer in unfreeze_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i, (sample, target, road_img, bbs, front_right, front_left) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        target /= 100.\n",
    "        target = target[:, :2]\n",
    "        \n",
    "        y_hat = model(sample)\n",
    "        \n",
    "#         target = target[:, 0]\n",
    "        loss = criterion(y_hat, target)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                10. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "            \n",
    "def val(model):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for i, (sample, target, road_img, bbs, front_right, front_left) in enumerate(val_loader):\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target /= 100.\n",
    "            target = target[:, :2]\n",
    "            # target = target[:, 0]\n",
    "            y_hat = model(sample)\n",
    "            loss = criterion(y_hat, target)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), 'best_val_loss_simple_class_then_reg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2772 (0%)]\tLoss: 23.363672\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 20\n",
    "best_val_loss = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, epoch)\n",
    "    val(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.07 best val regression loss <-- grayscale, classification pre-trained. \n",
    "# 2.71 best val regression loss (epoch 3) <-- grayscale and color jitter, classification pre-trained. Batch size 128.\n",
    "# Interestingly, the above + batch size 160 seems to get worse performance. Like 5.0 avg val loss. \n",
    "# <-- grayscale & color jitter, classification pre-trained, freeze everything except layers [3, 4]. Batch size 128."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
