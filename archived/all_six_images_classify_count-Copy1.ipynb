{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 50): 1,\n",
       " (0, 100): 2,\n",
       " (0, 150): 3,\n",
       " (0, 200): 4,\n",
       " (0, 250): 5,\n",
       " (0, 300): 6,\n",
       " (0, 350): 7,\n",
       " (0, 400): 8,\n",
       " (0, 450): 9,\n",
       " (0, 500): 10,\n",
       " (0, 550): 11,\n",
       " (0, 600): 12,\n",
       " (0, 650): 13,\n",
       " (0, 700): 14,\n",
       " (0, 750): 15,\n",
       " (50, 0): 16,\n",
       " (50, 50): 17,\n",
       " (50, 100): 18,\n",
       " (50, 150): 19,\n",
       " (50, 200): 20,\n",
       " (50, 250): 21,\n",
       " (50, 300): 22,\n",
       " (50, 350): 23,\n",
       " (50, 400): 24,\n",
       " (50, 450): 25,\n",
       " (50, 500): 26,\n",
       " (50, 550): 27,\n",
       " (50, 600): 28,\n",
       " (50, 650): 29,\n",
       " (50, 700): 30,\n",
       " (50, 750): 31,\n",
       " (100, 0): 32,\n",
       " (100, 50): 33,\n",
       " (100, 100): 34,\n",
       " (100, 150): 35,\n",
       " (100, 200): 36,\n",
       " (100, 250): 37,\n",
       " (100, 300): 38,\n",
       " (100, 350): 39,\n",
       " (100, 400): 40,\n",
       " (100, 450): 41,\n",
       " (100, 500): 42,\n",
       " (100, 550): 43,\n",
       " (100, 600): 44,\n",
       " (100, 650): 45,\n",
       " (100, 700): 46,\n",
       " (100, 750): 47,\n",
       " (150, 0): 48,\n",
       " (150, 50): 49,\n",
       " (150, 100): 50,\n",
       " (150, 150): 51,\n",
       " (150, 200): 52,\n",
       " (150, 250): 53,\n",
       " (150, 300): 54,\n",
       " (150, 350): 55,\n",
       " (150, 400): 56,\n",
       " (150, 450): 57,\n",
       " (150, 500): 58,\n",
       " (150, 550): 59,\n",
       " (150, 600): 60,\n",
       " (150, 650): 61,\n",
       " (150, 700): 62,\n",
       " (150, 750): 63,\n",
       " (200, 0): 64,\n",
       " (200, 50): 65,\n",
       " (200, 100): 66,\n",
       " (200, 150): 67,\n",
       " (200, 200): 68,\n",
       " (200, 250): 69,\n",
       " (200, 300): 70,\n",
       " (200, 350): 71,\n",
       " (200, 400): 72,\n",
       " (200, 450): 73,\n",
       " (200, 500): 74,\n",
       " (200, 550): 75,\n",
       " (200, 600): 76,\n",
       " (200, 650): 77,\n",
       " (200, 700): 78,\n",
       " (200, 750): 79,\n",
       " (250, 0): 80,\n",
       " (250, 50): 81,\n",
       " (250, 100): 82,\n",
       " (250, 150): 83,\n",
       " (250, 200): 84,\n",
       " (250, 250): 85,\n",
       " (250, 300): 86,\n",
       " (250, 350): 87,\n",
       " (250, 400): 88,\n",
       " (250, 450): 89,\n",
       " (250, 500): 90,\n",
       " (250, 550): 91,\n",
       " (250, 600): 92,\n",
       " (250, 650): 93,\n",
       " (250, 700): 94,\n",
       " (250, 750): 95,\n",
       " (300, 0): 96,\n",
       " (300, 50): 97,\n",
       " (300, 100): 98,\n",
       " (300, 150): 99,\n",
       " (300, 200): 100,\n",
       " (300, 250): 101,\n",
       " (300, 300): 102,\n",
       " (300, 350): 103,\n",
       " (300, 400): 104,\n",
       " (300, 450): 105,\n",
       " (300, 500): 106,\n",
       " (300, 550): 107,\n",
       " (300, 600): 108,\n",
       " (300, 650): 109,\n",
       " (300, 700): 110,\n",
       " (300, 750): 111,\n",
       " (350, 0): 112,\n",
       " (350, 50): 113,\n",
       " (350, 100): 114,\n",
       " (350, 150): 115,\n",
       " (350, 200): 116,\n",
       " (350, 250): 117,\n",
       " (350, 300): 118,\n",
       " (350, 350): 119,\n",
       " (350, 400): 120,\n",
       " (350, 450): 121,\n",
       " (350, 500): 122,\n",
       " (350, 550): 123,\n",
       " (350, 600): 124,\n",
       " (350, 650): 125,\n",
       " (350, 700): 126,\n",
       " (350, 750): 127,\n",
       " (400, 0): 128,\n",
       " (400, 50): 129,\n",
       " (400, 100): 130,\n",
       " (400, 150): 131,\n",
       " (400, 200): 132,\n",
       " (400, 250): 133,\n",
       " (400, 300): 134,\n",
       " (400, 350): 135,\n",
       " (400, 400): 136,\n",
       " (400, 450): 137,\n",
       " (400, 500): 138,\n",
       " (400, 550): 139,\n",
       " (400, 600): 140,\n",
       " (400, 650): 141,\n",
       " (400, 700): 142,\n",
       " (400, 750): 143,\n",
       " (450, 0): 144,\n",
       " (450, 50): 145,\n",
       " (450, 100): 146,\n",
       " (450, 150): 147,\n",
       " (450, 200): 148,\n",
       " (450, 250): 149,\n",
       " (450, 300): 150,\n",
       " (450, 350): 151,\n",
       " (450, 400): 152,\n",
       " (450, 450): 153,\n",
       " (450, 500): 154,\n",
       " (450, 550): 155,\n",
       " (450, 600): 156,\n",
       " (450, 650): 157,\n",
       " (450, 700): 158,\n",
       " (450, 750): 159,\n",
       " (500, 0): 160,\n",
       " (500, 50): 161,\n",
       " (500, 100): 162,\n",
       " (500, 150): 163,\n",
       " (500, 200): 164,\n",
       " (500, 250): 165,\n",
       " (500, 300): 166,\n",
       " (500, 350): 167,\n",
       " (500, 400): 168,\n",
       " (500, 450): 169,\n",
       " (500, 500): 170,\n",
       " (500, 550): 171,\n",
       " (500, 600): 172,\n",
       " (500, 650): 173,\n",
       " (500, 700): 174,\n",
       " (500, 750): 175,\n",
       " (550, 0): 176,\n",
       " (550, 50): 177,\n",
       " (550, 100): 178,\n",
       " (550, 150): 179,\n",
       " (550, 200): 180,\n",
       " (550, 250): 181,\n",
       " (550, 300): 182,\n",
       " (550, 350): 183,\n",
       " (550, 400): 184,\n",
       " (550, 450): 185,\n",
       " (550, 500): 186,\n",
       " (550, 550): 187,\n",
       " (550, 600): 188,\n",
       " (550, 650): 189,\n",
       " (550, 700): 190,\n",
       " (550, 750): 191,\n",
       " (600, 0): 192,\n",
       " (600, 50): 193,\n",
       " (600, 100): 194,\n",
       " (600, 150): 195,\n",
       " (600, 200): 196,\n",
       " (600, 250): 197,\n",
       " (600, 300): 198,\n",
       " (600, 350): 199,\n",
       " (600, 400): 200,\n",
       " (600, 450): 201,\n",
       " (600, 500): 202,\n",
       " (600, 550): 203,\n",
       " (600, 600): 204,\n",
       " (600, 650): 205,\n",
       " (600, 700): 206,\n",
       " (600, 750): 207,\n",
       " (650, 0): 208,\n",
       " (650, 50): 209,\n",
       " (650, 100): 210,\n",
       " (650, 150): 211,\n",
       " (650, 200): 212,\n",
       " (650, 250): 213,\n",
       " (650, 300): 214,\n",
       " (650, 350): 215,\n",
       " (650, 400): 216,\n",
       " (650, 450): 217,\n",
       " (650, 500): 218,\n",
       " (650, 550): 219,\n",
       " (650, 600): 220,\n",
       " (650, 650): 221,\n",
       " (650, 700): 222,\n",
       " (650, 750): 223,\n",
       " (700, 0): 224,\n",
       " (700, 50): 225,\n",
       " (700, 100): 226,\n",
       " (700, 150): 227,\n",
       " (700, 200): 228,\n",
       " (700, 250): 229,\n",
       " (700, 300): 230,\n",
       " (700, 350): 231,\n",
       " (700, 400): 232,\n",
       " (700, 450): 233,\n",
       " (700, 500): 234,\n",
       " (700, 550): 235,\n",
       " (700, 600): 236,\n",
       " (700, 650): 237,\n",
       " (700, 700): 238,\n",
       " (700, 750): 239,\n",
       " (750, 0): 240,\n",
       " (750, 50): 241,\n",
       " (750, 100): 242,\n",
       " (750, 150): 243,\n",
       " (750, 200): 244,\n",
       " (750, 250): 245,\n",
       " (750, 300): 246,\n",
       " (750, 350): 247,\n",
       " (750, 400): 248,\n",
       " (750, 450): 249,\n",
       " (750, 500): 250,\n",
       " (750, 550): 251,\n",
       " (750, 600): 252,\n",
       " (750, 650): 253,\n",
       " (750, 700): 254,\n",
       " (750, 750): 255}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 350)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_class_dict[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    BLOCK_SIZE = 5\n",
    "    images = []\n",
    "    target = []\n",
    "    road_maps = []\n",
    "    road_bins = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        \n",
    "        grid = []\n",
    "        # Get road_image and cast it to float\n",
    "        road_image = torch.as_tensor(x[2])\n",
    "        road_maps.append(road_image)\n",
    "        road_image = road_image.float()\n",
    "        \n",
    "        # Split up into blocks and assign pixel value for block\n",
    "        for x_ in range(0, 800, BLOCK_SIZE):\n",
    "            for y in range(0, 800, BLOCK_SIZE):\n",
    "                block = road_image[x_:x_+BLOCK_SIZE, y:y+BLOCK_SIZE]\n",
    "                score = torch.sum(block).item()\n",
    "                # If more than have the pixels are 1, classify as road\n",
    "                if score > (BLOCK_SIZE**2) / 2:\n",
    "                    grid.append(1.0)\n",
    "                else:\n",
    "                    grid.append(0.0)\n",
    "                \n",
    "        road_bins.append(torch.Tensor(grid))\n",
    "        \n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.Tensor(x[0][i]))\n",
    "        \n",
    "        road_imgs.append(torch.as_tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(256)\n",
    "        counts = np.zeros(90)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "#             if x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "            # Get its four bird's-eye view coordinates. \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "\n",
    "            # Grab the current bounding box. \n",
    "            current_bbs.append((xs, ys))\n",
    "\n",
    "            # Find the bin/grid cell it falls in, get its class mapping. \n",
    "            center_x, center_y = torch.mean(xs).item(), torch.mean(ys).item()\n",
    "            key = (round_down(center_x), round_down(center_y))\n",
    "            if key not in class_dict:\n",
    "                print(key)\n",
    "            bin_id = class_dict[key]\n",
    "            bins[bin_id] = 1\n",
    "            count += 1\n",
    "            \n",
    "        \n",
    "        counts[count] = 1\n",
    "\n",
    "        # Label Smoothing #\n",
    "        if count > 10 and count < 88:\n",
    "            counts[count+1] = 0.2\n",
    "            counts[count-1] = 0.2\n",
    "        target_counts.append(torch.Tensor(counts))\n",
    "        \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        target.append(torch.Tensor(bins))\n",
    "        \n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(target), torch.stack(road_maps), bbs, torch.stack(target_counts), torch.stack(road_bins)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.argmax(counts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx], nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    \n",
    "    draw_box(ax, class_box, 'green')\n",
    "    \n",
    "def append_first_to_last(tens):\n",
    "    ret = torch.cat((tens, torch.as_tensor([tens[0]])))\n",
    "    return ret\n",
    "\n",
    "    \n",
    "for bb in bbs[idx]:\n",
    "    ax.plot(append_first_to_last(bb[0]), append_first_to_last(bb[1]), color='orange')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 200)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "        self.segmentation = nn.Sequential(OrderedDict([\n",
    "            ('linear1_segmentation', nn.Linear(self.concat_dim, 25600)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_images = x.shape[1]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        # Reshape here\n",
    "        x = x.view(-1, channels, height, width)\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x), self.segmentation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel()\n",
    "\n",
    "# Weighting certain classes more. \n",
    "# positive_weight = torch.ones(256).to(device)\n",
    "# for (x, y), bin_id in class_dict.items():\n",
    "#     if abs(x - 400) <= 200 and abs(y - 400) <= 200:\n",
    "#         positive_weight[bin_id] = 2\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if(\"bn\" not in name):\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "# for layer in unfreeze_layers:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "        \n",
    "model = model.to(device)\n",
    "bin_criterion = nn.BCEWithLogitsLoss()\n",
    "count_criterion = nn.BCEWithLogitsLoss()\n",
    "segmentation_criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(device)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a.float()) + (1 - lam) * criterion(pred, y_b.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=10, num_workers=3, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    train_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    segmentation_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        \n",
    "#         sample, target_a, target_b, lam = mixup_data(sample, target)\n",
    "#         sample, target_a, target_b = map(torch.autograd.Variable, (sample, target_a, target_b))\n",
    "        \n",
    "    # Why were you doing this?\n",
    "#         batch_yhat = []\n",
    "#         batch_ycount = []\n",
    "#         for j, x in enumerate(sample):\n",
    "#             y_hat, y_count, segmentation = model(x)\n",
    "#             batch_yhat.append(y_hat)\n",
    "#             batch_ycount.append(y_count)\n",
    "        \n",
    "#         y_hat = torch.stack(batch_yhat).squeeze()\n",
    "#         y_count = torch.stack(batch_ycount).squeeze()\n",
    "        \n",
    "    \n",
    "        y_hat, y_count, segmentation = model(sample)\n",
    "        \n",
    "        # Mixup criterion here\n",
    "#         bin_loss = mixup_criterion(bin_criterion, y_hat, target_a, target_b, lam)\n",
    "        \n",
    "        bin_loss = bin_criterion(y_hat, target.float())\n",
    "        count_loss = count_criterion(y_count, target_count.float())\n",
    "        segmentation_loss = segmentation_criterion(segmentation, road_bins.float())\n",
    "        loss = bin_loss + 2 * count_loss + segmentation_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        bin_losses.append(bin_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "        segmentation_losses.append(segmentation_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                50. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    print(\"Average Train Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Train Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"Average Train Segmentation Epoch Loss: \", np.mean(segmentation_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    segmentation_losses = []\n",
    "    count_correct = 0\n",
    "    count_off_by_1 = 0\n",
    "    total_count = 0\n",
    "    bin_correct = 0\n",
    "    total_bins = 0\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             batch_yhat = []\n",
    "#             batch_ycount = []\n",
    "#             for j, x in enumerate(sample):\n",
    "#                 y_hat, y_count = model(x)\n",
    "#                 batch_yhat.append(y_hat)\n",
    "#                 batch_ycount.append(y_count)\n",
    "\n",
    "#             y_hat = torch.stack(batch_yhat).squeeze()\n",
    "#             y_count = torch.stack(batch_ycount).squeeze()\n",
    "            \n",
    "#             for j, x in enumerate(y_count):\n",
    "#                 pred_count = torch.argmax(y_count[j])\n",
    "#                 t_count = torch.argmax(target_count[j])\n",
    "#                 if pred_count == t_count:\n",
    "#                     count_correct += 1\n",
    "#                 elif abs(pred_count - t_count) == 1:\n",
    "#                     count_off_by_1 += 1\n",
    "                      \n",
    "#                 pred_bins = torch.topk(y_hat[j], k = pred_count).indices\n",
    "#                 t_bins = (target[j] == 1).nonzero()\n",
    "#                 for b in pred_bins:\n",
    "#                     if b in t_bins:\n",
    "#                         bin_correct += 1\n",
    "                    \n",
    "#                 total_bins += len(t_bins)\n",
    "            \n",
    "            y_hat, y_count, segmentation = model(sample)\n",
    "#             total_count += y_count.size(0)\n",
    "            \n",
    "            bin_loss = bin_criterion(y_hat, target.float())\n",
    "            count_loss = count_criterion(y_count, target_count.float())\n",
    "            segmentation_loss = segmentation_criterion(segmentation, road_bins.float())\n",
    "            loss = bin_loss + count_loss + segmentation_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            bin_losses.append(bin_loss.item())\n",
    "            count_losses.append(count_loss.item())\n",
    "            segmentation_losses.append(segmentation_loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Validation Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"Average Train Segmentation Epoch Loss: \", np.mean(segmentation_losses))\n",
    "#     print(\"\\tAverage Validation Count Accuracy: \", 100*count_correct/total_count)\n",
    "#     print(\"\\tAverage Validation Count-off-by-1 Accuracy: \", 100*count_off_by_1/total_count)\n",
    "#     if total_bins != 0:\n",
    "#         print(\"\\tAverage Validation Bin Accuracy: \", 100*bin_correct/total_bins)\n",
    "#     print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), '/scratch/brs426/all_six_images_classify_count.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 x 200\n",
    "# 10 x 1200\n",
    "x = torch.randn((12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2217e-01, -7.3901e-01, -1.8286e+00, -7.0720e-01,  5.5493e-01],\n",
       "        [ 8.8764e-01, -1.8905e-01, -7.1408e-02,  9.6601e-01, -2.5093e+00],\n",
       "        [ 4.9115e-01, -1.9423e+00, -1.1985e+00,  1.1864e+00,  1.8880e+00],\n",
       "        [-2.1469e-01,  2.9941e-01,  1.9331e+00,  4.4181e-01, -1.7342e+00],\n",
       "        [-3.7422e-01,  1.4801e+00, -8.4294e-01, -2.2830e+00, -1.5767e-01],\n",
       "        [-1.5516e-01,  7.6859e-01,  7.9272e-01, -9.6438e-01, -3.8129e-01],\n",
       "        [-1.2397e+00, -8.6256e-01, -1.8095e+00,  9.0200e-01, -7.2423e-01],\n",
       "        [-6.2345e-01, -1.9860e+00,  5.2594e-01, -6.8100e-01, -1.9903e-01],\n",
       "        [ 1.8554e+00, -3.2209e-02,  1.5327e+00, -1.2397e+00,  1.1343e+00],\n",
       "        [ 1.4491e+00, -8.2009e-01, -6.2748e-01,  1.9462e+00,  1.8221e-03],\n",
       "        [-8.3885e-01,  1.1912e+00,  8.6688e-01, -3.4989e-01,  5.7890e-01],\n",
       "        [-5.4305e-01, -7.8100e-01, -1.9309e+00, -8.4028e-01, -2.5602e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2217e-01, -7.3901e-01, -1.8286e+00, -7.0720e-01,  5.5493e-01,\n",
       "          8.8764e-01, -1.8905e-01, -7.1408e-02,  9.6601e-01, -2.5093e+00,\n",
       "          4.9115e-01, -1.9423e+00, -1.1985e+00,  1.1864e+00,  1.8880e+00,\n",
       "         -2.1469e-01,  2.9941e-01,  1.9331e+00,  4.4181e-01, -1.7342e+00,\n",
       "         -3.7422e-01,  1.4801e+00, -8.4294e-01, -2.2830e+00, -1.5767e-01,\n",
       "         -1.5516e-01,  7.6859e-01,  7.9272e-01, -9.6438e-01, -3.8129e-01],\n",
       "        [-1.2397e+00, -8.6256e-01, -1.8095e+00,  9.0200e-01, -7.2423e-01,\n",
       "         -6.2345e-01, -1.9860e+00,  5.2594e-01, -6.8100e-01, -1.9903e-01,\n",
       "          1.8554e+00, -3.2209e-02,  1.5327e+00, -1.2397e+00,  1.1343e+00,\n",
       "          1.4491e+00, -8.2009e-01, -6.2748e-01,  1.9462e+00,  1.8221e-03,\n",
       "         -8.3885e-01,  1.1912e+00,  8.6688e-01, -3.4989e-01,  5.7890e-01,\n",
       "         -5.4305e-01, -7.8100e-01, -1.9309e+00, -8.4028e-01, -2.5602e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(2, -1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.253 val bin loss\n",
    "\n",
    "# Random Affine 3 degrees\n",
    "# 0.263 val bin loss\n",
    "\n",
    "\n",
    "# Need to do 5 * bin_loss + count_loss or something like that. Also more extreme Random Affine maybe?\n",
    "\n",
    "# Random Affine 5 degrees\n",
    "# 0.266 val bin loss\n",
    "\n",
    "# Took out Random Affine. \n",
    "# 0.268 val bin loss\n",
    "\n",
    "# Increased compress dim from 128 to 200. \n",
    "# 0.259 val bin loss\n",
    "\n",
    "# 5 * bin_loss + count_loss\n",
    "# 0.249 + 0.055\n",
    "\n",
    "# 8 *\n",
    "# 0.251 + 0.054\n",
    "\n",
    "# 8*, RandomAffine(3)\n",
    "# 0.255\n",
    "\n",
    "# 8*, RandomAffine(3), weight_decay 0.1\n",
    "\n",
    "# 10 *, RandomAffine(3)\n",
    "# 0.259\n",
    "\n",
    "# 8 *, Normalize (mean, std)\n",
    "# 0.26\n",
    "\n",
    "# 8 *, Dropout\n",
    "# 0.241, 0.253\n",
    "\n",
    "# 5 *, Dropout\n",
    "# 0.254\n",
    "\n",
    "# 11 *, Dropout\n",
    "# 0.249\n",
    "\n",
    "# Want to try positive-weights for classes within 200 to 600. \n",
    "# Want to get the model to get those classes correct. \n",
    "\n",
    "# Mixup 0.2, 1 *, Dropout\n",
    "# (0.244, 0.053), \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "model.load_state_dict(torch.load('all_six_images_classify_count.pt'))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(samples):\n",
    "    \n",
    "    # samples is (batch_size, 6, 3, 256, 306)\n",
    "    \n",
    "    # You need to return a tuple with size batch_size and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of bounding boxes. \n",
    "    \n",
    "    # Okay so I have my model. \n",
    "    # \n",
    "    \n",
    "    bb_samples = []\n",
    "    \n",
    "    for x in samples:\n",
    "        preds_class, preds_count = model(x)\n",
    "        \n",
    "        # preds class is a 256-dimensional tensor, filled with probabilities\n",
    "        # I need to find the `preds_count` top indices with the top values.\n",
    "        \n",
    "        \n",
    "        result = torch.topk(preds_class, k = torch.argmax(preds_count).item())\n",
    "        pred_ids = result.indices\n",
    "        \n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            buck_x, buck_y = reverse_class_dict[idx.item()]\n",
    "            \n",
    "            xs = torch.as_tensor([buck_x, buck_x, buck_x + 50, buck_x + 50])\n",
    "            ys = torch.as_tensor([buck_y+16, buck_y+36, buck_y+16, buck_y+36])\n",
    "            \n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "            \n",
    "            xs /= 10\n",
    "            ys /= 10\n",
    "               \n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "            \n",
    "        bounding_boxes = torch.stack(bounding_boxes).cuda()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "    \n",
    "    return tuple(bb_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(val_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = get_bounding_boxes(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_preds = torch.sigmoid(model(sample[idx])[0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx].cpu().detach(), nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "\n",
    "bin_ids = (sigmoid_preds > 0.25).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'red')\n",
    "    \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'green')\n",
    "\n",
    "    \n",
    "for bb in boom[idx]:\n",
    "    box = bb.cpu().detach()\n",
    "    draw_box(ax, box, 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([box[:, 0], box[:, 1], box[:, 3], box[:, 2], box[:, 0]])\n",
    "\n",
    "def draw_box(ax, corners, color):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    \n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "    return point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vish_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
