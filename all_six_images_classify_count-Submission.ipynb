{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 132)\n",
    "val_labeled_scene_index = np.arange(132, 134)\n",
    "test_labeled_scene_index = np.arange(132, 134)\n",
    "\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 50): 1,\n",
       " (0, 100): 2,\n",
       " (0, 150): 3,\n",
       " (0, 200): 4,\n",
       " (0, 250): 5,\n",
       " (0, 300): 6,\n",
       " (0, 350): 7,\n",
       " (0, 400): 8,\n",
       " (0, 450): 9,\n",
       " (0, 500): 10,\n",
       " (0, 550): 11,\n",
       " (0, 600): 12,\n",
       " (0, 650): 13,\n",
       " (0, 700): 14,\n",
       " (0, 750): 15,\n",
       " (50, 0): 16,\n",
       " (50, 50): 17,\n",
       " (50, 100): 18,\n",
       " (50, 150): 19,\n",
       " (50, 200): 20,\n",
       " (50, 250): 21,\n",
       " (50, 300): 22,\n",
       " (50, 350): 23,\n",
       " (50, 400): 24,\n",
       " (50, 450): 25,\n",
       " (50, 500): 26,\n",
       " (50, 550): 27,\n",
       " (50, 600): 28,\n",
       " (50, 650): 29,\n",
       " (50, 700): 30,\n",
       " (50, 750): 31,\n",
       " (100, 0): 32,\n",
       " (100, 50): 33,\n",
       " (100, 100): 34,\n",
       " (100, 150): 35,\n",
       " (100, 200): 36,\n",
       " (100, 250): 37,\n",
       " (100, 300): 38,\n",
       " (100, 350): 39,\n",
       " (100, 400): 40,\n",
       " (100, 450): 41,\n",
       " (100, 500): 42,\n",
       " (100, 550): 43,\n",
       " (100, 600): 44,\n",
       " (100, 650): 45,\n",
       " (100, 700): 46,\n",
       " (100, 750): 47,\n",
       " (150, 0): 48,\n",
       " (150, 50): 49,\n",
       " (150, 100): 50,\n",
       " (150, 150): 51,\n",
       " (150, 200): 52,\n",
       " (150, 250): 53,\n",
       " (150, 300): 54,\n",
       " (150, 350): 55,\n",
       " (150, 400): 56,\n",
       " (150, 450): 57,\n",
       " (150, 500): 58,\n",
       " (150, 550): 59,\n",
       " (150, 600): 60,\n",
       " (150, 650): 61,\n",
       " (150, 700): 62,\n",
       " (150, 750): 63,\n",
       " (200, 0): 64,\n",
       " (200, 50): 65,\n",
       " (200, 100): 66,\n",
       " (200, 150): 67,\n",
       " (200, 200): 68,\n",
       " (200, 250): 69,\n",
       " (200, 300): 70,\n",
       " (200, 350): 71,\n",
       " (200, 400): 72,\n",
       " (200, 450): 73,\n",
       " (200, 500): 74,\n",
       " (200, 550): 75,\n",
       " (200, 600): 76,\n",
       " (200, 650): 77,\n",
       " (200, 700): 78,\n",
       " (200, 750): 79,\n",
       " (250, 0): 80,\n",
       " (250, 50): 81,\n",
       " (250, 100): 82,\n",
       " (250, 150): 83,\n",
       " (250, 200): 84,\n",
       " (250, 250): 85,\n",
       " (250, 300): 86,\n",
       " (250, 350): 87,\n",
       " (250, 400): 88,\n",
       " (250, 450): 89,\n",
       " (250, 500): 90,\n",
       " (250, 550): 91,\n",
       " (250, 600): 92,\n",
       " (250, 650): 93,\n",
       " (250, 700): 94,\n",
       " (250, 750): 95,\n",
       " (300, 0): 96,\n",
       " (300, 50): 97,\n",
       " (300, 100): 98,\n",
       " (300, 150): 99,\n",
       " (300, 200): 100,\n",
       " (300, 250): 101,\n",
       " (300, 300): 102,\n",
       " (300, 350): 103,\n",
       " (300, 400): 104,\n",
       " (300, 450): 105,\n",
       " (300, 500): 106,\n",
       " (300, 550): 107,\n",
       " (300, 600): 108,\n",
       " (300, 650): 109,\n",
       " (300, 700): 110,\n",
       " (300, 750): 111,\n",
       " (350, 0): 112,\n",
       " (350, 50): 113,\n",
       " (350, 100): 114,\n",
       " (350, 150): 115,\n",
       " (350, 200): 116,\n",
       " (350, 250): 117,\n",
       " (350, 300): 118,\n",
       " (350, 350): 119,\n",
       " (350, 400): 120,\n",
       " (350, 450): 121,\n",
       " (350, 500): 122,\n",
       " (350, 550): 123,\n",
       " (350, 600): 124,\n",
       " (350, 650): 125,\n",
       " (350, 700): 126,\n",
       " (350, 750): 127,\n",
       " (400, 0): 128,\n",
       " (400, 50): 129,\n",
       " (400, 100): 130,\n",
       " (400, 150): 131,\n",
       " (400, 200): 132,\n",
       " (400, 250): 133,\n",
       " (400, 300): 134,\n",
       " (400, 350): 135,\n",
       " (400, 400): 136,\n",
       " (400, 450): 137,\n",
       " (400, 500): 138,\n",
       " (400, 550): 139,\n",
       " (400, 600): 140,\n",
       " (400, 650): 141,\n",
       " (400, 700): 142,\n",
       " (400, 750): 143,\n",
       " (450, 0): 144,\n",
       " (450, 50): 145,\n",
       " (450, 100): 146,\n",
       " (450, 150): 147,\n",
       " (450, 200): 148,\n",
       " (450, 250): 149,\n",
       " (450, 300): 150,\n",
       " (450, 350): 151,\n",
       " (450, 400): 152,\n",
       " (450, 450): 153,\n",
       " (450, 500): 154,\n",
       " (450, 550): 155,\n",
       " (450, 600): 156,\n",
       " (450, 650): 157,\n",
       " (450, 700): 158,\n",
       " (450, 750): 159,\n",
       " (500, 0): 160,\n",
       " (500, 50): 161,\n",
       " (500, 100): 162,\n",
       " (500, 150): 163,\n",
       " (500, 200): 164,\n",
       " (500, 250): 165,\n",
       " (500, 300): 166,\n",
       " (500, 350): 167,\n",
       " (500, 400): 168,\n",
       " (500, 450): 169,\n",
       " (500, 500): 170,\n",
       " (500, 550): 171,\n",
       " (500, 600): 172,\n",
       " (500, 650): 173,\n",
       " (500, 700): 174,\n",
       " (500, 750): 175,\n",
       " (550, 0): 176,\n",
       " (550, 50): 177,\n",
       " (550, 100): 178,\n",
       " (550, 150): 179,\n",
       " (550, 200): 180,\n",
       " (550, 250): 181,\n",
       " (550, 300): 182,\n",
       " (550, 350): 183,\n",
       " (550, 400): 184,\n",
       " (550, 450): 185,\n",
       " (550, 500): 186,\n",
       " (550, 550): 187,\n",
       " (550, 600): 188,\n",
       " (550, 650): 189,\n",
       " (550, 700): 190,\n",
       " (550, 750): 191,\n",
       " (600, 0): 192,\n",
       " (600, 50): 193,\n",
       " (600, 100): 194,\n",
       " (600, 150): 195,\n",
       " (600, 200): 196,\n",
       " (600, 250): 197,\n",
       " (600, 300): 198,\n",
       " (600, 350): 199,\n",
       " (600, 400): 200,\n",
       " (600, 450): 201,\n",
       " (600, 500): 202,\n",
       " (600, 550): 203,\n",
       " (600, 600): 204,\n",
       " (600, 650): 205,\n",
       " (600, 700): 206,\n",
       " (600, 750): 207,\n",
       " (650, 0): 208,\n",
       " (650, 50): 209,\n",
       " (650, 100): 210,\n",
       " (650, 150): 211,\n",
       " (650, 200): 212,\n",
       " (650, 250): 213,\n",
       " (650, 300): 214,\n",
       " (650, 350): 215,\n",
       " (650, 400): 216,\n",
       " (650, 450): 217,\n",
       " (650, 500): 218,\n",
       " (650, 550): 219,\n",
       " (650, 600): 220,\n",
       " (650, 650): 221,\n",
       " (650, 700): 222,\n",
       " (650, 750): 223,\n",
       " (700, 0): 224,\n",
       " (700, 50): 225,\n",
       " (700, 100): 226,\n",
       " (700, 150): 227,\n",
       " (700, 200): 228,\n",
       " (700, 250): 229,\n",
       " (700, 300): 230,\n",
       " (700, 350): 231,\n",
       " (700, 400): 232,\n",
       " (700, 450): 233,\n",
       " (700, 500): 234,\n",
       " (700, 550): 235,\n",
       " (700, 600): 236,\n",
       " (700, 650): 237,\n",
       " (700, 700): 238,\n",
       " (700, 750): 239,\n",
       " (750, 0): 240,\n",
       " (750, 50): 241,\n",
       " (750, 100): 242,\n",
       " (750, 150): 243,\n",
       " (750, 200): 244,\n",
       " (750, 250): 245,\n",
       " (750, 300): 246,\n",
       " (750, 350): 247,\n",
       " (750, 400): 248,\n",
       " (750, 450): 249,\n",
       " (750, 500): 250,\n",
       " (750, 550): 251,\n",
       " (750, 600): 252,\n",
       " (750, 650): 253,\n",
       " (750, 700): 254,\n",
       " (750, 750): 255}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 350)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_class_dict[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    BLOCK_SIZE = 5\n",
    "    images = []\n",
    "    target = []\n",
    "    road_maps = []\n",
    "    road_bins = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        \n",
    "        grid = []\n",
    "        # Get road_image and cast it to float\n",
    "        road_image = torch.as_tensor(x[2])\n",
    "        road_maps.append(road_image)\n",
    "        road_image = road_image.float()\n",
    "        \n",
    "        # Split up into blocks and assign pixel value for block\n",
    "        for x_ in range(0, 800, BLOCK_SIZE):\n",
    "            for y in range(0, 800, BLOCK_SIZE):\n",
    "                block = road_image[x_:x_+BLOCK_SIZE, y:y+BLOCK_SIZE]\n",
    "                score = torch.sum(block).item()\n",
    "                # If more than have the pixels are 1, classify as road\n",
    "                if score > (BLOCK_SIZE**2) / 2:\n",
    "                    grid.append(1.0)\n",
    "                else:\n",
    "                    grid.append(0.0)\n",
    "                \n",
    "        road_bins.append(torch.Tensor(grid))\n",
    "        \n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.Tensor(x[0][i]))\n",
    "        \n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(256)\n",
    "        counts = np.zeros(90)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "#             if x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "            # Get its four bird's-eye view coordinates. \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "\n",
    "            # Grab the current bounding box. \n",
    "            current_bbs.append((xs, ys))\n",
    "\n",
    "            # Find the bin/grid cell it falls in, get its class mapping. \n",
    "            center_x, center_y = torch.mean(xs).item(), torch.mean(ys).item()\n",
    "            key = (round_down(center_x), round_down(center_y))\n",
    "            if key not in class_dict:\n",
    "                print(key)\n",
    "            bin_id = class_dict[key]\n",
    "            bins[bin_id] = 1\n",
    "            count += 1\n",
    "            \n",
    "        \n",
    "        counts[count] = 1\n",
    "\n",
    "        # Label Smoothing #\n",
    "        if count > 10 and count < 88:\n",
    "            counts[count+1] = 0.2\n",
    "            counts[count-1] = 0.2\n",
    "        target_counts.append(torch.Tensor(counts))\n",
    "        \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        target.append(torch.Tensor(bins))\n",
    "        \n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(target), torch.stack(road_maps), bbs, torch.stack(target_counts), torch.stack(road_bins)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=1, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.argmax(counts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx], nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    \n",
    "    draw_box(ax, class_box, 'green')\n",
    "    \n",
    "def append_first_to_last(tens):\n",
    "    ret = torch.cat((tens, torch.as_tensor([tens[0]])))\n",
    "    return ret\n",
    "\n",
    "    \n",
    "for bb in bbs[idx]:\n",
    "    ax.plot(append_first_to_last(bb[0]), append_first_to_last(bb[1]), color='orange')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "        self.segmentation = nn.Sequential(OrderedDict([\n",
    "            ('linear1_segmentation', nn.Linear(self.concat_dim, 25600)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_images = x.shape[1]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        # Reshape here\n",
    "        x = x.view(-1, channels, height, width)\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x), self.segmentation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel()\n",
    "\n",
    "# Weighting certain classes more. \n",
    "# positive_weight = torch.ones(256).to(device)\n",
    "# for (x, y), bin_id in class_dict.items():\n",
    "#     if abs(x - 400) <= 200 and abs(y - 400) <= 200:\n",
    "#         positive_weight[bin_id] = 2\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if(\"bn\" not in name):\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "# for layer in unfreeze_layers:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "        \n",
    "model = model.to(device)\n",
    "bin_criterion = nn.BCEWithLogitsLoss()\n",
    "count_criterion = nn.BCEWithLogitsLoss()\n",
    "segmentation_criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(device)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a.float()) + (1 - lam) * criterion(pred, y_b.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=10, num_workers=10, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    train_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    segmentation_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        \n",
    "#         sample, target_a, target_b, lam = mixup_data(sample, target)\n",
    "#         sample, target_a, target_b = map(torch.autograd.Variable, (sample, target_a, target_b))\n",
    "        \n",
    "    # Why were you doing this?\n",
    "#         batch_yhat = []\n",
    "#         batch_ycount = []\n",
    "#         for j, x in enumerate(sample):\n",
    "#             y_hat, y_count, segmentation = model(x)\n",
    "#             batch_yhat.append(y_hat)\n",
    "#             batch_ycount.append(y_count)\n",
    "        \n",
    "#         y_hat = torch.stack(batch_yhat).squeeze()\n",
    "#         y_count = torch.stack(batch_ycount).squeeze()\n",
    "        \n",
    "    \n",
    "        y_hat, y_count, segmentation = model(sample)\n",
    "        \n",
    "        # Mixup criterion here\n",
    "#         bin_loss = mixup_criterion(bin_criterion, y_hat, target_a, target_b, lam)\n",
    "        \n",
    "        bin_loss = bin_criterion(y_hat, target.float())\n",
    "        count_loss = count_criterion(y_count, target_count.float())\n",
    "        segmentation_loss = segmentation_criterion(segmentation, road_bins.float())\n",
    "        loss = bin_loss + 2 * count_loss + segmentation_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        bin_losses.append(bin_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "        segmentation_losses.append(segmentation_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                50. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    print(\"Average Train Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Train Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"Average Train Segmentation Epoch Loss: \", np.mean(segmentation_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    segmentation_losses = []\n",
    "    count_correct = 0\n",
    "    count_off_by_1 = 0\n",
    "    total_count = 0\n",
    "    bin_correct = 0\n",
    "    total_bins = 0\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             batch_yhat = []\n",
    "#             batch_ycount = []\n",
    "#             for j, x in enumerate(sample):\n",
    "#                 y_hat, y_count = model(x)\n",
    "#                 batch_yhat.append(y_hat)\n",
    "#                 batch_ycount.append(y_count)\n",
    "\n",
    "#             y_hat = torch.stack(batch_yhat).squeeze()\n",
    "#             y_count = torch.stack(batch_ycount).squeeze()\n",
    "            \n",
    "#             for j, x in enumerate(y_count):\n",
    "#                 pred_count = torch.argmax(y_count[j])\n",
    "#                 t_count = torch.argmax(target_count[j])\n",
    "#                 if pred_count == t_count:\n",
    "#                     count_correct += 1\n",
    "#                 elif abs(pred_count - t_count) == 1:\n",
    "#                     count_off_by_1 += 1\n",
    "                      \n",
    "#                 pred_bins = torch.topk(y_hat[j], k = pred_count).indices\n",
    "#                 t_bins = (target[j] == 1).nonzero()\n",
    "#                 for b in pred_bins:\n",
    "#                     if b in t_bins:\n",
    "#                         bin_correct += 1\n",
    "                    \n",
    "#                 total_bins += len(t_bins)\n",
    "            \n",
    "            y_hat, y_count, segmentation = model(sample)\n",
    "\n",
    "#             total_count += y_count.size(0)\n",
    "            \n",
    "            bin_loss = bin_criterion(y_hat, target.float())\n",
    "            count_loss = count_criterion(y_count, target_count.float())\n",
    "            segmentation_loss = segmentation_criterion(segmentation, road_bins.float())\n",
    "            loss = bin_loss + count_loss + segmentation_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            bin_losses.append(bin_loss.item())\n",
    "            count_losses.append(count_loss.item())\n",
    "            segmentation_losses.append(segmentation_loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Validation Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"Average Train Segmentation Epoch Loss: \", np.mean(segmentation_losses))\n",
    "#     print(\"\\tAverage Validation Count Accuracy: \", 100*count_correct/total_count)\n",
    "#     print(\"\\tAverage Validation Count-off-by-1 Accuracy: \", 100*count_off_by_1/total_count)\n",
    "#     if total_bins != 0:\n",
    "#         print(\"\\tAverage Validation Bin Accuracy: \", 100*bin_correct/total_bins)\n",
    "#     print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), '/scratch/brs426/all_six_images_classify_count_better.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3276 (0%)]\tLoss: 0.284711\n",
      "Train Epoch: 0 [500/3276 (8%)]\tLoss: 0.251558\n",
      "Train Epoch: 0 [1000/3276 (15%)]\tLoss: 0.242934\n",
      "Train Epoch: 0 [1500/3276 (23%)]\tLoss: 0.182964\n",
      "Train Epoch: 0 [2000/3276 (30%)]\tLoss: 0.239159\n",
      "Train Epoch: 0 [2500/3276 (38%)]\tLoss: 0.236224\n",
      "Train Epoch: 0 [3000/3276 (46%)]\tLoss: 0.191914\n",
      "\n",
      "Average Train Epoch Loss:  0.2678060893241952\n",
      "Average Train Bin Epoch Loss:  0.11329438929196174\n",
      "Average Train Count Epoch Loss:  0.021498083869540472\n",
      "Average Train Segmentation Epoch Loss:  0.11151553219093419\n",
      "Average Validation Epoch Loss:  0.507631428539753\n",
      "Average Validation Bin Epoch Loss:  0.2046801969408989\n",
      "Average Validation Count Epoch Loss:  0.08257488766685128\n",
      "Average Train Segmentation Epoch Loss:  0.2203763360157609\n",
      "Train Epoch: 1 [0/3276 (0%)]\tLoss: 0.227765\n",
      "Train Epoch: 1 [500/3276 (8%)]\tLoss: 0.258290\n",
      "Train Epoch: 1 [1000/3276 (15%)]\tLoss: 0.300365\n",
      "Train Epoch: 1 [1500/3276 (23%)]\tLoss: 0.251662\n",
      "Train Epoch: 1 [2000/3276 (30%)]\tLoss: 0.257154\n",
      "Train Epoch: 1 [2500/3276 (38%)]\tLoss: 0.308082\n",
      "Train Epoch: 1 [3000/3276 (46%)]\tLoss: 0.227013\n",
      "\n",
      "Average Train Epoch Loss:  0.2664699015184874\n",
      "Average Train Bin Epoch Loss:  0.11294054405809176\n",
      "Average Train Count Epoch Loss:  0.021205262970974352\n",
      "Average Train Segmentation Epoch Loss:  0.11111883192164142\n",
      "Average Validation Epoch Loss:  0.504282146692276\n",
      "Average Validation Bin Epoch Loss:  0.20751345623284578\n",
      "Average Validation Count Epoch Loss:  0.08721865201368928\n",
      "Average Train Segmentation Epoch Loss:  0.20955003332346678\n",
      "Train Epoch: 2 [0/3276 (0%)]\tLoss: 0.258771\n",
      "Train Epoch: 2 [500/3276 (8%)]\tLoss: 0.306969\n",
      "Train Epoch: 2 [1000/3276 (15%)]\tLoss: 0.252870\n",
      "Train Epoch: 2 [1500/3276 (23%)]\tLoss: 0.294462\n",
      "Train Epoch: 2 [2000/3276 (30%)]\tLoss: 0.243433\n",
      "Train Epoch: 2 [2500/3276 (38%)]\tLoss: 0.225869\n",
      "Train Epoch: 2 [3000/3276 (46%)]\tLoss: 0.197916\n",
      "\n",
      "Average Train Epoch Loss:  0.26623650581189773\n",
      "Average Train Bin Epoch Loss:  0.1131070770504998\n",
      "Average Train Count Epoch Loss:  0.021194303334440764\n",
      "Average Train Segmentation Epoch Loss:  0.1107408225127473\n",
      "Average Validation Epoch Loss:  0.5135972108691931\n",
      "Average Validation Bin Epoch Loss:  0.21013215091079473\n",
      "Average Validation Count Epoch Loss:  0.08286556834354997\n",
      "Average Train Segmentation Epoch Loss:  0.22059948835521936\n",
      "Train Epoch: 3 [0/3276 (0%)]\tLoss: 0.216507\n",
      "Train Epoch: 3 [500/3276 (8%)]\tLoss: 0.376634\n",
      "Train Epoch: 3 [1000/3276 (15%)]\tLoss: 0.242895\n",
      "Train Epoch: 3 [1500/3276 (23%)]\tLoss: 0.273707\n",
      "Train Epoch: 3 [2000/3276 (30%)]\tLoss: 0.208455\n",
      "Train Epoch: 3 [2500/3276 (38%)]\tLoss: 0.228887\n",
      "Train Epoch: 3 [3000/3276 (46%)]\tLoss: 0.264859\n",
      "\n",
      "Average Train Epoch Loss:  0.2640135862478396\n",
      "Average Train Bin Epoch Loss:  0.1119603734781466\n",
      "Average Train Count Epoch Loss:  0.021288792412478205\n",
      "Average Train Segmentation Epoch Loss:  0.10947562829114316\n",
      "Average Validation Epoch Loss:  0.49543012864887714\n",
      "Average Validation Bin Epoch Loss:  0.2068554349243641\n",
      "Average Validation Count Epoch Loss:  0.07984448526985943\n",
      "Average Train Segmentation Epoch Loss:  0.20873021334409714\n",
      "Train Epoch: 4 [0/3276 (0%)]\tLoss: 0.259826\n",
      "Train Epoch: 4 [500/3276 (8%)]\tLoss: 0.264715\n",
      "Train Epoch: 4 [1000/3276 (15%)]\tLoss: 0.273296\n",
      "Train Epoch: 4 [1500/3276 (23%)]\tLoss: 0.306762\n",
      "Train Epoch: 4 [2000/3276 (30%)]\tLoss: 0.261322\n",
      "Train Epoch: 4 [2500/3276 (38%)]\tLoss: 0.245737\n",
      "Train Epoch: 4 [3000/3276 (46%)]\tLoss: 0.267291\n",
      "\n",
      "Average Train Epoch Loss:  0.2619024748209773\n",
      "Average Train Bin Epoch Loss:  0.1118578977383128\n",
      "Average Train Count Epoch Loss:  0.020694822714686756\n",
      "Average Train Segmentation Epoch Loss:  0.10865493149428469\n",
      "Average Validation Epoch Loss:  0.5133983343839645\n",
      "Average Validation Bin Epoch Loss:  0.2099716505035758\n",
      "Average Validation Count Epoch Loss:  0.08858830062672496\n",
      "Average Train Segmentation Epoch Loss:  0.21483837999403477\n",
      "Train Epoch: 5 [0/3276 (0%)]\tLoss: 0.214415\n",
      "Train Epoch: 5 [500/3276 (8%)]\tLoss: 0.313417\n",
      "Train Epoch: 5 [1000/3276 (15%)]\tLoss: 0.204921\n",
      "Train Epoch: 5 [1500/3276 (23%)]\tLoss: 0.274853\n",
      "Train Epoch: 5 [2000/3276 (30%)]\tLoss: 0.372086\n",
      "Train Epoch: 5 [2500/3276 (38%)]\tLoss: 0.268327\n",
      "Train Epoch: 5 [3000/3276 (46%)]\tLoss: 0.342903\n",
      "\n",
      "Average Train Epoch Loss:  0.259955903924093\n",
      "Average Train Bin Epoch Loss:  0.11078139094681275\n",
      "Average Train Count Epoch Loss:  0.020602042548267578\n",
      "Average Train Segmentation Epoch Loss:  0.10797042875527972\n",
      "Average Validation Epoch Loss:  0.4890252146869898\n",
      "Average Validation Bin Epoch Loss:  0.20586044248193502\n",
      "Average Validation Count Epoch Loss:  0.08452104474417865\n",
      "Average Train Segmentation Epoch Loss:  0.19864372722804546\n",
      "Train Epoch: 6 [0/3276 (0%)]\tLoss: 0.259744\n",
      "Train Epoch: 6 [500/3276 (8%)]\tLoss: 0.302171\n",
      "Train Epoch: 6 [1000/3276 (15%)]\tLoss: 0.256534\n",
      "Train Epoch: 6 [1500/3276 (23%)]\tLoss: 0.206624\n",
      "Train Epoch: 6 [2000/3276 (30%)]\tLoss: 0.254358\n",
      "Train Epoch: 6 [2500/3276 (38%)]\tLoss: 0.270969\n",
      "Train Epoch: 6 [3000/3276 (46%)]\tLoss: 0.263499\n",
      "\n",
      "Average Train Epoch Loss:  0.25859232760238937\n",
      "Average Train Bin Epoch Loss:  0.11022170629670344\n",
      "Average Train Count Epoch Loss:  0.020499106022393014\n",
      "Average Train Segmentation Epoch Loss:  0.1073724098798887\n",
      "Average Validation Epoch Loss:  0.4971951860934496\n",
      "Average Validation Bin Epoch Loss:  0.20591626595705748\n",
      "Average Validation Count Epoch Loss:  0.08604728430509567\n",
      "Average Train Segmentation Epoch Loss:  0.20523164421319962\n",
      "Train Epoch: 7 [0/3276 (0%)]\tLoss: 0.254328\n",
      "Train Epoch: 7 [500/3276 (8%)]\tLoss: 0.274289\n",
      "Train Epoch: 7 [1000/3276 (15%)]\tLoss: 0.187538\n",
      "Train Epoch: 7 [1500/3276 (23%)]\tLoss: 0.214670\n",
      "Train Epoch: 7 [2000/3276 (30%)]\tLoss: 0.281835\n",
      "Train Epoch: 7 [2500/3276 (38%)]\tLoss: 0.239706\n",
      "Train Epoch: 7 [3000/3276 (46%)]\tLoss: 0.245675\n",
      "\n",
      "Average Train Epoch Loss:  0.25632383074702286\n",
      "Average Train Bin Epoch Loss:  0.10962604909635536\n",
      "Average Train Count Epoch Loss:  0.01989980088830812\n",
      "Average Train Segmentation Epoch Loss:  0.10689817995923322\n",
      "Average Validation Epoch Loss:  0.5009636022150517\n",
      "Average Validation Bin Epoch Loss:  0.2064978089183569\n",
      "Average Validation Count Epoch Loss:  0.08430197322741151\n",
      "Average Train Segmentation Epoch Loss:  0.21016381680965424\n",
      "Train Epoch: 8 [0/3276 (0%)]\tLoss: 0.205165\n",
      "Train Epoch: 8 [500/3276 (8%)]\tLoss: 0.220186\n",
      "Train Epoch: 8 [1000/3276 (15%)]\tLoss: 0.306028\n",
      "Train Epoch: 8 [1500/3276 (23%)]\tLoss: 0.306496\n",
      "Train Epoch: 8 [2000/3276 (30%)]\tLoss: 0.229391\n",
      "Train Epoch: 8 [2500/3276 (38%)]\tLoss: 0.287618\n",
      "Train Epoch: 8 [3000/3276 (46%)]\tLoss: 0.236085\n",
      "\n",
      "Average Train Epoch Loss:  0.2568569211516438\n",
      "Average Train Bin Epoch Loss:  0.10925873819874918\n",
      "Average Train Count Epoch Loss:  0.01982158544564211\n",
      "Average Train Segmentation Epoch Loss:  0.10795501190260416\n",
      "Average Validation Epoch Loss:  0.504298709332943\n",
      "Average Validation Bin Epoch Loss:  0.20813284255564213\n",
      "Average Validation Count Epoch Loss:  0.08565323101356626\n",
      "Average Train Segmentation Epoch Loss:  0.21051263622939587\n",
      "Train Epoch: 9 [0/3276 (0%)]\tLoss: 0.272159\n",
      "Train Epoch: 9 [500/3276 (8%)]\tLoss: 0.228872\n",
      "Train Epoch: 9 [1000/3276 (15%)]\tLoss: 0.285214\n",
      "Train Epoch: 9 [1500/3276 (23%)]\tLoss: 0.232984\n",
      "Train Epoch: 9 [2000/3276 (30%)]\tLoss: 0.231270\n",
      "Train Epoch: 9 [2500/3276 (38%)]\tLoss: 0.278114\n",
      "Train Epoch: 9 [3000/3276 (46%)]\tLoss: 0.281182\n",
      "\n",
      "Average Train Epoch Loss:  0.25451949488644193\n",
      "Average Train Bin Epoch Loss:  0.10881115471171897\n",
      "Average Train Count Epoch Loss:  0.019775029189498503\n",
      "Average Train Segmentation Epoch Loss:  0.1061582815969681\n",
      "Average Validation Epoch Loss:  0.47450533136725426\n",
      "Average Validation Bin Epoch Loss:  0.20231780130416155\n",
      "Average Validation Count Epoch Loss:  0.07816214906051755\n",
      "Average Train Segmentation Epoch Loss:  0.1940253833308816\n",
      "Train Epoch: 10 [0/3276 (0%)]\tLoss: 0.307709\n",
      "Train Epoch: 10 [500/3276 (8%)]\tLoss: 0.216949\n",
      "Train Epoch: 10 [1000/3276 (15%)]\tLoss: 0.248760\n",
      "Train Epoch: 10 [1500/3276 (23%)]\tLoss: 0.251575\n",
      "Train Epoch: 10 [2000/3276 (30%)]\tLoss: 0.197041\n",
      "Train Epoch: 10 [2500/3276 (38%)]\tLoss: 0.276181\n",
      "Train Epoch: 10 [3000/3276 (46%)]\tLoss: 0.238930\n",
      "\n",
      "Average Train Epoch Loss:  0.25521298920417707\n",
      "Average Train Bin Epoch Loss:  0.10881854254161803\n",
      "Average Train Count Epoch Loss:  0.019908820307336567\n",
      "Average Train Segmentation Epoch Loss:  0.10657680669526864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Epoch Loss:  0.5100494138896465\n",
      "Average Validation Bin Epoch Loss:  0.20818653609603643\n",
      "Average Validation Count Epoch Loss:  0.08449621731415391\n",
      "Average Train Segmentation Epoch Loss:  0.21736666187644005\n",
      "Train Epoch: 11 [0/3276 (0%)]\tLoss: 0.194991\n",
      "Train Epoch: 11 [500/3276 (8%)]\tLoss: 0.259093\n",
      "Train Epoch: 11 [1000/3276 (15%)]\tLoss: 0.243502\n",
      "Train Epoch: 11 [1500/3276 (23%)]\tLoss: 0.212282\n",
      "Train Epoch: 11 [2000/3276 (30%)]\tLoss: 0.183072\n",
      "Train Epoch: 11 [2500/3276 (38%)]\tLoss: 0.209177\n",
      "Train Epoch: 11 [3000/3276 (46%)]\tLoss: 0.286275\n",
      "\n",
      "Average Train Epoch Loss:  0.25187753954129977\n",
      "Average Train Bin Epoch Loss:  0.10742340526520842\n",
      "Average Train Count Epoch Loss:  0.019428862403601227\n",
      "Average Train Segmentation Epoch Loss:  0.1055964089975488\n",
      "Average Validation Epoch Loss:  0.4932816121727228\n",
      "Average Validation Bin Epoch Loss:  0.20494049601256847\n",
      "Average Validation Count Epoch Loss:  0.08189560193568468\n",
      "Average Train Segmentation Epoch Loss:  0.20644552074372768\n",
      "Train Epoch: 12 [0/3276 (0%)]\tLoss: 0.206776\n",
      "Train Epoch: 12 [500/3276 (8%)]\tLoss: 0.260935\n",
      "Train Epoch: 12 [1000/3276 (15%)]\tLoss: 0.263143\n",
      "Train Epoch: 12 [1500/3276 (23%)]\tLoss: 0.268072\n",
      "Train Epoch: 12 [2000/3276 (30%)]\tLoss: 0.294675\n",
      "Train Epoch: 12 [2500/3276 (38%)]\tLoss: 0.178870\n",
      "Train Epoch: 12 [3000/3276 (46%)]\tLoss: 0.188484\n",
      "\n",
      "Average Train Epoch Loss:  0.2521494542952718\n",
      "Average Train Bin Epoch Loss:  0.10757170828831632\n",
      "Average Train Count Epoch Loss:  0.019479766167223272\n",
      "Average Train Segmentation Epoch Loss:  0.10561821300808977\n",
      "Average Validation Epoch Loss:  0.4957782756537199\n",
      "Average Validation Bin Epoch Loss:  0.20603241957724094\n",
      "Average Validation Count Epoch Loss:  0.0882456568069756\n",
      "Average Train Segmentation Epoch Loss:  0.20150019880384207\n",
      "Train Epoch: 13 [0/3276 (0%)]\tLoss: 0.180429\n",
      "Train Epoch: 13 [500/3276 (8%)]\tLoss: 0.284752\n",
      "Train Epoch: 13 [1000/3276 (15%)]\tLoss: 0.217579\n",
      "Train Epoch: 13 [1500/3276 (23%)]\tLoss: 0.237929\n",
      "Train Epoch: 13 [2000/3276 (30%)]\tLoss: 0.263389\n",
      "Train Epoch: 13 [2500/3276 (38%)]\tLoss: 0.255955\n",
      "Train Epoch: 13 [3000/3276 (46%)]\tLoss: 0.187535\n",
      "\n",
      "Average Train Epoch Loss:  0.2493093595726461\n",
      "Average Train Bin Epoch Loss:  0.10669932096469693\n",
      "Average Train Count Epoch Loss:  0.019018699964451626\n",
      "Average Train Segmentation Epoch Loss:  0.10457263893743114\n",
      "Average Validation Epoch Loss:  0.5086581259965897\n",
      "Average Validation Bin Epoch Loss:  0.2129304287955165\n",
      "Average Validation Count Epoch Loss:  0.09008638421073556\n",
      "Average Train Segmentation Epoch Loss:  0.2056413171812892\n",
      "Train Epoch: 14 [0/3276 (0%)]\tLoss: 0.202847\n",
      "Train Epoch: 14 [500/3276 (8%)]\tLoss: 0.218336\n",
      "Train Epoch: 14 [1000/3276 (15%)]\tLoss: 0.222243\n",
      "Train Epoch: 14 [1500/3276 (23%)]\tLoss: 0.160052\n",
      "Train Epoch: 14 [2000/3276 (30%)]\tLoss: 0.348686\n",
      "Train Epoch: 14 [2500/3276 (38%)]\tLoss: 0.232384\n",
      "Train Epoch: 14 [3000/3276 (46%)]\tLoss: 0.226504\n",
      "\n",
      "Average Train Epoch Loss:  0.24779759106657853\n",
      "Average Train Bin Epoch Loss:  0.10642978778426967\n",
      "Average Train Count Epoch Loss:  0.01898435838577315\n",
      "Average Train Segmentation Epoch Loss:  0.1033990864596534\n",
      "Average Validation Epoch Loss:  0.5215872135013342\n",
      "Average Validation Bin Epoch Loss:  0.21292984019964933\n",
      "Average Validation Count Epoch Loss:  0.0911486204713583\n",
      "Average Train Segmentation Epoch Loss:  0.21750874444842339\n",
      "Train Epoch: 15 [0/3276 (0%)]\tLoss: 0.213548\n",
      "Train Epoch: 15 [500/3276 (8%)]\tLoss: 0.184319\n",
      "Train Epoch: 15 [1000/3276 (15%)]\tLoss: 0.225030\n",
      "Train Epoch: 15 [1500/3276 (23%)]\tLoss: 0.256162\n",
      "Train Epoch: 15 [2000/3276 (30%)]\tLoss: 0.234292\n",
      "Train Epoch: 15 [2500/3276 (38%)]\tLoss: 0.242983\n",
      "Train Epoch: 15 [3000/3276 (46%)]\tLoss: 0.279646\n",
      "\n",
      "Average Train Epoch Loss:  0.24618797054196276\n",
      "Average Train Bin Epoch Loss:  0.10571932668865817\n",
      "Average Train Count Epoch Loss:  0.018790048910532056\n",
      "Average Train Segmentation Epoch Loss:  0.10288854544164568\n",
      "Average Validation Epoch Loss:  0.5115475282073021\n",
      "Average Validation Bin Epoch Loss:  0.2131066843867302\n",
      "Average Validation Count Epoch Loss:  0.09069907339289784\n",
      "Average Train Segmentation Epoch Loss:  0.20774177461862564\n",
      "Train Epoch: 16 [0/3276 (0%)]\tLoss: 0.260899\n",
      "Train Epoch: 16 [500/3276 (8%)]\tLoss: 0.174282\n",
      "Train Epoch: 16 [1000/3276 (15%)]\tLoss: 0.278337\n",
      "Train Epoch: 16 [1500/3276 (23%)]\tLoss: 0.241999\n",
      "Train Epoch: 16 [2000/3276 (30%)]\tLoss: 0.227567\n",
      "Train Epoch: 16 [2500/3276 (38%)]\tLoss: 0.309669\n",
      "Train Epoch: 16 [3000/3276 (46%)]\tLoss: 0.258709\n",
      "\n",
      "Average Train Epoch Loss:  0.24676910865052445\n",
      "Average Train Bin Epoch Loss:  0.10598581781747138\n",
      "Average Train Count Epoch Loss:  0.01881167199888562\n",
      "Average Train Segmentation Epoch Loss:  0.10315994649739345\n",
      "Average Validation Epoch Loss:  0.51035644300282\n",
      "Average Validation Bin Epoch Loss:  0.21100437827408314\n",
      "Average Validation Count Epoch Loss:  0.08632605662569404\n",
      "Average Train Segmentation Epoch Loss:  0.2130260057747364\n",
      "Train Epoch: 17 [0/3276 (0%)]\tLoss: 0.205192\n",
      "Train Epoch: 17 [500/3276 (8%)]\tLoss: 0.317102\n",
      "Train Epoch: 17 [1000/3276 (15%)]\tLoss: 0.221099\n",
      "Train Epoch: 17 [1500/3276 (23%)]\tLoss: 0.224874\n",
      "Train Epoch: 17 [2000/3276 (30%)]\tLoss: 0.272522\n",
      "Train Epoch: 17 [2500/3276 (38%)]\tLoss: 0.224745\n",
      "Train Epoch: 17 [3000/3276 (46%)]\tLoss: 0.244863\n",
      "\n",
      "Average Train Epoch Loss:  0.24412893213149978\n",
      "Average Train Bin Epoch Loss:  0.10462654719310926\n",
      "Average Train Count Epoch Loss:  0.01859780047770317\n",
      "Average Train Segmentation Epoch Loss:  0.1023067839489114\n",
      "Average Validation Epoch Loss:  0.4925915598869324\n",
      "Average Validation Bin Epoch Loss:  0.20844423584640026\n",
      "Average Validation Count Epoch Loss:  0.088020128197968\n",
      "Average Train Segmentation Epoch Loss:  0.19612720049917698\n",
      "Train Epoch: 18 [0/3276 (0%)]\tLoss: 0.178306\n",
      "Train Epoch: 18 [500/3276 (8%)]\tLoss: 0.260335\n",
      "Train Epoch: 18 [1000/3276 (15%)]\tLoss: 0.268488\n",
      "Train Epoch: 18 [1500/3276 (23%)]\tLoss: 0.249891\n",
      "Train Epoch: 18 [2000/3276 (30%)]\tLoss: 0.222983\n",
      "Train Epoch: 18 [2500/3276 (38%)]\tLoss: 0.223857\n",
      "Train Epoch: 18 [3000/3276 (46%)]\tLoss: 0.253889\n",
      "\n",
      "Average Train Epoch Loss:  0.24426870293370107\n",
      "Average Train Bin Epoch Loss:  0.10478213886026203\n",
      "Average Train Count Epoch Loss:  0.018519833008190843\n",
      "Average Train Segmentation Epoch Loss:  0.1024468987498705\n",
      "Average Validation Epoch Loss:  0.5027310065925121\n",
      "Average Validation Bin Epoch Loss:  0.21091038081794977\n",
      "Average Validation Count Epoch Loss:  0.08739253412932158\n",
      "Average Train Segmentation Epoch Loss:  0.20442808791995049\n",
      "Train Epoch: 19 [0/3276 (0%)]\tLoss: 0.314682\n",
      "Train Epoch: 19 [500/3276 (8%)]\tLoss: 0.222647\n",
      "Train Epoch: 19 [1000/3276 (15%)]\tLoss: 0.208804\n",
      "Train Epoch: 19 [1500/3276 (23%)]\tLoss: 0.236853\n",
      "Train Epoch: 19 [2000/3276 (30%)]\tLoss: 0.167141\n",
      "Train Epoch: 19 [2500/3276 (38%)]\tLoss: 0.172962\n",
      "Train Epoch: 19 [3000/3276 (46%)]\tLoss: 0.294994\n",
      "\n",
      "Average Train Epoch Loss:  0.24326588676833524\n",
      "Average Train Bin Epoch Loss:  0.1044612687489972\n",
      "Average Train Count Epoch Loss:  0.01843278177037108\n",
      "Average Train Segmentation Epoch Loss:  0.10193905528498495\n",
      "Average Validation Epoch Loss:  0.5001830980181694\n",
      "Average Validation Bin Epoch Loss:  0.2106807380914688\n",
      "Average Validation Count Epoch Loss:  0.089981475379318\n",
      "Average Train Segmentation Epoch Loss:  0.19952088128775358\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 x 200\n",
    "# 10 x 1200\n",
    "x = torch.randn((12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute threat scores\n",
    "def reconstruct_from_bins(bins, block_size, threshold):\n",
    "    print(bins.shape)\n",
    "    road_map = torch.zeros((800, 800))\n",
    "    idx = 0\n",
    "    for x in range(0, 800, block_size):\n",
    "        for y in range(0, 800, block_size):\n",
    "            road_map[x:x+block_size, y:y+block_size] = bins[idx]\n",
    "            idx += 1\n",
    "    return road_map > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel().to(device)\n",
    "model.load_state_dict(torch.load('/scratch/brs426/all_six_images_classify_count_better.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "Average threat score tensor(0.8504)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "threat_scores = 0\n",
    "threshold = 0.4\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(val_loader):\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        \n",
    "        y_hat, y_count, segmentation = model(sample)\n",
    "        segmentation = segmentation.squeeze()\n",
    "        road_img = road_img.squeeze()\n",
    "        \n",
    "        reconstructed_road_map = reconstruct_from_bins(segmentation, 5, threshold).cpu()\n",
    "        ts_road_map = compute_ts_road_map(road_img,reconstructed_road_map)\n",
    "        threat_scores += ts_road_map\n",
    "    \n",
    "    print(\"Average threat score\", threat_scores / len(val_loader))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(tensor([720.7258, 720.4731, 674.0170, 674.2697], dtype=torch.float64), tensor([284.0238, 304.5063, 303.9379, 283.4554], dtype=torch.float64)), (tensor([348.8326, 348.8452, 394.3148, 394.3023], dtype=torch.float64), tensor([248.6888, 230.6341, 230.6614, 248.7162], dtype=torch.float64)), (tensor([754.6695, 754.4052, 707.3991, 707.6634], dtype=torch.float64), tensor([319.4501, 340.8722, 340.2971, 318.8749], dtype=torch.float64)), (tensor([363.8959, 364.1186, 409.5848, 409.3621], dtype=torch.float64), tensor([212.2098, 194.1564, 194.7127, 212.7661], dtype=torch.float64)), (tensor([64.8990, 65.0014, 17.0024, 16.9000], dtype=torch.float64), tensor([316.6336, 336.6375, 336.8878, 316.8838], dtype=torch.float64)), (tensor([573.0020, 573.1035, 620.2725, 620.1709], dtype=torch.float64), tensor([182.1661, 163.1918, 163.4397, 182.4139], dtype=torch.float64)), (tensor([98.3245, 98.0880, 54.1317, 54.3682], dtype=torch.float64), tensor([387.1317, 406.3047, 405.7669, 386.5939], dtype=torch.float64)), (tensor([495.9951, 496.2178, 543.3238, 543.1011], dtype=torch.float64), tensor([242.7653, 224.7119, 225.2882, 243.3416], dtype=torch.float64)), (tensor([728.4811, 711.8671, 730.0442, 746.6582], dtype=torch.float64), tensor([761.5901, 754.6416, 711.2122, 718.1607], dtype=torch.float64)), (tensor([576.1045, 575.8825, 533.6161, 533.8380], dtype=torch.float64), tensor([317.3620, 335.3555, 334.8383, 316.8449], dtype=torch.float64)), (tensor([763.5354, 762.9483, 770.2697, 770.8567], dtype=torch.float64), tensor([463.8041, 455.5775, 455.0546, 463.2813], dtype=torch.float64)), (tensor([244.9134, 244.6840, 200.7276, 200.9571], dtype=torch.float64), tensor([351.9633, 370.5666, 370.0288, 351.4255], dtype=torch.float64)), (tensor([119.0978, 118.8738,  74.9175,  75.1414], dtype=torch.float64), tensor([351.2849, 369.4383, 368.9005, 350.7471], dtype=torch.float64)), (tensor([388.6209, 388.1447, 314.7551, 315.2313], dtype=torch.float64), tensor([354.0510, 374.5396, 372.8416, 352.3530], dtype=torch.float64)), (tensor([630.8775, 630.6342, 584.0681, 584.3114], dtype=torch.float64), tensor([389.9339, 409.6567, 409.0870, 389.3642], dtype=torch.float64)), (tensor([403.2959, 403.0692, 359.2329, 359.4595], dtype=torch.float64), tensor([316.3213, 334.6946, 334.1583, 315.7850], dtype=torch.float64)), (tensor([302.6404, 302.4174, 258.1211, 258.3441], dtype=torch.float64), tensor([389.8374, 407.9208, 407.3789, 389.2955], dtype=torch.float64))]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3b4a71e566da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mbb_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mts_bounding_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_ats_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mthreat_scores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mts_bounding_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Self-Driving/helper.py\u001b[0m in \u001b[0;36mcompute_ats_bounding_boxes\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_ats_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mnum_boxes1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mnum_boxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mboxes1_max_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))\n",
    "\n",
    "\n",
    "def get_bounding_boxes(self, samples):\n",
    "\n",
    "    # samples is (batch_size, 6, 3, 256, 306)      \n",
    "\n",
    "    bb_samples = []\n",
    "\n",
    "    for x in samples:\n",
    "        preds_class, preds_count = self.model.detection(x) \n",
    "\n",
    "        result = torch.topk(preds_class, k = torch.argmax(preds_count).item())\n",
    "        pred_ids = result.indices\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            buck_x, buck_y = reverse_class_dict[idx.item()]\n",
    "\n",
    "            xs = torch.Tensor([buck_x, buck_x, buck_x + 50, buck_x + 50]).double()\n",
    "            ys = torch.Tensor([buck_y+16, buck_y+36, buck_y+16, buck_y+36]).double()\n",
    "\n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "\n",
    "            xs /= 10.\n",
    "            ys /= 10.\n",
    "\n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "\n",
    "        bounding_boxes = torch.stack(bounding_boxes).double().cuda()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "\n",
    "    return tuple(bb_samples)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "threat_scores = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    bb_samples = []\n",
    "    for i, (sample, target, road_img, bbs, target_count, road_bins) in enumerate(val_loader):\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        road_bins = road_bins.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        \n",
    "        y_hat, y_count, segmentation = model(sample)\n",
    "        \n",
    "        result = torch.topk(y_hat, k = torch.argmax(y_count).item())\n",
    "        pred_ids = result.indices\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            buck_x, buck_y = reverse_class_dict[idx.item()]\n",
    "\n",
    "            xs = torch.Tensor([buck_x, buck_x, buck_x + 50, buck_x + 50]).double()\n",
    "            ys = torch.Tensor([buck_y+16, buck_y+36, buck_y+16, buck_y+36]).double()\n",
    "\n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "\n",
    "            xs /= 10.\n",
    "            ys /= 10.\n",
    "\n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "\n",
    "        bounding_boxes = torch.stack(bounding_boxes).double().cuda()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "        bb_samples = tuple(bb_samples)\n",
    "        \n",
    "        print(bbs)\n",
    "        bb_samples = bb_samples[0].cpu()\n",
    "        ts_bounding_box = compute_ats_bounding_boxes(bb_samples, bbs)\n",
    "        \n",
    "        threat_scores += ts_bounding_box\n",
    "    \n",
    "    print(\"Average threat score\", threat_scores / len(val_loader))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.253 val bin loss\n",
    "\n",
    "# Random Affine 3 degrees\n",
    "# 0.263 val bin loss\n",
    "\n",
    "\n",
    "# Need to do 5 * bin_loss + count_loss or something like that. Also more extreme Random Affine maybe?\n",
    "\n",
    "# Random Affine 5 degrees\n",
    "# 0.266 val bin loss\n",
    "\n",
    "# Took out Random Affine. \n",
    "# 0.268 val bin loss\n",
    "\n",
    "# Increased compress dim from 128 to 200. \n",
    "# 0.259 val bin loss\n",
    "\n",
    "# 5 * bin_loss + count_loss\n",
    "# 0.249 + 0.055\n",
    "\n",
    "# 8 *\n",
    "# 0.251 + 0.054\n",
    "\n",
    "# 8*, RandomAffine(3)\n",
    "# 0.255\n",
    "\n",
    "# 8*, RandomAffine(3), weight_decay 0.1\n",
    "\n",
    "# 10 *, RandomAffine(3)\n",
    "# 0.259\n",
    "\n",
    "# 8 *, Normalize (mean, std)\n",
    "# 0.26\n",
    "\n",
    "# 8 *, Dropout\n",
    "# 0.241, 0.253\n",
    "\n",
    "# 5 *, Dropout\n",
    "# 0.254\n",
    "\n",
    "# 11 *, Dropout\n",
    "# 0.249\n",
    "\n",
    "# Want to try positive-weights for classes within 200 to 600. \n",
    "# Want to get the model to get those classes correct. \n",
    "\n",
    "# Mixup 0.2, 1 *, Dropout\n",
    "# (0.244, 0.053), \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "model.load_state_dict(torch.load('all_six_images_classify_count.pt'))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(samples):\n",
    "    \n",
    "    # samples is (batch_size, 6, 3, 256, 306)\n",
    "    \n",
    "    # You need to return a tuple with size batch_size and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of bounding boxes. \n",
    "    \n",
    "    # Okay so I have my model. \n",
    "    # \n",
    "    \n",
    "    bb_samples = []\n",
    "    \n",
    "    for x in samples:\n",
    "        preds_class, preds_count = model(x)\n",
    "        \n",
    "        # preds class is a 256-dimensional tensor, filled with probabilities\n",
    "        # I need to find the `preds_count` top indices with the top values.\n",
    "        \n",
    "        \n",
    "        result = torch.topk(preds_class, k = torch.argmax(preds_count).item())\n",
    "        pred_ids = result.indices\n",
    "        \n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            buck_x, buck_y = reverse_class_dict[idx.item()]\n",
    "            \n",
    "            xs = torch.as_tensor([buck_x, buck_x, buck_x + 50, buck_x + 50])\n",
    "            ys = torch.as_tensor([buck_y+16, buck_y+36, buck_y+16, buck_y+36])\n",
    "            \n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "            \n",
    "            xs /= 10\n",
    "            ys /= 10\n",
    "               \n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "            \n",
    "        bounding_boxes = torch.stack(bounding_boxes).cuda()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "    \n",
    "    return tuple(bb_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(val_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = get_bounding_boxes(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_preds = torch.sigmoid(model(sample[idx])[0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx].cpu().detach(), nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "\n",
    "bin_ids = (sigmoid_preds > 0.25).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'red')\n",
    "    \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'green')\n",
    "\n",
    "    \n",
    "for bb in boom[idx]:\n",
    "    box = bb.cpu().detach()\n",
    "    draw_box(ax, box, 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([box[:, 0], box[:, 1], box[:, 3], box[:, 2], box[:, 0]])\n",
    "\n",
    "def draw_box(ax, corners, color):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    \n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "    return point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vish_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
