{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/vr1059/self-driving-data/data'\n",
    "annotation_csv = '/scratch/vr1059/self-driving-data/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 50): 1,\n",
       " (0, 100): 2,\n",
       " (0, 150): 3,\n",
       " (0, 200): 4,\n",
       " (0, 250): 5,\n",
       " (0, 300): 6,\n",
       " (0, 350): 7,\n",
       " (0, 400): 8,\n",
       " (0, 450): 9,\n",
       " (0, 500): 10,\n",
       " (0, 550): 11,\n",
       " (0, 600): 12,\n",
       " (0, 650): 13,\n",
       " (0, 700): 14,\n",
       " (0, 750): 15,\n",
       " (50, 0): 16,\n",
       " (50, 50): 17,\n",
       " (50, 100): 18,\n",
       " (50, 150): 19,\n",
       " (50, 200): 20,\n",
       " (50, 250): 21,\n",
       " (50, 300): 22,\n",
       " (50, 350): 23,\n",
       " (50, 400): 24,\n",
       " (50, 450): 25,\n",
       " (50, 500): 26,\n",
       " (50, 550): 27,\n",
       " (50, 600): 28,\n",
       " (50, 650): 29,\n",
       " (50, 700): 30,\n",
       " (50, 750): 31,\n",
       " (100, 0): 32,\n",
       " (100, 50): 33,\n",
       " (100, 100): 34,\n",
       " (100, 150): 35,\n",
       " (100, 200): 36,\n",
       " (100, 250): 37,\n",
       " (100, 300): 38,\n",
       " (100, 350): 39,\n",
       " (100, 400): 40,\n",
       " (100, 450): 41,\n",
       " (100, 500): 42,\n",
       " (100, 550): 43,\n",
       " (100, 600): 44,\n",
       " (100, 650): 45,\n",
       " (100, 700): 46,\n",
       " (100, 750): 47,\n",
       " (150, 0): 48,\n",
       " (150, 50): 49,\n",
       " (150, 100): 50,\n",
       " (150, 150): 51,\n",
       " (150, 200): 52,\n",
       " (150, 250): 53,\n",
       " (150, 300): 54,\n",
       " (150, 350): 55,\n",
       " (150, 400): 56,\n",
       " (150, 450): 57,\n",
       " (150, 500): 58,\n",
       " (150, 550): 59,\n",
       " (150, 600): 60,\n",
       " (150, 650): 61,\n",
       " (150, 700): 62,\n",
       " (150, 750): 63,\n",
       " (200, 0): 64,\n",
       " (200, 50): 65,\n",
       " (200, 100): 66,\n",
       " (200, 150): 67,\n",
       " (200, 200): 68,\n",
       " (200, 250): 69,\n",
       " (200, 300): 70,\n",
       " (200, 350): 71,\n",
       " (200, 400): 72,\n",
       " (200, 450): 73,\n",
       " (200, 500): 74,\n",
       " (200, 550): 75,\n",
       " (200, 600): 76,\n",
       " (200, 650): 77,\n",
       " (200, 700): 78,\n",
       " (200, 750): 79,\n",
       " (250, 0): 80,\n",
       " (250, 50): 81,\n",
       " (250, 100): 82,\n",
       " (250, 150): 83,\n",
       " (250, 200): 84,\n",
       " (250, 250): 85,\n",
       " (250, 300): 86,\n",
       " (250, 350): 87,\n",
       " (250, 400): 88,\n",
       " (250, 450): 89,\n",
       " (250, 500): 90,\n",
       " (250, 550): 91,\n",
       " (250, 600): 92,\n",
       " (250, 650): 93,\n",
       " (250, 700): 94,\n",
       " (250, 750): 95,\n",
       " (300, 0): 96,\n",
       " (300, 50): 97,\n",
       " (300, 100): 98,\n",
       " (300, 150): 99,\n",
       " (300, 200): 100,\n",
       " (300, 250): 101,\n",
       " (300, 300): 102,\n",
       " (300, 350): 103,\n",
       " (300, 400): 104,\n",
       " (300, 450): 105,\n",
       " (300, 500): 106,\n",
       " (300, 550): 107,\n",
       " (300, 600): 108,\n",
       " (300, 650): 109,\n",
       " (300, 700): 110,\n",
       " (300, 750): 111,\n",
       " (350, 0): 112,\n",
       " (350, 50): 113,\n",
       " (350, 100): 114,\n",
       " (350, 150): 115,\n",
       " (350, 200): 116,\n",
       " (350, 250): 117,\n",
       " (350, 300): 118,\n",
       " (350, 350): 119,\n",
       " (350, 400): 120,\n",
       " (350, 450): 121,\n",
       " (350, 500): 122,\n",
       " (350, 550): 123,\n",
       " (350, 600): 124,\n",
       " (350, 650): 125,\n",
       " (350, 700): 126,\n",
       " (350, 750): 127,\n",
       " (400, 0): 128,\n",
       " (400, 50): 129,\n",
       " (400, 100): 130,\n",
       " (400, 150): 131,\n",
       " (400, 200): 132,\n",
       " (400, 250): 133,\n",
       " (400, 300): 134,\n",
       " (400, 350): 135,\n",
       " (400, 400): 136,\n",
       " (400, 450): 137,\n",
       " (400, 500): 138,\n",
       " (400, 550): 139,\n",
       " (400, 600): 140,\n",
       " (400, 650): 141,\n",
       " (400, 700): 142,\n",
       " (400, 750): 143,\n",
       " (450, 0): 144,\n",
       " (450, 50): 145,\n",
       " (450, 100): 146,\n",
       " (450, 150): 147,\n",
       " (450, 200): 148,\n",
       " (450, 250): 149,\n",
       " (450, 300): 150,\n",
       " (450, 350): 151,\n",
       " (450, 400): 152,\n",
       " (450, 450): 153,\n",
       " (450, 500): 154,\n",
       " (450, 550): 155,\n",
       " (450, 600): 156,\n",
       " (450, 650): 157,\n",
       " (450, 700): 158,\n",
       " (450, 750): 159,\n",
       " (500, 0): 160,\n",
       " (500, 50): 161,\n",
       " (500, 100): 162,\n",
       " (500, 150): 163,\n",
       " (500, 200): 164,\n",
       " (500, 250): 165,\n",
       " (500, 300): 166,\n",
       " (500, 350): 167,\n",
       " (500, 400): 168,\n",
       " (500, 450): 169,\n",
       " (500, 500): 170,\n",
       " (500, 550): 171,\n",
       " (500, 600): 172,\n",
       " (500, 650): 173,\n",
       " (500, 700): 174,\n",
       " (500, 750): 175,\n",
       " (550, 0): 176,\n",
       " (550, 50): 177,\n",
       " (550, 100): 178,\n",
       " (550, 150): 179,\n",
       " (550, 200): 180,\n",
       " (550, 250): 181,\n",
       " (550, 300): 182,\n",
       " (550, 350): 183,\n",
       " (550, 400): 184,\n",
       " (550, 450): 185,\n",
       " (550, 500): 186,\n",
       " (550, 550): 187,\n",
       " (550, 600): 188,\n",
       " (550, 650): 189,\n",
       " (550, 700): 190,\n",
       " (550, 750): 191,\n",
       " (600, 0): 192,\n",
       " (600, 50): 193,\n",
       " (600, 100): 194,\n",
       " (600, 150): 195,\n",
       " (600, 200): 196,\n",
       " (600, 250): 197,\n",
       " (600, 300): 198,\n",
       " (600, 350): 199,\n",
       " (600, 400): 200,\n",
       " (600, 450): 201,\n",
       " (600, 500): 202,\n",
       " (600, 550): 203,\n",
       " (600, 600): 204,\n",
       " (600, 650): 205,\n",
       " (600, 700): 206,\n",
       " (600, 750): 207,\n",
       " (650, 0): 208,\n",
       " (650, 50): 209,\n",
       " (650, 100): 210,\n",
       " (650, 150): 211,\n",
       " (650, 200): 212,\n",
       " (650, 250): 213,\n",
       " (650, 300): 214,\n",
       " (650, 350): 215,\n",
       " (650, 400): 216,\n",
       " (650, 450): 217,\n",
       " (650, 500): 218,\n",
       " (650, 550): 219,\n",
       " (650, 600): 220,\n",
       " (650, 650): 221,\n",
       " (650, 700): 222,\n",
       " (650, 750): 223,\n",
       " (700, 0): 224,\n",
       " (700, 50): 225,\n",
       " (700, 100): 226,\n",
       " (700, 150): 227,\n",
       " (700, 200): 228,\n",
       " (700, 250): 229,\n",
       " (700, 300): 230,\n",
       " (700, 350): 231,\n",
       " (700, 400): 232,\n",
       " (700, 450): 233,\n",
       " (700, 500): 234,\n",
       " (700, 550): 235,\n",
       " (700, 600): 236,\n",
       " (700, 650): 237,\n",
       " (700, 700): 238,\n",
       " (700, 750): 239,\n",
       " (750, 0): 240,\n",
       " (750, 50): 241,\n",
       " (750, 100): 242,\n",
       " (750, 150): 243,\n",
       " (750, 200): 244,\n",
       " (750, 250): 245,\n",
       " (750, 300): 246,\n",
       " (750, 350): 247,\n",
       " (750, 400): 248,\n",
       " (750, 450): 249,\n",
       " (750, 500): 250,\n",
       " (750, 550): 251,\n",
       " (750, 600): 252,\n",
       " (750, 650): 253,\n",
       " (750, 700): 254,\n",
       " (750, 750): 255}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 350)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_class_dict[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        \n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.Tensor(x[0][i]))\n",
    "        \n",
    "        road_imgs.append(torch.as_tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(256)\n",
    "        counts = np.zeros(90)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "#             if x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "            # Get its four bird's-eye view coordinates. \n",
    "            point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "            xs = point_squence.T[0] * 10 + 400\n",
    "            ys = -point_squence.T[1] * 10 + 400\n",
    "\n",
    "            # Grab the current bounding box. \n",
    "            current_bbs.append((xs, ys))\n",
    "\n",
    "            # Find the bin/grid cell it falls in, get its class mapping. \n",
    "            center_x, center_y = torch.mean(xs).item(), torch.mean(ys).item()\n",
    "            key = (round_down(center_x), round_down(center_y))\n",
    "            if key not in class_dict:\n",
    "                print(key)\n",
    "            bin_id = class_dict[key]\n",
    "            bins[bin_id] = 1\n",
    "            count += 1\n",
    "            \n",
    "        \n",
    "        counts[count] = 1\n",
    "\n",
    "        # Label Smoothing #\n",
    "        if count > 10 and count < 88:\n",
    "            counts[count+1] = 0.2\n",
    "            counts[count-1] = 0.2\n",
    "        target_counts.append(torch.Tensor(counts))\n",
    "        \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        target.append(torch.Tensor(bins))\n",
    "        \n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(target), torch.stack(road_imgs), bbs, torch.stack(target_counts)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.argmax(counts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx], nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    \n",
    "    draw_box(ax, class_box, 'green')\n",
    "    \n",
    "def append_first_to_last(tens):\n",
    "    ret = torch.cat((tens, torch.as_tensor([tens[0]])))\n",
    "    return ret\n",
    "\n",
    "    \n",
    "for bb in bbs[idx]:\n",
    "    ax.plot(append_first_to_last(bb[0]), append_first_to_last(bb[1]), color='orange')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 256 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 256)),\n",
    "            ('drop', nn.Dropout(p = 0.6)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel()\n",
    "\n",
    "# Weighting certain classes more. \n",
    "# positive_weight = torch.ones(256).to(device)\n",
    "# for (x, y), bin_id in class_dict.items():\n",
    "#     if abs(x - 400) <= 200 and abs(y - 400) <= 200:\n",
    "#         positive_weight[bin_id] = 2\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if(\"bn\" not in name):\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "# for layer in unfreeze_layers:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "        \n",
    "model = model.to(device)\n",
    "bin_criterion = nn.BCEWithLogitsLoss()\n",
    "count_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(device)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a.float()) + (1 - lam) * criterion(pred, y_b.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=10, num_workers=3, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    train_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        \n",
    "#         sample, target_a, target_b, lam = mixup_data(sample, target)\n",
    "#         sample, target_a, target_b = map(torch.autograd.Variable, (sample, target_a, target_b))\n",
    "        \n",
    "        batch_yhat = []\n",
    "        batch_ycount = []\n",
    "        for j, x in enumerate(sample):\n",
    "            y_hat, y_count = model(x)\n",
    "            batch_yhat.append(y_hat)\n",
    "            batch_ycount.append(y_count)\n",
    "        \n",
    "        y_hat = torch.stack(batch_yhat).squeeze()\n",
    "        y_count = torch.stack(batch_ycount).squeeze()\n",
    "        \n",
    "        # Mixup criterion here\n",
    "#         bin_loss = mixup_criterion(bin_criterion, y_hat, target_a, target_b, lam)\n",
    "        \n",
    "        bin_loss = bin_criterion(y_hat, target.float())\n",
    "        count_loss = count_criterion(y_count, target_count.float())\n",
    "        loss = bin_loss + 2 * count_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        bin_losses.append(bin_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                50. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    print(\"Average Train Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Train Count Epoch Loss: \", np.mean(count_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    count_correct = 0\n",
    "    count_off_by_1 = 0\n",
    "    total_count = 0\n",
    "    bin_correct = 0\n",
    "    total_bins = 0\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_yhat = []\n",
    "            batch_ycount = []\n",
    "            for j, x in enumerate(sample):\n",
    "                y_hat, y_count = model(x)\n",
    "                batch_yhat.append(y_hat)\n",
    "                batch_ycount.append(y_count)\n",
    "\n",
    "            y_hat = torch.stack(batch_yhat).squeeze()\n",
    "            y_count = torch.stack(batch_ycount).squeeze()\n",
    "            \n",
    "            for j, x in enumerate(y_count):\n",
    "                pred_count = torch.argmax(y_count[j])\n",
    "                t_count = torch.argmax(target_count[j])\n",
    "                if pred_count == t_count:\n",
    "                    count_correct += 1\n",
    "                elif abs(pred_count - t_count) == 1:\n",
    "                    count_off_by_1 += 1\n",
    "                      \n",
    "                pred_bins = torch.topk(y_hat[j], k = pred_count).indices\n",
    "                t_bins = (target[j] == 1).nonzero()\n",
    "                for b in pred_bins:\n",
    "                    if b in t_bins:\n",
    "                        bin_correct += 1\n",
    "                    \n",
    "                total_bins += len(t_bins)\n",
    "            total_count += y_count.size(0)\n",
    "            \n",
    "            bin_loss = bin_criterion(y_hat, target.float())\n",
    "            count_loss = count_criterion(y_count, target_count.float())\n",
    "            loss = bin_loss + 2 * count_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            bin_losses.append(bin_loss.item())\n",
    "            count_losses.append(count_loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Validation Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"\\tAverage Validation Count Accuracy: \", 100*count_correct/total_count)\n",
    "    print(\"\\tAverage Validation Count-off-by-1 Accuracy: \", 100*count_off_by_1/total_count)\n",
    "    if total_bins != 0:\n",
    "        print(\"\\tAverage Validation Bin Accuracy: \", 100*bin_correct/total_bins)\n",
    "    print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), 'all_six_images_classify_count.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2772 (0%)]\tLoss: 2.155430\n",
      "Train Epoch: 0 [500/2772 (9%)]\tLoss: 0.387341\n",
      "Train Epoch: 0 [1000/2772 (18%)]\tLoss: 0.419313\n",
      "Train Epoch: 0 [1500/2772 (27%)]\tLoss: 0.382842\n",
      "Train Epoch: 0 [2000/2772 (36%)]\tLoss: 0.356920\n",
      "Train Epoch: 0 [2500/2772 (45%)]\tLoss: 0.331879\n",
      "\n",
      "Average Train Epoch Loss:  0.392071440708723\n",
      "Average Train Bin Epoch Loss:  0.2288028459004361\n",
      "Average Train Count Epoch Loss:  0.08163429843596846\n",
      "Average Validation Epoch Loss:  0.4310843637213111\n",
      "Average Validation Bin Epoch Loss:  0.28558460297062993\n",
      "Average Validation Count Epoch Loss:  0.07274987874552608\n",
      "\tAverage Validation Count Accuracy:  3.1746031746031744\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  11.507936507936508\n",
      "\tAverage Validation Bin Accuracy:  19.317478560693132\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/2772 (0%)]\tLoss: 0.301096\n",
      "Train Epoch: 1 [500/2772 (9%)]\tLoss: 0.343567\n",
      "Train Epoch: 1 [1000/2772 (18%)]\tLoss: 0.266616\n",
      "Train Epoch: 1 [1500/2772 (27%)]\tLoss: 0.355097\n",
      "Train Epoch: 1 [2000/2772 (36%)]\tLoss: 0.309070\n",
      "Train Epoch: 1 [2500/2772 (45%)]\tLoss: 0.290318\n",
      "\n",
      "Average Train Epoch Loss:  0.3266168363767562\n",
      "Average Train Bin Epoch Loss:  0.20046493689790904\n",
      "Average Train Count Epoch Loss:  0.06307594921681092\n",
      "Average Validation Epoch Loss:  0.43219662085175514\n",
      "Average Validation Bin Epoch Loss:  0.2856540367938578\n",
      "Average Validation Count Epoch Loss:  0.07327129133045673\n",
      "\tAverage Validation Count Accuracy:  3.5714285714285716\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  6.746031746031746\n",
      "\tAverage Validation Bin Accuracy:  20.04243656617452\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/2772 (0%)]\tLoss: 0.294282\n",
      "Train Epoch: 2 [500/2772 (9%)]\tLoss: 0.297715\n",
      "Train Epoch: 2 [1000/2772 (18%)]\tLoss: 0.379705\n",
      "Train Epoch: 2 [1500/2772 (27%)]\tLoss: 0.295070\n",
      "Train Epoch: 2 [2000/2772 (36%)]\tLoss: 0.289933\n",
      "Train Epoch: 2 [2500/2772 (45%)]\tLoss: 0.328130\n",
      "\n",
      "Average Train Epoch Loss:  0.31502235938104794\n",
      "Average Train Bin Epoch Loss:  0.19316177900960976\n",
      "Average Train Count Epoch Loss:  0.06093029005171584\n",
      "Average Validation Epoch Loss:  0.4356950195506215\n",
      "Average Validation Bin Epoch Loss:  0.2911626724526286\n",
      "Average Validation Count Epoch Loss:  0.07226617448031902\n",
      "\tAverage Validation Count Accuracy:  2.7777777777777777\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  7.738095238095238\n",
      "\tAverage Validation Bin Accuracy:  20.130846079038104\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/2772 (0%)]\tLoss: 0.247592\n",
      "Train Epoch: 3 [500/2772 (9%)]\tLoss: 0.277997\n",
      "Train Epoch: 3 [1000/2772 (18%)]\tLoss: 0.348437\n",
      "Train Epoch: 3 [1500/2772 (27%)]\tLoss: 0.248749\n",
      "Train Epoch: 3 [2000/2772 (36%)]\tLoss: 0.272244\n",
      "Train Epoch: 3 [2500/2772 (45%)]\tLoss: 0.374597\n",
      "\n",
      "Average Train Epoch Loss:  0.2999988798078873\n",
      "Average Train Bin Epoch Loss:  0.1856004683930668\n",
      "Average Train Count Epoch Loss:  0.05719920541260311\n",
      "Average Validation Epoch Loss:  0.41474657133221626\n",
      "Average Validation Bin Epoch Loss:  0.2776807607151568\n",
      "Average Validation Count Epoch Loss:  0.06853290484286845\n",
      "\tAverage Validation Count Accuracy:  3.9682539682539684\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  8.134920634920634\n",
      "\tAverage Validation Bin Accuracy:  21.76642206701441\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/2772 (0%)]\tLoss: 0.270549\n",
      "Train Epoch: 4 [500/2772 (9%)]\tLoss: 0.237749\n",
      "Train Epoch: 4 [1000/2772 (18%)]\tLoss: 0.303419\n",
      "Train Epoch: 4 [1500/2772 (27%)]\tLoss: 0.265841\n",
      "Train Epoch: 4 [2000/2772 (36%)]\tLoss: 0.237039\n",
      "Train Epoch: 4 [2500/2772 (45%)]\tLoss: 0.299714\n",
      "\n",
      "Average Train Epoch Loss:  0.28830568680017116\n",
      "Average Train Bin Epoch Loss:  0.1799111200643958\n",
      "Average Train Count Epoch Loss:  0.05419728371629612\n",
      "Average Validation Epoch Loss:  0.4211104791611433\n",
      "Average Validation Bin Epoch Loss:  0.2808290431275964\n",
      "Average Validation Count Epoch Loss:  0.07014071708545089\n",
      "\tAverage Validation Count Accuracy:  5.9523809523809526\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.119047619047619\n",
      "\tAverage Validation Bin Accuracy:  18.76049862965255\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/2772 (0%)]\tLoss: 0.245993\n",
      "Train Epoch: 5 [500/2772 (9%)]\tLoss: 0.291007\n",
      "Train Epoch: 5 [1000/2772 (18%)]\tLoss: 0.200348\n",
      "Train Epoch: 5 [1500/2772 (27%)]\tLoss: 0.257809\n",
      "Train Epoch: 5 [2000/2772 (36%)]\tLoss: 0.240331\n",
      "Train Epoch: 5 [2500/2772 (45%)]\tLoss: 0.272067\n",
      "\n",
      "Average Train Epoch Loss:  0.27968227316578514\n",
      "Average Train Bin Epoch Loss:  0.17600912142464584\n",
      "Average Train Count Epoch Loss:  0.05183657600457291\n",
      "Average Validation Epoch Loss:  0.4189332500100136\n",
      "Average Validation Bin Epoch Loss:  0.2797456313855946\n",
      "Average Validation Count Epoch Loss:  0.06959380942862481\n",
      "\tAverage Validation Count Accuracy:  5.555555555555555\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.317460317460318\n",
      "\tAverage Validation Bin Accuracy:  20.396074617628855\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/2772 (0%)]\tLoss: 0.289720\n",
      "Train Epoch: 6 [500/2772 (9%)]\tLoss: 0.281707\n",
      "Train Epoch: 6 [1000/2772 (18%)]\tLoss: 0.296519\n",
      "Train Epoch: 6 [1500/2772 (27%)]\tLoss: 0.225450\n",
      "Train Epoch: 6 [2000/2772 (36%)]\tLoss: 0.256586\n",
      "Train Epoch: 6 [2500/2772 (45%)]\tLoss: 0.260386\n",
      "\n",
      "Average Train Epoch Loss:  0.2703572655860469\n",
      "Average Train Bin Epoch Loss:  0.1714751031383765\n",
      "Average Train Count Epoch Loss:  0.04944108104963097\n",
      "Average Validation Epoch Loss:  0.4451286345720291\n",
      "Average Validation Bin Epoch Loss:  0.29203642765060067\n",
      "Average Validation Count Epoch Loss:  0.07654610322788358\n",
      "\tAverage Validation Count Accuracy:  4.563492063492063\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  11.507936507936508\n",
      "\tAverage Validation Bin Accuracy:  20.846963133233135\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/2772 (0%)]\tLoss: 0.240968\n",
      "Train Epoch: 7 [500/2772 (9%)]\tLoss: 0.236616\n",
      "Train Epoch: 7 [1000/2772 (18%)]\tLoss: 0.237879\n",
      "Train Epoch: 7 [1500/2772 (27%)]\tLoss: 0.221579\n",
      "Train Epoch: 7 [2000/2772 (36%)]\tLoss: 0.251510\n",
      "Train Epoch: 7 [2500/2772 (45%)]\tLoss: 0.282942\n",
      "\n",
      "Average Train Epoch Loss:  0.2627763600765372\n",
      "Average Train Bin Epoch Loss:  0.1674776082898644\n",
      "Average Train Count Epoch Loss:  0.04764937533052276\n",
      "Average Validation Epoch Loss:  0.4458384867757559\n",
      "Average Validation Bin Epoch Loss:  0.28994963597506285\n",
      "Average Validation Count Epoch Loss:  0.07794442726299167\n",
      "\tAverage Validation Count Accuracy:  3.1746031746031744\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  11.507936507936508\n",
      "\tAverage Validation Bin Accuracy:  21.04146406153302\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/2772 (0%)]\tLoss: 0.216825\n",
      "Train Epoch: 8 [500/2772 (9%)]\tLoss: 0.223100\n",
      "Train Epoch: 8 [1000/2772 (18%)]\tLoss: 0.235002\n",
      "Train Epoch: 8 [1500/2772 (27%)]\tLoss: 0.313075\n",
      "Train Epoch: 8 [2000/2772 (36%)]\tLoss: 0.218233\n",
      "Average Validation Epoch Loss:  0.4323818925768137\n",
      "Average Validation Bin Epoch Loss:  0.2806501528248191\n",
      "Average Validation Count Epoch Loss:  0.07586586941033602\n",
      "\tAverage Validation Count Accuracy:  2.9761904761904763\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  9.126984126984127\n",
      "\tAverage Validation Bin Accuracy:  23.05720095482274\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/2772 (0%)]\tLoss: 0.248854\n",
      "Train Epoch: 9 [500/2772 (9%)]\tLoss: 0.233459\n",
      "Train Epoch: 9 [1000/2772 (18%)]\tLoss: 0.216052\n",
      "Train Epoch: 9 [1500/2772 (27%)]\tLoss: 0.246601\n",
      "Train Epoch: 9 [2000/2772 (36%)]\tLoss: 0.329976\n",
      "Train Epoch: 9 [2500/2772 (45%)]\tLoss: 0.266788\n",
      "\n",
      "Average Train Epoch Loss:  0.24815680435021148\n",
      "Average Train Bin Epoch Loss:  0.16082508706574816\n",
      "Average Train Count Epoch Loss:  0.04366585858863035\n",
      "Average Validation Epoch Loss:  0.43994257040321827\n",
      "Average Validation Bin Epoch Loss:  0.2818107670173049\n",
      "Average Validation Count Epoch Loss:  0.07906590052880347\n",
      "\tAverage Validation Count Accuracy:  5.753968253968254\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  8.134920634920634\n",
      "\tAverage Validation Bin Accuracy:  22.72124480594112\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/2772 (0%)]\tLoss: 0.292439\n",
      "Train Epoch: 10 [500/2772 (9%)]\tLoss: 0.190642\n",
      "Train Epoch: 10 [1000/2772 (18%)]\tLoss: 0.273890\n",
      "Train Epoch: 10 [1500/2772 (27%)]\tLoss: 0.184087\n",
      "Train Epoch: 10 [2000/2772 (36%)]\tLoss: 0.191101\n",
      "Train Epoch: 10 [2500/2772 (45%)]\tLoss: 0.275585\n",
      "\n",
      "Average Train Epoch Loss:  0.24237236332335918\n",
      "Average Train Bin Epoch Loss:  0.1584082430483197\n",
      "Average Train Count Epoch Loss:  0.04198206026482282\n",
      "Average Validation Epoch Loss:  0.4341701567173004\n",
      "Average Validation Bin Epoch Loss:  0.27593695744872093\n",
      "Average Validation Count Epoch Loss:  0.07911660009995103\n",
      "\tAverage Validation Count Accuracy:  3.9682539682539684\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.714285714285714\n",
      "\tAverage Validation Bin Accuracy:  25.709486340730262\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [0/2772 (0%)]\tLoss: 0.234702\n",
      "Train Epoch: 11 [500/2772 (9%)]\tLoss: 0.225547\n",
      "Train Epoch: 11 [1000/2772 (18%)]\tLoss: 0.220721\n",
      "Train Epoch: 11 [1500/2772 (27%)]\tLoss: 0.227281\n",
      "Train Epoch: 11 [2000/2772 (36%)]\tLoss: 0.234583\n",
      "Train Epoch: 11 [2500/2772 (45%)]\tLoss: 0.243360\n",
      "\n",
      "Average Train Epoch Loss:  0.2344648753889173\n",
      "Average Train Bin Epoch Loss:  0.15464804814659433\n",
      "Average Train Count Epoch Loss:  0.039908413668062616\n",
      "Average Validation Epoch Loss:  0.44595608394593\n",
      "Average Validation Bin Epoch Loss:  0.28726937621831894\n",
      "Average Validation Count Epoch Loss:  0.07934335432946682\n",
      "\tAverage Validation Count Accuracy:  2.7777777777777777\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.119047619047619\n",
      "\tAverage Validation Bin Accuracy:  20.65246220493325\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/2772 (0%)]\tLoss: 0.224454\n",
      "Train Epoch: 12 [500/2772 (9%)]\tLoss: 0.274266\n",
      "Train Epoch: 12 [1000/2772 (18%)]\tLoss: 0.287609\n",
      "Train Epoch: 12 [1500/2772 (27%)]\tLoss: 0.268775\n",
      "Train Epoch: 12 [2000/2772 (36%)]\tLoss: 0.224410\n",
      "Train Epoch: 12 [2500/2772 (45%)]\tLoss: 0.229085\n",
      "\n",
      "Average Train Epoch Loss:  0.2287423817052258\n",
      "Average Train Bin Epoch Loss:  0.15223024010979871\n",
      "Average Train Count Epoch Loss:  0.03825607075751256\n",
      "Average Validation Epoch Loss:  0.42213682923465967\n",
      "Average Validation Bin Epoch Loss:  0.2673030565492809\n",
      "Average Validation Count Epoch Loss:  0.07741688541136682\n",
      "\tAverage Validation Count Accuracy:  1.9841269841269842\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  9.523809523809524\n",
      "\tAverage Validation Bin Accuracy:  23.808681814163204\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/2772 (0%)]\tLoss: 0.215105\n",
      "Train Epoch: 13 [500/2772 (9%)]\tLoss: 0.232804\n",
      "Train Epoch: 13 [1000/2772 (18%)]\tLoss: 0.250337\n",
      "Train Epoch: 13 [1500/2772 (27%)]\tLoss: 0.206656\n",
      "Train Epoch: 13 [2000/2772 (36%)]\tLoss: 0.303630\n",
      "Train Epoch: 13 [2500/2772 (45%)]\tLoss: 0.250097\n",
      "\n",
      "Average Train Epoch Loss:  0.2229692695809783\n",
      "Average Train Bin Epoch Loss:  0.14917052625644978\n",
      "Average Train Count Epoch Loss:  0.03689937180966782\n",
      "Average Validation Epoch Loss:  0.4467387832701206\n",
      "Average Validation Bin Epoch Loss:  0.28307554917410016\n",
      "Average Validation Count Epoch Loss:  0.08183161704801023\n",
      "\tAverage Validation Count Accuracy:  1.9841269841269842\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  8.134920634920634\n",
      "\tAverage Validation Bin Accuracy:  20.033595614888164\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/2772 (0%)]\tLoss: 0.201664\n",
      "Train Epoch: 14 [500/2772 (9%)]\tLoss: 0.212992\n",
      "Train Epoch: 14 [1000/2772 (18%)]\tLoss: 0.243144\n",
      "Train Epoch: 14 [1500/2772 (27%)]\tLoss: 0.201779\n",
      "Train Epoch: 14 [2000/2772 (36%)]\tLoss: 0.235609\n",
      "Train Epoch: 14 [2500/2772 (45%)]\tLoss: 0.261887\n",
      "\n",
      "Average Train Epoch Loss:  0.21802681025198037\n",
      "Average Train Bin Epoch Loss:  0.14734031581621376\n",
      "Average Train Count Epoch Loss:  0.03534324731838574\n",
      "Average Validation Epoch Loss:  0.47382986545562744\n",
      "Average Validation Bin Epoch Loss:  0.29215682158246636\n",
      "Average Validation Count Epoch Loss:  0.09083652147091925\n",
      "\tAverage Validation Count Accuracy:  3.373015873015873\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  11.30952380952381\n",
      "\tAverage Validation Bin Accuracy:  24.44523030678101\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/2772 (0%)]\tLoss: 0.226179\n",
      "Train Epoch: 15 [500/2772 (9%)]\tLoss: 0.265591\n",
      "Train Epoch: 15 [1000/2772 (18%)]\tLoss: 0.208441\n",
      "Train Epoch: 15 [1500/2772 (27%)]\tLoss: 0.264440\n",
      "Train Epoch: 15 [2000/2772 (36%)]\tLoss: 0.191259\n",
      "Train Epoch: 15 [2500/2772 (45%)]\tLoss: 0.219973\n",
      "\n",
      "Average Train Epoch Loss:  0.21300173416840945\n",
      "Average Train Bin Epoch Loss:  0.14454840804389912\n",
      "Average Train Count Epoch Loss:  0.03422666263344477\n",
      "Average Validation Epoch Loss:  0.4412627760320902\n",
      "Average Validation Bin Epoch Loss:  0.2764541422948241\n",
      "Average Validation Count Epoch Loss:  0.08240431733429432\n",
      "\tAverage Validation Count Accuracy:  1.9841269841269842\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.515873015873016\n",
      "\tAverage Validation Bin Accuracy:  22.853859075236496\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/2772 (0%)]\tLoss: 0.250662\n",
      "Train Epoch: 16 [500/2772 (9%)]\tLoss: 0.178751\n",
      "Train Epoch: 16 [1000/2772 (18%)]\tLoss: 0.203037\n",
      "Train Epoch: 16 [1500/2772 (27%)]\tLoss: 0.201598\n",
      "Train Epoch: 16 [2000/2772 (36%)]\tLoss: 0.184984\n",
      "Train Epoch: 16 [2500/2772 (45%)]\tLoss: 0.159623\n",
      "\n",
      "Average Train Epoch Loss:  0.20697959415989814\n",
      "Average Train Bin Epoch Loss:  0.1428703004025298\n",
      "Average Train Count Epoch Loss:  0.032054646871984005\n",
      "Average Validation Epoch Loss:  0.4369937814772129\n",
      "Average Validation Bin Epoch Loss:  0.27082763239741325\n",
      "Average Validation Count Epoch Loss:  0.08308307523839176\n",
      "\tAverage Validation Count Accuracy:  4.761904761904762\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.317460317460318\n",
      "\tAverage Validation Bin Accuracy:  24.28609318362656\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/2772 (0%)]\tLoss: 0.221803\n",
      "Train Epoch: 17 [500/2772 (9%)]\tLoss: 0.212279\n",
      "Train Epoch: 17 [1000/2772 (18%)]\tLoss: 0.254023\n",
      "Train Epoch: 17 [1500/2772 (27%)]\tLoss: 0.246101\n",
      "Train Epoch: 17 [2000/2772 (36%)]\tLoss: 0.218638\n",
      "Train Epoch: 17 [2500/2772 (45%)]\tLoss: 0.181265\n",
      "\n",
      "Average Train Epoch Loss:  0.20434438394342394\n",
      "Average Train Bin Epoch Loss:  0.14127034970003066\n",
      "Average Train Count Epoch Loss:  0.03153701698769339\n",
      "Average Validation Epoch Loss:  0.4653823571279645\n",
      "Average Validation Bin Epoch Loss:  0.28736150497570634\n",
      "Average Validation Count Epoch Loss:  0.08901042747311294\n",
      "\tAverage Validation Count Accuracy:  5.357142857142857\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  11.904761904761905\n",
      "\tAverage Validation Bin Accuracy:  23.923614180885863\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/2772 (0%)]\tLoss: 0.186120\n",
      "Train Epoch: 18 [500/2772 (9%)]\tLoss: 0.147681\n",
      "Train Epoch: 18 [1000/2772 (18%)]\tLoss: 0.225230\n",
      "Train Epoch: 18 [1500/2772 (27%)]\tLoss: 0.200743\n",
      "Train Epoch: 18 [2000/2772 (36%)]\tLoss: 0.233547\n",
      "Train Epoch: 18 [2500/2772 (45%)]\tLoss: 0.168543\n",
      "\n",
      "Average Train Epoch Loss:  0.19959801679356493\n",
      "Average Train Bin Epoch Loss:  0.1395501732665429\n",
      "Average Train Count Epoch Loss:  0.03002392181376223\n",
      "Average Validation Epoch Loss:  0.4528134008869529\n",
      "Average Validation Bin Epoch Loss:  0.27540932362899184\n",
      "Average Validation Count Epoch Loss:  0.0887020390946418\n",
      "\tAverage Validation Count Accuracy:  2.380952380952381\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  10.119047619047619\n",
      "\tAverage Validation Bin Accuracy:  24.72814074794448\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/2772 (0%)]\tLoss: 0.199141\n",
      "Train Epoch: 19 [500/2772 (9%)]\tLoss: 0.178944\n",
      "Train Epoch: 19 [1000/2772 (18%)]\tLoss: 0.203550\n",
      "Train Epoch: 19 [1500/2772 (27%)]\tLoss: 0.243663\n",
      "Train Epoch: 19 [2000/2772 (36%)]\tLoss: 0.183099\n",
      "Train Epoch: 19 [2500/2772 (45%)]\tLoss: 0.158355\n",
      "\n",
      "Average Train Epoch Loss:  0.19552967823345027\n",
      "Average Train Bin Epoch Loss:  0.13762934886508707\n",
      "Average Train Count Epoch Loss:  0.02895016481818484\n",
      "Average Validation Epoch Loss:  0.4954912317916751\n",
      "Average Validation Bin Epoch Loss:  0.2889205701649189\n",
      "Average Validation Count Epoch Loss:  0.10328533151187003\n",
      "\tAverage Validation Count Accuracy:  3.5714285714285716\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  9.722222222222221\n",
      "\tAverage Validation Bin Accuracy:  23.030678100963662\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/2772 (0%)]\tLoss: 0.242178\n",
      "Train Epoch: 20 [500/2772 (9%)]\tLoss: 0.179428\n",
      "Train Epoch: 20 [1000/2772 (18%)]\tLoss: 0.208610\n",
      "Train Epoch: 20 [1500/2772 (27%)]\tLoss: 0.186361\n",
      "Train Epoch: 20 [2000/2772 (36%)]\tLoss: 0.122299\n",
      "Train Epoch: 20 [2500/2772 (45%)]\tLoss: 0.201892\n",
      "\n",
      "Average Train Epoch Loss:  0.1910714486466466\n",
      "Average Train Bin Epoch Loss:  0.1354905087634814\n",
      "Average Train Count Epoch Loss:  0.027790469983458625\n",
      "Average Validation Epoch Loss:  0.43676948081701994\n",
      "Average Validation Bin Epoch Loss:  0.2663736566901207\n",
      "Average Validation Count Epoch Loss:  0.08519791252911091\n",
      "\tAverage Validation Count Accuracy:  5.555555555555555\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  8.928571428571429\n",
      "\tAverage Validation Bin Accuracy:  25.93935107417558\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/2772 (0%)]\tLoss: 0.262199\n",
      "Train Epoch: 21 [500/2772 (9%)]\tLoss: 0.164381\n",
      "Train Epoch: 21 [1000/2772 (18%)]\tLoss: 0.144196\n",
      "Train Epoch: 21 [1500/2772 (27%)]\tLoss: 0.132438\n",
      "Train Epoch: 21 [2000/2772 (36%)]\tLoss: 0.181280\n",
      "Train Epoch: 21 [2500/2772 (45%)]\tLoss: 0.214094\n",
      "\n",
      "Average Train Epoch Loss:  0.18757345460408884\n",
      "Average Train Bin Epoch Loss:  0.13396218526277612\n",
      "Average Train Count Epoch Loss:  0.026805634787909205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Epoch Loss:  0.45103101991117\n",
      "Average Validation Bin Epoch Loss:  0.2750449092127383\n",
      "Average Validation Count Epoch Loss:  0.08799305488355458\n",
      "\tAverage Validation Count Accuracy:  4.9603174603174605\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  9.325396825396826\n",
      "\tAverage Validation Bin Accuracy:  23.844045619308638\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/2772 (0%)]\tLoss: 0.153745\n",
      "Train Epoch: 22 [500/2772 (9%)]\tLoss: 0.188334\n",
      "Train Epoch: 22 [1000/2772 (18%)]\tLoss: 0.157897\n",
      "Train Epoch: 22 [1500/2772 (27%)]\tLoss: 0.223644\n",
      "Train Epoch: 22 [2000/2772 (36%)]\tLoss: 0.197209\n",
      "Train Epoch: 22 [2500/2772 (45%)]\tLoss: 0.179921\n",
      "\n",
      "Average Train Epoch Loss:  0.1831860724541781\n",
      "Average Train Bin Epoch Loss:  0.13230002540156996\n",
      "Average Train Count Epoch Loss:  0.025443023626806494\n",
      "Average Validation Epoch Loss:  0.47837675362825394\n",
      "Average Validation Bin Epoch Loss:  0.28083591675385833\n",
      "Average Validation Count Epoch Loss:  0.09877041634172201\n",
      "\tAverage Validation Count Accuracy:  3.1746031746031744\n",
      "\tAverage Validation Count-off-by-1 Accuracy:  6.746031746031746\n",
      "\tAverage Validation Bin Accuracy:  27.627972769870038\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/2772 (0%)]\tLoss: 0.202639\n",
      "Train Epoch: 23 [500/2772 (9%)]\tLoss: 0.247039\n",
      "Train Epoch: 23 [1000/2772 (18%)]\tLoss: 0.180987\n",
      "Train Epoch: 23 [1500/2772 (27%)]\tLoss: 0.245141\n",
      "Train Epoch: 23 [2000/2772 (36%)]\tLoss: 0.199568\n",
      "Train Epoch: 23 [2500/2772 (45%)]\tLoss: 0.165617\n",
      "\n",
      "Average Train Epoch Loss:  0.18065857742544558\n",
      "Average Train Bin Epoch Loss:  0.13098916057631266\n",
      "Average Train Count Epoch Loss:  0.0248347085418193\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.253 val bin loss\n",
    "\n",
    "# Random Affine 3 degrees\n",
    "# 0.263 val bin loss\n",
    "\n",
    "\n",
    "# Need to do 5 * bin_loss + count_loss or something like that. Also more extreme Random Affine maybe?\n",
    "\n",
    "# Random Affine 5 degrees\n",
    "# 0.266 val bin loss\n",
    "\n",
    "# Took out Random Affine. \n",
    "# 0.268 val bin loss\n",
    "\n",
    "# Increased compress dim from 128 to 200. \n",
    "# 0.259 val bin loss\n",
    "\n",
    "# 5 * bin_loss + count_loss\n",
    "# 0.249 + 0.055\n",
    "\n",
    "# 8 *\n",
    "# 0.251 + 0.054\n",
    "\n",
    "# 8*, RandomAffine(3)\n",
    "# 0.255\n",
    "\n",
    "# 8*, RandomAffine(3), weight_decay 0.1\n",
    "\n",
    "# 10 *, RandomAffine(3)\n",
    "# 0.259\n",
    "\n",
    "# 8 *, Normalize (mean, std)\n",
    "# 0.26\n",
    "\n",
    "# 8 *, Dropout\n",
    "# 0.241, 0.253\n",
    "\n",
    "# 5 *, Dropout\n",
    "# 0.254\n",
    "\n",
    "# 11 *, Dropout\n",
    "# 0.249\n",
    "\n",
    "# Want to try positive-weights for classes within 200 to 600. \n",
    "# Want to get the model to get those classes correct. \n",
    "\n",
    "# Mixup 0.2, 1 *, Dropout\n",
    "# (0.244, 0.053), \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 200 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 200)),\n",
    "            ('drop', nn.Dropout(p = 0.5)),\n",
    "            ('relu', nn.ReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "model.load_state_dict(torch.load('all_six_images_classify_count.pt'))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(samples):\n",
    "    \n",
    "    # samples is (batch_size, 6, 3, 256, 306)\n",
    "    \n",
    "    # You need to return a tuple with size batch_size and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of bounding boxes. \n",
    "    \n",
    "    # Okay so I have my model. \n",
    "    # \n",
    "    \n",
    "    bb_samples = []\n",
    "    \n",
    "    for x in samples:\n",
    "        preds_class, preds_count = model(x)\n",
    "        \n",
    "        # preds class is a 256-dimensional tensor, filled with probabilities\n",
    "        # I need to find the `preds_count` top indices with the top values.\n",
    "        \n",
    "        \n",
    "        result = torch.topk(preds_class, k = torch.argmax(preds_count).item())\n",
    "        pred_ids = result.indices\n",
    "        \n",
    "        bounding_boxes = []\n",
    "        for idx in pred_ids[0]:\n",
    "            buck_x, buck_y = reverse_class_dict[idx.item()]\n",
    "            \n",
    "            xs = torch.as_tensor([buck_x, buck_x, buck_x + 50, buck_x + 50])\n",
    "            ys = torch.as_tensor([buck_y+16, buck_y+36, buck_y+16, buck_y+36])\n",
    "            \n",
    "            xs = xs - 400\n",
    "            ys = 800 - ys # right-side up\n",
    "            ys = ys - 400\n",
    "            \n",
    "            xs /= 10\n",
    "            ys /= 10\n",
    "               \n",
    "            coords = torch.stack((xs, ys))\n",
    "            bounding_boxes.append(coords)\n",
    "            \n",
    "        bounding_boxes = torch.stack(bounding_boxes).cuda()\n",
    "        bb_samples.append(bounding_boxes)\n",
    "    \n",
    "    return tuple(bb_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(val_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = get_bounding_boxes(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_preds = torch.sigmoid(model(sample[idx])[0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx].cpu().detach(), nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "\n",
    "bin_ids = (sigmoid_preds > 0.25).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'red')\n",
    "    \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    draw_vish_box(ax, class_box, 'green')\n",
    "\n",
    "    \n",
    "for bb in boom[idx]:\n",
    "    box = bb.cpu().detach()\n",
    "    draw_box(ax, box, 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([box[:, 0], box[:, 1], box[:, 3], box[:, 2], box[:, 0]])\n",
    "\n",
    "def draw_box(ax, corners, color):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    \n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "    return point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vish_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
