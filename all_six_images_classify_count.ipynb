{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import draw_box\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '/scratch/vr1059/self-driving-data/data'\n",
    "annotation_csv = '/scratch/vr1059/self-driving-data/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "train_labeled_scene_index = np.arange(106, 128)\n",
    "val_labeled_scene_index = np.arange(128, 132)\n",
    "test_labeled_scene_index = np.arange(132, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x):\n",
    "    return int(math.ceil(x / 50.0)) * 50\n",
    "\n",
    "def round_down(x):\n",
    "    return round_up(x) - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "class_dict = dict()\n",
    "reverse_class_dict = []\n",
    "for i in range(0, 800, 50):\n",
    "    for j in range(0, 800, 50):\n",
    "        class_dict[(i, j)] = class_label\n",
    "        class_label += 1\n",
    "        reverse_class_dict.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 50): 1,\n",
       " (0, 100): 2,\n",
       " (0, 150): 3,\n",
       " (0, 200): 4,\n",
       " (0, 250): 5,\n",
       " (0, 300): 6,\n",
       " (0, 350): 7,\n",
       " (0, 400): 8,\n",
       " (0, 450): 9,\n",
       " (0, 500): 10,\n",
       " (0, 550): 11,\n",
       " (0, 600): 12,\n",
       " (0, 650): 13,\n",
       " (0, 700): 14,\n",
       " (0, 750): 15,\n",
       " (50, 0): 16,\n",
       " (50, 50): 17,\n",
       " (50, 100): 18,\n",
       " (50, 150): 19,\n",
       " (50, 200): 20,\n",
       " (50, 250): 21,\n",
       " (50, 300): 22,\n",
       " (50, 350): 23,\n",
       " (50, 400): 24,\n",
       " (50, 450): 25,\n",
       " (50, 500): 26,\n",
       " (50, 550): 27,\n",
       " (50, 600): 28,\n",
       " (50, 650): 29,\n",
       " (50, 700): 30,\n",
       " (50, 750): 31,\n",
       " (100, 0): 32,\n",
       " (100, 50): 33,\n",
       " (100, 100): 34,\n",
       " (100, 150): 35,\n",
       " (100, 200): 36,\n",
       " (100, 250): 37,\n",
       " (100, 300): 38,\n",
       " (100, 350): 39,\n",
       " (100, 400): 40,\n",
       " (100, 450): 41,\n",
       " (100, 500): 42,\n",
       " (100, 550): 43,\n",
       " (100, 600): 44,\n",
       " (100, 650): 45,\n",
       " (100, 700): 46,\n",
       " (100, 750): 47,\n",
       " (150, 0): 48,\n",
       " (150, 50): 49,\n",
       " (150, 100): 50,\n",
       " (150, 150): 51,\n",
       " (150, 200): 52,\n",
       " (150, 250): 53,\n",
       " (150, 300): 54,\n",
       " (150, 350): 55,\n",
       " (150, 400): 56,\n",
       " (150, 450): 57,\n",
       " (150, 500): 58,\n",
       " (150, 550): 59,\n",
       " (150, 600): 60,\n",
       " (150, 650): 61,\n",
       " (150, 700): 62,\n",
       " (150, 750): 63,\n",
       " (200, 0): 64,\n",
       " (200, 50): 65,\n",
       " (200, 100): 66,\n",
       " (200, 150): 67,\n",
       " (200, 200): 68,\n",
       " (200, 250): 69,\n",
       " (200, 300): 70,\n",
       " (200, 350): 71,\n",
       " (200, 400): 72,\n",
       " (200, 450): 73,\n",
       " (200, 500): 74,\n",
       " (200, 550): 75,\n",
       " (200, 600): 76,\n",
       " (200, 650): 77,\n",
       " (200, 700): 78,\n",
       " (200, 750): 79,\n",
       " (250, 0): 80,\n",
       " (250, 50): 81,\n",
       " (250, 100): 82,\n",
       " (250, 150): 83,\n",
       " (250, 200): 84,\n",
       " (250, 250): 85,\n",
       " (250, 300): 86,\n",
       " (250, 350): 87,\n",
       " (250, 400): 88,\n",
       " (250, 450): 89,\n",
       " (250, 500): 90,\n",
       " (250, 550): 91,\n",
       " (250, 600): 92,\n",
       " (250, 650): 93,\n",
       " (250, 700): 94,\n",
       " (250, 750): 95,\n",
       " (300, 0): 96,\n",
       " (300, 50): 97,\n",
       " (300, 100): 98,\n",
       " (300, 150): 99,\n",
       " (300, 200): 100,\n",
       " (300, 250): 101,\n",
       " (300, 300): 102,\n",
       " (300, 350): 103,\n",
       " (300, 400): 104,\n",
       " (300, 450): 105,\n",
       " (300, 500): 106,\n",
       " (300, 550): 107,\n",
       " (300, 600): 108,\n",
       " (300, 650): 109,\n",
       " (300, 700): 110,\n",
       " (300, 750): 111,\n",
       " (350, 0): 112,\n",
       " (350, 50): 113,\n",
       " (350, 100): 114,\n",
       " (350, 150): 115,\n",
       " (350, 200): 116,\n",
       " (350, 250): 117,\n",
       " (350, 300): 118,\n",
       " (350, 350): 119,\n",
       " (350, 400): 120,\n",
       " (350, 450): 121,\n",
       " (350, 500): 122,\n",
       " (350, 550): 123,\n",
       " (350, 600): 124,\n",
       " (350, 650): 125,\n",
       " (350, 700): 126,\n",
       " (350, 750): 127,\n",
       " (400, 0): 128,\n",
       " (400, 50): 129,\n",
       " (400, 100): 130,\n",
       " (400, 150): 131,\n",
       " (400, 200): 132,\n",
       " (400, 250): 133,\n",
       " (400, 300): 134,\n",
       " (400, 350): 135,\n",
       " (400, 400): 136,\n",
       " (400, 450): 137,\n",
       " (400, 500): 138,\n",
       " (400, 550): 139,\n",
       " (400, 600): 140,\n",
       " (400, 650): 141,\n",
       " (400, 700): 142,\n",
       " (400, 750): 143,\n",
       " (450, 0): 144,\n",
       " (450, 50): 145,\n",
       " (450, 100): 146,\n",
       " (450, 150): 147,\n",
       " (450, 200): 148,\n",
       " (450, 250): 149,\n",
       " (450, 300): 150,\n",
       " (450, 350): 151,\n",
       " (450, 400): 152,\n",
       " (450, 450): 153,\n",
       " (450, 500): 154,\n",
       " (450, 550): 155,\n",
       " (450, 600): 156,\n",
       " (450, 650): 157,\n",
       " (450, 700): 158,\n",
       " (450, 750): 159,\n",
       " (500, 0): 160,\n",
       " (500, 50): 161,\n",
       " (500, 100): 162,\n",
       " (500, 150): 163,\n",
       " (500, 200): 164,\n",
       " (500, 250): 165,\n",
       " (500, 300): 166,\n",
       " (500, 350): 167,\n",
       " (500, 400): 168,\n",
       " (500, 450): 169,\n",
       " (500, 500): 170,\n",
       " (500, 550): 171,\n",
       " (500, 600): 172,\n",
       " (500, 650): 173,\n",
       " (500, 700): 174,\n",
       " (500, 750): 175,\n",
       " (550, 0): 176,\n",
       " (550, 50): 177,\n",
       " (550, 100): 178,\n",
       " (550, 150): 179,\n",
       " (550, 200): 180,\n",
       " (550, 250): 181,\n",
       " (550, 300): 182,\n",
       " (550, 350): 183,\n",
       " (550, 400): 184,\n",
       " (550, 450): 185,\n",
       " (550, 500): 186,\n",
       " (550, 550): 187,\n",
       " (550, 600): 188,\n",
       " (550, 650): 189,\n",
       " (550, 700): 190,\n",
       " (550, 750): 191,\n",
       " (600, 0): 192,\n",
       " (600, 50): 193,\n",
       " (600, 100): 194,\n",
       " (600, 150): 195,\n",
       " (600, 200): 196,\n",
       " (600, 250): 197,\n",
       " (600, 300): 198,\n",
       " (600, 350): 199,\n",
       " (600, 400): 200,\n",
       " (600, 450): 201,\n",
       " (600, 500): 202,\n",
       " (600, 550): 203,\n",
       " (600, 600): 204,\n",
       " (600, 650): 205,\n",
       " (600, 700): 206,\n",
       " (600, 750): 207,\n",
       " (650, 0): 208,\n",
       " (650, 50): 209,\n",
       " (650, 100): 210,\n",
       " (650, 150): 211,\n",
       " (650, 200): 212,\n",
       " (650, 250): 213,\n",
       " (650, 300): 214,\n",
       " (650, 350): 215,\n",
       " (650, 400): 216,\n",
       " (650, 450): 217,\n",
       " (650, 500): 218,\n",
       " (650, 550): 219,\n",
       " (650, 600): 220,\n",
       " (650, 650): 221,\n",
       " (650, 700): 222,\n",
       " (650, 750): 223,\n",
       " (700, 0): 224,\n",
       " (700, 50): 225,\n",
       " (700, 100): 226,\n",
       " (700, 150): 227,\n",
       " (700, 200): 228,\n",
       " (700, 250): 229,\n",
       " (700, 300): 230,\n",
       " (700, 350): 231,\n",
       " (700, 400): 232,\n",
       " (700, 450): 233,\n",
       " (700, 500): 234,\n",
       " (700, 550): 235,\n",
       " (700, 600): 236,\n",
       " (700, 650): 237,\n",
       " (700, 700): 238,\n",
       " (700, 750): 239,\n",
       " (750, 0): 240,\n",
       " (750, 50): 241,\n",
       " (750, 100): 242,\n",
       " (750, 150): 243,\n",
       " (750, 200): 244,\n",
       " (750, 250): 245,\n",
       " (750, 300): 246,\n",
       " (750, 350): 247,\n",
       " (750, 400): 248,\n",
       " (750, 450): 249,\n",
       " (750, 500): 250,\n",
       " (750, 550): 251,\n",
       " (750, 600): 252,\n",
       " (750, 650): 253,\n",
       " (750, 700): 254,\n",
       " (750, 750): 255}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 350)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_class_dict[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    target = []\n",
    "    road_imgs = []\n",
    "    bbs = []\n",
    "    target_counts = []\n",
    "    for x in batch:\n",
    "        \n",
    "        # Collect six images for this sample. \n",
    "        six_images = []\n",
    "        for i in range(6):\n",
    "            six_images.append(torch.as_tensor(x[0][i]))\n",
    "        \n",
    "        road_imgs.append(torch.as_tensor(x[2]))\n",
    "        \n",
    "        # target\n",
    "        bb_tens = x[1]['bounding_box']\n",
    "        current_bbs = []\n",
    "        bins = np.zeros(256)\n",
    "        counts = np.zeros(90)\n",
    "        count = 0\n",
    "        \n",
    "        for i, corners in enumerate(bb_tens):\n",
    "            if x[1]['category'][i] not in [1, 3, 6, 8]:\n",
    "                # Get its four bird's-eye view coordinates. \n",
    "                point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2]])\n",
    "                xs = point_squence.T[0] * 10 + 400\n",
    "                ys = -point_squence.T[1] * 10 + 400\n",
    "\n",
    "                # Grab the current bounding box. \n",
    "                current_bbs.append((xs, ys))\n",
    "\n",
    "                # Find the bin/grid cell it falls in, get its class mapping. \n",
    "                center_x, center_y = torch.mean(xs).item(), torch.mean(ys).item()\n",
    "                key = (round_down(center_x), round_down(center_y))\n",
    "                if key not in class_dict:\n",
    "                    print(key)\n",
    "                bin_id = class_dict[key]\n",
    "                bins[bin_id] = 1\n",
    "                count += 1\n",
    "            \n",
    "        \n",
    "        counts[count] = 1\n",
    "        target_counts.append(torch.as_tensor(counts))\n",
    "        \n",
    "        images.append(torch.stack(six_images))\n",
    "                \n",
    "        target.append(torch.as_tensor(bins))\n",
    "        \n",
    "        bbs.append(current_bbs)\n",
    "                \n",
    "    boom = torch.stack(images), torch.stack(target), torch.stack(road_imgs), bbs, torch.stack(target_counts)\n",
    "    return boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "val_transform = transforms.ToTensor()\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=6, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(labeled_valset, batch_size=6, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_img, bbs, counts = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.argmax(counts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[idx], nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(road_img[idx], cmap ='binary');\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "# `target` is 32 by 81. Find the indices where there's a 1. \n",
    "bin_ids = (target[idx] == 1).nonzero()\n",
    "for bin_id in bin_ids:\n",
    "    class_box = reverse_class_dict[bin_id]\n",
    "    \n",
    "    draw_box(ax, class_box, 'green')\n",
    "    \n",
    "def append_first_to_last(tens):\n",
    "    ret = torch.cat((tens, torch.as_tensor([tens[0]])))\n",
    "    return ret\n",
    "\n",
    "    \n",
    "for bb in bbs[idx]:\n",
    "    ax.plot(append_first_to_last(bb[0]), append_first_to_last(bb[1]), color='orange')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, class_box, color):\n",
    "    box_xs = [class_box[0], class_box[0], class_box[0]+50, class_box[0]+50, class_box[0]]\n",
    "    box_ys = [class_box[1], class_box[1]+50, class_box[1]+50, class_box[1], class_box[1]]\n",
    "    ax.plot(box_xs, box_ys, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.concat_dim = 128 * 6\n",
    "        \n",
    "        self.compress = nn.Sequential(OrderedDict([\n",
    "            ('linear0', nn.Linear(2048, 128)),\n",
    "            ('relu', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "        self.classification = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.concat_dim, 256)),\n",
    "        ]))\n",
    "        \n",
    "        self.counts = nn.Sequential(OrderedDict([\n",
    "            ('count1', nn.Linear(self.concat_dim, 90))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.compress(x)\n",
    "        x = x.view(-1, self.concat_dim)\n",
    "        return self.classification(x), self.counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel()\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if(\"bn\" not in name):\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# unfreeze_layers = [model.encoder.layer3, model.encoder.layer4]\n",
    "# for layer in unfreeze_layers:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "        \n",
    "model = model.to(device)\n",
    "bin_criterion = nn.BCEWithLogitsLoss()\n",
    "count_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "best_val_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_trainset, batch_size=6, num_workers=3, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    train_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "        \n",
    "        batch_yhat = []\n",
    "        batch_ycount = []\n",
    "        for j, x in enumerate(sample):\n",
    "            y_hat, y_count = model(x)\n",
    "            batch_yhat.append(y_hat)\n",
    "            batch_ycount.append(y_count)\n",
    "        \n",
    "        y_hat = torch.stack(batch_yhat).squeeze()\n",
    "        y_count = torch.stack(batch_ycount).squeeze()\n",
    "        \n",
    "        bin_loss = bin_criterion(y_hat, target.float())\n",
    "        count_loss = count_criterion(y_count, target_count.float())\n",
    "        loss = bin_loss + count_loss\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        bin_losses.append(bin_loss.item())\n",
    "        count_losses.append(count_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(train_loader.dataset),\n",
    "                50. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    print(\"Average Train Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Train Count Epoch Loss: \", np.mean(count_losses))\n",
    "            \n",
    "def val():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    bin_losses = []\n",
    "    count_losses = []\n",
    "    for i, (sample, target, road_img, bbs, target_count) in enumerate(val_loader):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        target = target.to(device)\n",
    "        target_count = target_count.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_yhat = []\n",
    "            batch_ycount = []\n",
    "            for j, x in enumerate(sample):\n",
    "                y_hat, y_count = model(x)\n",
    "                batch_yhat.append(y_hat)\n",
    "                batch_ycount.append(y_count)\n",
    "\n",
    "            y_hat = torch.stack(batch_yhat).squeeze()\n",
    "            y_count = torch.stack(batch_ycount).squeeze()\n",
    "            \n",
    "            bin_loss = bin_criterion(y_hat, target.float())\n",
    "            count_loss = count_criterion(y_count, target_count.float())\n",
    "            loss = bin_loss + count_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            bin_losses.append(bin_loss.item())\n",
    "            count_losses.append(count_loss.item())\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "    print(\"Average Validation Bin Epoch Loss: \", np.mean(bin_losses))\n",
    "    print(\"Average Validation Count Epoch Loss: \", np.mean(count_losses))\n",
    "    print(\"\\n\")\n",
    "    global best_val_loss\n",
    "    if np.mean(val_losses) < best_val_loss:\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        torch.save(model.state_dict(), 'all_six_images_classify_count.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2772 (0%)]\tLoss: 1.407491\n",
      "Train Epoch: 0 [300/2772 (5%)]\tLoss: 0.208616\n",
      "Train Epoch: 0 [600/2772 (11%)]\tLoss: 0.193522\n",
      "Train Epoch: 0 [900/2772 (16%)]\tLoss: 0.264841\n",
      "Train Epoch: 0 [1200/2772 (22%)]\tLoss: 0.210100\n",
      "Train Epoch: 0 [1500/2772 (27%)]\tLoss: 0.248769\n",
      "Train Epoch: 0 [1800/2772 (32%)]\tLoss: 0.218228\n",
      "Train Epoch: 0 [2100/2772 (38%)]\tLoss: 0.240777\n",
      "Train Epoch: 0 [2400/2772 (43%)]\tLoss: 0.219011\n",
      "Train Epoch: 0 [2700/2772 (49%)]\tLoss: 0.314052\n",
      "\n",
      "Average Train Epoch Loss:  0.24988039173347093\n",
      "Average Train Bin Epoch Loss:  0.18915801492088286\n",
      "Average Train Count Epoch Loss:  0.060722377586674384\n",
      "Average Validation Epoch Loss:  0.325856310625871\n",
      "Average Validation Bin Epoch Loss:  0.2680763767233917\n",
      "Average Validation Count Epoch Loss:  0.05777993199548551\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/2772 (0%)]\tLoss: 0.208320\n",
      "Train Epoch: 1 [300/2772 (5%)]\tLoss: 0.297581\n",
      "Train Epoch: 1 [600/2772 (11%)]\tLoss: 0.210798\n",
      "Train Epoch: 1 [900/2772 (16%)]\tLoss: 0.188289\n",
      "Train Epoch: 1 [1200/2772 (22%)]\tLoss: 0.210070\n",
      "Train Epoch: 1 [1500/2772 (27%)]\tLoss: 0.247895\n",
      "Train Epoch: 1 [1800/2772 (32%)]\tLoss: 0.268270\n",
      "Train Epoch: 1 [2100/2772 (38%)]\tLoss: 0.214562\n",
      "Train Epoch: 1 [2400/2772 (43%)]\tLoss: 0.250228\n",
      "Train Epoch: 1 [2700/2772 (49%)]\tLoss: 0.230980\n",
      "\n",
      "Average Train Epoch Loss:  0.21596165436815906\n",
      "Average Train Bin Epoch Loss:  0.1687741745989044\n",
      "Average Train Count Epoch Loss:  0.047187479970839635\n",
      "Average Validation Epoch Loss:  0.3260715802510579\n",
      "Average Validation Bin Epoch Loss:  0.26751304275932763\n",
      "Average Validation Count Epoch Loss:  0.058558537669125055\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/2772 (0%)]\tLoss: 0.234955\n",
      "Train Epoch: 2 [300/2772 (5%)]\tLoss: 0.163610\n",
      "Train Epoch: 2 [600/2772 (11%)]\tLoss: 0.269715\n",
      "Train Epoch: 2 [900/2772 (16%)]\tLoss: 0.245045\n",
      "Train Epoch: 2 [1200/2772 (22%)]\tLoss: 0.178293\n",
      "Train Epoch: 2 [1500/2772 (27%)]\tLoss: 0.200289\n",
      "Train Epoch: 2 [1800/2772 (32%)]\tLoss: 0.183259\n",
      "Train Epoch: 2 [2100/2772 (38%)]\tLoss: 0.130380\n",
      "Train Epoch: 2 [2400/2772 (43%)]\tLoss: 0.211923\n",
      "Train Epoch: 2 [2700/2772 (49%)]\tLoss: 0.224975\n",
      "\n",
      "Average Train Epoch Loss:  0.2013249739327214\n",
      "Average Train Bin Epoch Loss:  0.1583499251118986\n",
      "Average Train Count Epoch Loss:  0.04297504845393813\n",
      "Average Validation Epoch Loss:  0.33766250276849386\n",
      "Average Validation Bin Epoch Loss:  0.27736694401218775\n",
      "Average Validation Count Epoch Loss:  0.06029555906674692\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/2772 (0%)]\tLoss: 0.265351\n",
      "Train Epoch: 3 [300/2772 (5%)]\tLoss: 0.179847\n",
      "Train Epoch: 3 [600/2772 (11%)]\tLoss: 0.158360\n",
      "Train Epoch: 3 [900/2772 (16%)]\tLoss: 0.214331\n",
      "Train Epoch: 3 [1200/2772 (22%)]\tLoss: 0.243912\n",
      "Train Epoch: 3 [1500/2772 (27%)]\tLoss: 0.107389\n",
      "Train Epoch: 3 [1800/2772 (32%)]\tLoss: 0.137079\n",
      "Train Epoch: 3 [2100/2772 (38%)]\tLoss: 0.158232\n",
      "Train Epoch: 3 [2400/2772 (43%)]\tLoss: 0.194280\n",
      "Train Epoch: 3 [2700/2772 (49%)]\tLoss: 0.221289\n",
      "\n",
      "Average Train Epoch Loss:  0.18798483677563213\n",
      "Average Train Bin Epoch Loss:  0.1489823873021773\n",
      "Average Train Count Epoch Loss:  0.039002449263806464\n",
      "Average Validation Epoch Loss:  0.3158311354262488\n",
      "Average Validation Bin Epoch Loss:  0.2590119494568734\n",
      "Average Validation Count Epoch Loss:  0.056819187565928415\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/2772 (0%)]\tLoss: 0.095819\n",
      "Train Epoch: 4 [300/2772 (5%)]\tLoss: 0.175169\n",
      "Train Epoch: 4 [600/2772 (11%)]\tLoss: 0.216637\n",
      "Train Epoch: 4 [900/2772 (16%)]\tLoss: 0.175118\n",
      "Train Epoch: 4 [1200/2772 (22%)]\tLoss: 0.161809\n",
      "Train Epoch: 4 [1500/2772 (27%)]\tLoss: 0.231566\n",
      "Train Epoch: 4 [1800/2772 (32%)]\tLoss: 0.239180\n",
      "Train Epoch: 4 [2100/2772 (38%)]\tLoss: 0.150070\n",
      "Train Epoch: 4 [2400/2772 (43%)]\tLoss: 0.175164\n",
      "Train Epoch: 4 [2700/2772 (49%)]\tLoss: 0.152373\n",
      "\n",
      "Average Train Epoch Loss:  0.17581250750786298\n",
      "Average Train Bin Epoch Loss:  0.13971945916774212\n",
      "Average Train Count Epoch Loss:  0.03609304833608917\n",
      "Average Validation Epoch Loss:  0.3193708330038048\n",
      "Average Validation Bin Epoch Loss:  0.2618468425103596\n",
      "Average Validation Count Epoch Loss:  0.057523989029938265\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/2772 (0%)]\tLoss: 0.172332\n",
      "Train Epoch: 5 [300/2772 (5%)]\tLoss: 0.141931\n",
      "Train Epoch: 5 [600/2772 (11%)]\tLoss: 0.082167\n",
      "Train Epoch: 5 [900/2772 (16%)]\tLoss: 0.183230\n",
      "Train Epoch: 5 [1200/2772 (22%)]\tLoss: 0.109451\n",
      "Train Epoch: 5 [1500/2772 (27%)]\tLoss: 0.168905\n",
      "Train Epoch: 5 [1800/2772 (32%)]\tLoss: 0.131085\n",
      "Train Epoch: 5 [2100/2772 (38%)]\tLoss: 0.155743\n",
      "Train Epoch: 5 [2400/2772 (43%)]\tLoss: 0.208699\n",
      "Train Epoch: 5 [2700/2772 (49%)]\tLoss: 0.108079\n",
      "\n",
      "Average Train Epoch Loss:  0.16410146550440685\n",
      "Average Train Bin Epoch Loss:  0.13087697698859682\n",
      "Average Train Count Epoch Loss:  0.03322448839082733\n",
      "Average Validation Epoch Loss:  0.3139979275209563\n",
      "Average Validation Bin Epoch Loss:  0.25355886330916766\n",
      "Average Validation Count Epoch Loss:  0.06043906509876251\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/2772 (0%)]\tLoss: 0.144880\n",
      "Train Epoch: 6 [300/2772 (5%)]\tLoss: 0.154605\n",
      "Train Epoch: 6 [600/2772 (11%)]\tLoss: 0.146255\n",
      "Train Epoch: 6 [900/2772 (16%)]\tLoss: 0.211508\n",
      "Train Epoch: 6 [1200/2772 (22%)]\tLoss: 0.174481\n",
      "Train Epoch: 6 [1500/2772 (27%)]\tLoss: 0.179070\n",
      "Train Epoch: 6 [1800/2772 (32%)]\tLoss: 0.164749\n",
      "Train Epoch: 6 [2100/2772 (38%)]\tLoss: 0.077581\n",
      "Train Epoch: 6 [2400/2772 (43%)]\tLoss: 0.155256\n",
      "Train Epoch: 6 [2700/2772 (49%)]\tLoss: 0.123116\n",
      "\n",
      "Average Train Epoch Loss:  0.15385621794180954\n",
      "Average Train Bin Epoch Loss:  0.12341342776110678\n",
      "Average Train Count Epoch Loss:  0.030442789912594734\n",
      "Average Validation Epoch Loss:  0.3193707127301466\n",
      "Average Validation Bin Epoch Loss:  0.2617707160257158\n",
      "Average Validation Count Epoch Loss:  0.05759999754705599\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/2772 (0%)]\tLoss: 0.119372\n",
      "Train Epoch: 7 [300/2772 (5%)]\tLoss: 0.120556\n",
      "Train Epoch: 7 [600/2772 (11%)]\tLoss: 0.149255\n",
      "Train Epoch: 7 [900/2772 (16%)]\tLoss: 0.126709\n",
      "Train Epoch: 7 [1200/2772 (22%)]\tLoss: 0.130000\n",
      "Train Epoch: 7 [1500/2772 (27%)]\tLoss: 0.210570\n",
      "Train Epoch: 7 [1800/2772 (32%)]\tLoss: 0.167365\n",
      "Train Epoch: 7 [2100/2772 (38%)]\tLoss: 0.143532\n",
      "Train Epoch: 7 [2400/2772 (43%)]\tLoss: 0.148687\n",
      "Train Epoch: 7 [2700/2772 (49%)]\tLoss: 0.139447\n",
      "\n",
      "Average Train Epoch Loss:  0.14471332576464524\n",
      "Average Train Bin Epoch Loss:  0.11632909395961792\n",
      "Average Train Count Epoch Loss:  0.028384232155785158\n",
      "Average Validation Epoch Loss:  0.3248328534620149\n",
      "Average Validation Bin Epoch Loss:  0.25824435019776937\n",
      "Average Validation Count Epoch Loss:  0.06658850361903508\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/2772 (0%)]\tLoss: 0.121538\n",
      "Train Epoch: 8 [300/2772 (5%)]\tLoss: 0.126326\n",
      "Train Epoch: 8 [600/2772 (11%)]\tLoss: 0.106870\n",
      "Train Epoch: 8 [900/2772 (16%)]\tLoss: 0.111500\n",
      "Train Epoch: 8 [1200/2772 (22%)]\tLoss: 0.156917\n",
      "Train Epoch: 8 [1500/2772 (27%)]\tLoss: 0.179619\n",
      "Train Epoch: 8 [1800/2772 (32%)]\tLoss: 0.136867\n",
      "Train Epoch: 8 [2100/2772 (38%)]\tLoss: 0.086740\n",
      "Train Epoch: 8 [2400/2772 (43%)]\tLoss: 0.147290\n",
      "Train Epoch: 8 [2700/2772 (49%)]\tLoss: 0.117724\n",
      "\n",
      "Average Train Epoch Loss:  0.13608057071249208\n",
      "Average Train Bin Epoch Loss:  0.11000014296470782\n",
      "Average Train Count Epoch Loss:  0.026080427610706457\n",
      "Average Validation Epoch Loss:  0.32742233680827276\n",
      "Average Validation Bin Epoch Loss:  0.26333227948773474\n",
      "Average Validation Count Epoch Loss:  0.06409005563528765\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/2772 (0%)]\tLoss: 0.154682\n",
      "Train Epoch: 9 [300/2772 (5%)]\tLoss: 0.118921\n",
      "Train Epoch: 9 [600/2772 (11%)]\tLoss: 0.134462\n",
      "Train Epoch: 9 [900/2772 (16%)]\tLoss: 0.129210\n",
      "Train Epoch: 9 [1200/2772 (22%)]\tLoss: 0.129944\n",
      "Train Epoch: 9 [1500/2772 (27%)]\tLoss: 0.111387\n",
      "Train Epoch: 9 [1800/2772 (32%)]\tLoss: 0.114720\n",
      "Train Epoch: 9 [2100/2772 (38%)]\tLoss: 0.088735\n",
      "Train Epoch: 9 [2400/2772 (43%)]\tLoss: 0.151068\n",
      "Train Epoch: 9 [2700/2772 (49%)]\tLoss: 0.084420\n",
      "\n",
      "Average Train Epoch Loss:  0.12777202180485964\n",
      "Average Train Bin Epoch Loss:  0.10386708230425269\n",
      "Average Train Count Epoch Loss:  0.023904939638692644\n",
      "Average Validation Epoch Loss:  0.33610358976182486\n",
      "Average Validation Bin Epoch Loss:  0.2683064728265717\n",
      "Average Validation Count Epoch Loss:  0.0677971165361149\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/2772 (0%)]\tLoss: 0.088272\n",
      "Train Epoch: 10 [300/2772 (5%)]\tLoss: 0.162495\n",
      "Train Epoch: 10 [600/2772 (11%)]\tLoss: 0.061398\n",
      "Train Epoch: 10 [900/2772 (16%)]\tLoss: 0.130992\n",
      "Train Epoch: 10 [1200/2772 (22%)]\tLoss: 0.134291\n",
      "Train Epoch: 10 [1500/2772 (27%)]\tLoss: 0.118807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [1800/2772 (32%)]\tLoss: 0.153917\n",
      "Train Epoch: 10 [2100/2772 (38%)]\tLoss: 0.138782\n",
      "Train Epoch: 10 [2400/2772 (43%)]\tLoss: 0.172132\n",
      "Train Epoch: 10 [2700/2772 (49%)]\tLoss: 0.123063\n",
      "\n",
      "Average Train Epoch Loss:  0.11956459561209658\n",
      "Average Train Bin Epoch Loss:  0.09805702593871009\n",
      "Average Train Count Epoch Loss:  0.021507569614926846\n",
      "Average Validation Epoch Loss:  0.3370223045349121\n",
      "Average Validation Bin Epoch Loss:  0.2694134564981574\n",
      "Average Validation Count Epoch Loss:  0.06760884777066253\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/2772 (0%)]\tLoss: 0.104943\n",
      "Train Epoch: 11 [300/2772 (5%)]\tLoss: 0.062462\n",
      "Train Epoch: 11 [600/2772 (11%)]\tLoss: 0.116472\n",
      "Train Epoch: 11 [900/2772 (16%)]\tLoss: 0.086081\n",
      "Train Epoch: 11 [1200/2772 (22%)]\tLoss: 0.134763\n",
      "Train Epoch: 11 [1500/2772 (27%)]\tLoss: 0.091290\n",
      "Train Epoch: 11 [1800/2772 (32%)]\tLoss: 0.121088\n",
      "Train Epoch: 11 [2100/2772 (38%)]\tLoss: 0.081894\n",
      "Train Epoch: 11 [2400/2772 (43%)]\tLoss: 0.117602\n",
      "Train Epoch: 11 [2700/2772 (49%)]\tLoss: 0.087547\n",
      "\n",
      "Average Train Epoch Loss:  0.11155737171280178\n",
      "Average Train Bin Epoch Loss:  0.09242112004879491\n",
      "Average Train Count Epoch Loss:  0.019136251907924683\n",
      "Average Validation Epoch Loss:  0.3536996671131679\n",
      "Average Validation Bin Epoch Loss:  0.2900994499879224\n",
      "Average Validation Count Epoch Loss:  0.06360021694785073\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/2772 (0%)]\tLoss: 0.092886\n",
      "Train Epoch: 12 [300/2772 (5%)]\tLoss: 0.110603\n",
      "Train Epoch: 12 [600/2772 (11%)]\tLoss: 0.093490\n",
      "Train Epoch: 12 [900/2772 (16%)]\tLoss: 0.141700\n",
      "Train Epoch: 12 [1200/2772 (22%)]\tLoss: 0.112347\n",
      "Train Epoch: 12 [1500/2772 (27%)]\tLoss: 0.108490\n",
      "Train Epoch: 12 [1800/2772 (32%)]\tLoss: 0.114573\n",
      "Train Epoch: 12 [2100/2772 (38%)]\tLoss: 0.094235\n",
      "Train Epoch: 12 [2400/2772 (43%)]\tLoss: 0.070102\n",
      "Train Epoch: 12 [2700/2772 (49%)]\tLoss: 0.095507\n",
      "\n",
      "Average Train Epoch Loss:  0.10462450905344187\n",
      "Average Train Bin Epoch Loss:  0.08770949253610848\n",
      "Average Train Count Epoch Loss:  0.01691501657831285\n",
      "Average Validation Epoch Loss:  0.37526718917347135\n",
      "Average Validation Bin Epoch Loss:  0.3088333196938038\n",
      "Average Validation Count Epoch Loss:  0.06643387045533884\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/2772 (0%)]\tLoss: 0.079448\n",
      "Train Epoch: 13 [300/2772 (5%)]\tLoss: 0.078661\n",
      "Train Epoch: 13 [600/2772 (11%)]\tLoss: 0.097873\n",
      "Train Epoch: 13 [900/2772 (16%)]\tLoss: 0.181669\n",
      "Train Epoch: 13 [1200/2772 (22%)]\tLoss: 0.110068\n",
      "Train Epoch: 13 [1500/2772 (27%)]\tLoss: 0.081189\n",
      "Train Epoch: 13 [1800/2772 (32%)]\tLoss: 0.099686\n",
      "Train Epoch: 13 [2100/2772 (38%)]\tLoss: 0.127060\n",
      "Train Epoch: 13 [2400/2772 (43%)]\tLoss: 0.093978\n",
      "Train Epoch: 13 [2700/2772 (49%)]\tLoss: 0.110496\n",
      "\n",
      "Average Train Epoch Loss:  0.09730628701289754\n",
      "Average Train Bin Epoch Loss:  0.08257695226990558\n",
      "Average Train Count Epoch Loss:  0.014729334718801758\n",
      "Average Validation Epoch Loss:  0.3729886423264231\n",
      "Average Validation Bin Epoch Loss:  0.30069022572466303\n",
      "Average Validation Count Epoch Loss:  0.07229841859745127\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/2772 (0%)]\tLoss: 0.077915\n",
      "Train Epoch: 14 [300/2772 (5%)]\tLoss: 0.067938\n",
      "Train Epoch: 14 [600/2772 (11%)]\tLoss: 0.083980\n",
      "Train Epoch: 14 [900/2772 (16%)]\tLoss: 0.043237\n",
      "Train Epoch: 14 [1200/2772 (22%)]\tLoss: 0.084260\n",
      "Train Epoch: 14 [1500/2772 (27%)]\tLoss: 0.065855\n",
      "Train Epoch: 14 [1800/2772 (32%)]\tLoss: 0.104446\n",
      "Train Epoch: 14 [2100/2772 (38%)]\tLoss: 0.076960\n",
      "Train Epoch: 14 [2400/2772 (43%)]\tLoss: 0.087399\n",
      "Train Epoch: 14 [2700/2772 (49%)]\tLoss: 0.105273\n",
      "\n",
      "Average Train Epoch Loss:  0.08980066878761435\n",
      "Average Train Bin Epoch Loss:  0.07747484183995239\n",
      "Average Train Count Epoch Loss:  0.012325827104898242\n",
      "Average Validation Epoch Loss:  0.38947297419820515\n",
      "Average Validation Bin Epoch Loss:  0.3084888992210229\n",
      "Average Validation Count Epoch Loss:  0.08098407475543874\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/2772 (0%)]\tLoss: 0.082072\n",
      "Train Epoch: 15 [300/2772 (5%)]\tLoss: 0.065654\n",
      "Train Epoch: 15 [600/2772 (11%)]\tLoss: 0.114146\n",
      "Train Epoch: 15 [900/2772 (16%)]\tLoss: 0.042652\n",
      "Train Epoch: 15 [1200/2772 (22%)]\tLoss: 0.097961\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.253 val bin loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
