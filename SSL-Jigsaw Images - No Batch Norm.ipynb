{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [4, 4]\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import ben_resnet_no_batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants taken from data_helper.py\n",
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]\n",
    "\n",
    "# Unlabeled scenes\n",
    "val_unlabeled_scene_index = np.arange(80,106)\n",
    "train_unlabeled_scene_index = np.arange(80)\n",
    "\n",
    "# Image folder\n",
    "image_folder = '/scratch/brs426/data'\n",
    "annotation_csv = '/scratch/brs426/data/annotation.csv'\n",
    "first_dim = 'image'\n",
    "\n",
    "# Option 1\n",
    "CROP_SIZE = 225\n",
    "PATCH_SIZE = 64\n",
    "\n",
    "# Option 2\n",
    "# CROP_SIZE = 150\n",
    "# PATCH_SIZE = 32\n",
    "\n",
    "# Add other data augmentation\n",
    "\n",
    "# Augmentation trasnforms\n",
    "aug_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(CROP_SIZE),\n",
    "    torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.4, hue = (-0.5, 0.5)),\n",
    "        torchvision.transforms.Grayscale(3),\n",
    "#         transforms.RandomAffine(3),\n",
    "    ]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "# Transforms\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(CROP_SIZE),torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximal Hamming permutations\n",
    "# N is the number of permutations to return (subsample size)\n",
    "# k are the permutations (k!)\n",
    "# TODO: Speed up\n",
    "def get_maximal_Hamming_permutation_set(N,k):\n",
    "    # Generate all permutations and convert to matrix\n",
    "    perms = list(itertools.permutations(range(k)))\n",
    "    perms = np.array(perms).T\n",
    "    \n",
    "    # Get random index\n",
    "    idx = np.random.randint(0, len(perms))\n",
    "    \n",
    "    # Counter\n",
    "    i = 1\n",
    "    \n",
    "    # Get corresponding permutation and reshape to be 3d for broadcasting\n",
    "    p = perms[:, idx].T\n",
    "    p = p.reshape(i, k, -1)\n",
    "    \n",
    "    # Remove permutation to prevent resampling\n",
    "    perms = np.concatenate((perms[:, 0:idx],perms[:, idx+1:]), axis=1)\n",
    "    \n",
    "    # Compute Hamming distance where resulting matrix is num_perms x num_remaining_perms\n",
    "    while i <= N-1:\n",
    "        print(i)\n",
    "        # Compute Hamming distance\n",
    "        hamming_dist = np.count_nonzero(p != perms, axis=1)\n",
    "        ones = np.ones(i)\n",
    "        dist = np.dot(ones, hamming_dist)\n",
    "        # Get next index\n",
    "        idx = np.argmax(dist)\n",
    "        # Get corresponding permutation\n",
    "        new_p = perms[:, idx].T\n",
    "        # Reshape and append\n",
    "        new_p = new_p.reshape(1, k, -1)\n",
    "        p = np.append(p, new_p, axis=0)\n",
    "        # Increment\n",
    "        i += 1\n",
    "        # Remove\n",
    "        perms = np.concatenate((perms[:, 0:idx], perms[:, idx+1:]), axis=1)\n",
    "    p = p.reshape(N, k).tolist()\n",
    "    return p, perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "PERMUTATION_LIMIT = 200\n",
    "hard_permutations, remaining = get_maximal_Hamming_permutation_set(PERMUTATION_LIMIT, 9)\n",
    "\n",
    "with open(\"/scratch/brs426/permutation_200_hard.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hard_permutations, f)\n",
    "    pickle.dump(remaining, f)\n",
    "#permutations = list(itertools.permutations(range(9)))\n",
    "#random_permutations = random.sample(permutations, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/scratch/brs426/permutation_200_hard.pkl\", \"rb\") as f:\n",
    "    permutations = pickle.load(f)\n",
    "    remaining = pickle.load(f)\n",
    "    \n",
    "new_remaining = remaining.T\n",
    "new_remaining = new_remaining.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 50 Random Permutations to Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_permutations = random.sample(new_remaining, 50)\n",
    "final_permutations = permutations + extra_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off of the UnlabeledDataset in data_helper.py\n",
    "# Creates: grid_size x grid_size of cells that have cell_size x cell_size\n",
    "# In each cell, we randomly extract a patch of patch_size x patch_size\n",
    "# TODO: Make sure crop_size, grid_size, cell_size, and patch_size are compatible\n",
    "# TODO: Probably need to clean up a little. Especially patch creation.\n",
    "class JigsawPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, scene_index, transform, patch_size, cell_size, grid_size, permutations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): the location of the image folder\n",
    "            scene_index (list): a list of scene indices for the unlabeled data \n",
    "            first_dim ({'sample', 'image'}):\n",
    "                'sample' will return [batch_size, NUM_IMAGE_PER_SAMPLE, 3, H, W]\n",
    "                'image' will return [batch_size, 3, H, W] and the index of the camera [0 - 5]\n",
    "                    CAM_FRONT_LEFT: 0\n",
    "                    CAM_FRONT: 1\n",
    "                    CAM_FRONT_RIGHT: 2\n",
    "                    CAM_BACK_LEFT: 3\n",
    "                    CAM_BACK.jpeg: 4\n",
    "                    CAM_BACK_RIGHT: 5\n",
    "            transform (Transform): The function to process the image\n",
    "        \"\"\"\n",
    "\n",
    "        self.image_folder = image_folder\n",
    "        self.scene_index = scene_index\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.cell_size = cell_size\n",
    "        self.grid_size = grid_size\n",
    "        assert first_dim in ['sample', 'image']\n",
    "        self.first_dim = first_dim\n",
    "        self.permutations = permutations\n",
    "        \n",
    "        # Number of examples\n",
    "        self.length = self.scene_index.size * NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE\n",
    "        \n",
    "        # Differential between patch_size and grid cell size\n",
    "        self.diff = self.cell_size - self.patch_size\n",
    "        \n",
    "        # List to store differentials to get patches from cells.\n",
    "        # Set differentials so none of patches will be on the boundary\n",
    "        self.x_diff = [random.randint(1, self.diff-2) for i in range(self.length)]\n",
    "        self.y_diff = [random.randint(1, self.diff-2) for i in range(self.length)]\n",
    "        \n",
    "        # List to store the permutation for each image\n",
    "        self.idx_permutations = [random.randint(0, len(self.permutations)-1) for i in range(self.length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        scene_id = self.scene_index[index // (NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE)]\n",
    "        sample_id = (index % (NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE)) // NUM_IMAGE_PER_SAMPLE\n",
    "        image_name = image_names[index % NUM_IMAGE_PER_SAMPLE]\n",
    "\n",
    "        image_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}', image_name) \n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "        patches = []\n",
    "        diff_x = self.x_diff[index]\n",
    "        diff_y = self.y_diff[index]\n",
    "        permutation = self.permutations[self.idx_permutations[index]]\n",
    "        # Create random patch from grid cell\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                patch = image[:, self.cell_size*j:self.cell_size*(j+1), self.cell_size*i:self.cell_size*(i+1)]\n",
    "                patch = patch[:, diff_x:diff_x+self.patch_size, diff_y:diff_y+self.patch_size]\n",
    "                patches.append(patch)\n",
    "        patch_tensor = torch.stack(patches)\n",
    "        original_patch_tensor = patch_tensor\n",
    "        # Permute\n",
    "        patch_tensor = patch_tensor[permutation, :, :, :]\n",
    "        \n",
    "        # Return patch tensor and permutation\n",
    "        return original_patch_tensor, patch_tensor, image, self.idx_permutations[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jigsaw = JigsawPredictionDataset(image_folder, train_unlabeled_scene_index, aug_transform, patch_size=64, cell_size=75, grid_size=3, permutations=final_permutations)\n",
    "val_jigsaw = JigsawPredictionDataset(image_folder, val_unlabeled_scene_index, transform, patch_size=64, cell_size=75, grid_size=3, permutations=final_permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_train_loader = torch.utils.data.DataLoader(train_jigsaw, batch_size=1, shuffle=True, num_workers=4)\n",
    "jigsaw_val_loader = torch.utils.data.DataLoader(val_jigsaw, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resnet-based model\n",
    "# Resnet outputs unnormalized scores for 1000 class classification problem\n",
    "# Specify any of the available ResNet models and load them in\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, resnet, num_classes, hidden_size):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        if resnet not in [18, 34, 50, 101, 152]:\n",
    "            raise Exception(\"Invalid resnet size\")\n",
    "        self.resnet = getattr(ben_resnet_no_batch_norm, \"resnet{}\".format(resnet))()\n",
    "        \n",
    "        # Remove batchnorm layers\n",
    "        self.fc1 = nn.Linear(1000, hidden_size)\n",
    "        # Mutliply by 9 because we are concatenating the representations for each image together\n",
    "        self.fc2 = nn.Linear(9 * hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x is (Batch x Number of Images x Channels x Height x Width)\n",
    "        batch = x.shape[0]\n",
    "        channels = x.shape[2]\n",
    "        height = x.shape[3]\n",
    "        width = x.shape[4]\n",
    "        x = x.reshape(-1, channels, height, width)\n",
    "        # Now our input is ((Batch x Number of Images) x Channels x Height x Width)\n",
    "        x = F.relu(self.resnet(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Now we need to recast to have original batch size (should double check this)\n",
    "        x = x.reshape(batch, -1)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logic, return average loss over training set after each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch, log_file_path, log_interval = 250):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Number correct for accuracy\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Train loss\n",
    "    train_loss = 0\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # Loop through examples\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Send data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero out optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass data through model\n",
    "        output = model(data)\n",
    "        predictions = torch.argmax(output, 1)\n",
    "        num_correct += torch.sum(predictions == target).item()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            f.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    \n",
    "    # Average train loss\n",
    "    average_train_loss = train_loss / len(train_loader)\n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        average_train_loss, num_correct, len(train_loader.dataset),\n",
    "        100. * num_correct / len(train_loader.dataset)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test method\n",
    "def test(model, device, test_loader, log_file_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    f = open(log_file_path, \"a+\")\n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        # Loop through data points\n",
    "        for data, target in test_loader:\n",
    "            pass # remove once implemented\n",
    "        \n",
    "            # Send data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Pass data through model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
    "            loss = F.cross_entropy(output, target, reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predictions from the model for each data point\n",
    "            predictions = torch.argmax(output, 1)\n",
    "\n",
    "            # Add number of correct predictions to total num_correct \n",
    "            num_correct += torch.sum(predictions == target).item()\n",
    "            \n",
    "    \n",
    "    # Compute the average test_loss\n",
    "    # avg_test_loss = TODO\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    f.write('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))\n",
    "    f.close()\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet model\n",
    "res = ResNetModel(50,250,256).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(res.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"/scratch/brs426/ben_models/pretrained_resnet50_250_permutations.log\"\n",
    "#best_val_loss = None\n",
    "save_path = \"/scratch/brs426/ben_models/pretrained_resnet50_250_permutations.p\"\n",
    "epochs = 30\n",
    "for epoch in range(1, epoch + 1):\n",
    "    # Train model\n",
    "    train(res, device, jigsaw_train_loader, optimizer, epoch, f)\n",
    "    val_loss = test(res, device, jigsaw_val_loader, f)\n",
    "    # Save model\n",
    "    if best_val_loss is None:\n",
    "        torch.save({'model_state_dict': res.state_dict(),\n",
    "                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "                   'loss': val_loss}, save_path)\n",
    "        best_val_loss = val_loss\n",
    "    elif val_loss < best_val_loss:\n",
    "        torch.save({'model_state_dict': res.state_dict(),\n",
    "                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "                   'loss': val_loss}, save_path)\n",
    "        best_val_loss = val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
